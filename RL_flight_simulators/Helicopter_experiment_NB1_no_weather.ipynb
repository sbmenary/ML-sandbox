{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643c6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ste/miniforge3/envs/tf-sandbox-py3p9/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has found devices:\n",
      "-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "#  Required imports\n",
    "\n",
    "import math, os, sys, time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import animation, pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from multiprocess import Process, Queue\n",
    "from threading import Lock, Thread\n",
    "\n",
    "import threading\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers    import Conv2D, Concatenate, Dense, Dropout, Flatten, Input, MaxPooling2D, Rescaling\n",
    "from tensorflow.keras.models    import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"TensorFlow has found devices:\")\n",
    "for device in tf.config.list_physical_devices() :\n",
    "    print(f\"-  {device}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Global constants\n",
    "###\n",
    "\n",
    "#  Initially we will just run on a game board of fixed size, to avoid building an architecture to handle \n",
    "#  variable board sizes, so let's configure this here\n",
    "\n",
    "horizontal_size = 30\n",
    "vertical_size   = 20\n",
    "horizontal_pad  = 5\n",
    "vertical_pad    = 5\n",
    "horizontal_max  = horizontal_size + 2*horizontal_pad\n",
    "vertical_max    = vertical_size   + 2*vertical_pad\n",
    "\n",
    "# reward = -1 per turn more interesting because it forces policy to learn optimal long-term return, not\n",
    "#    just optimise the immediate return\n",
    "reward_per_turn = -1\n",
    "lambda_r        = 0\n",
    "lambda_b        = -2.\n",
    "gamma           = 1.\n",
    "\n",
    "r_start = np.array([horizontal_pad, vertical_pad+vertical_size-1])\n",
    "r_end   = np.array([horizontal_pad+horizontal_size-1, vertical_pad]) \n",
    "r_norm  = np.array([horizontal_max, vertical_max])\n",
    "\n",
    "action_list = np.array([[-1, -1], [-1, 0], [-1, 1], [0, -1], [0, 0], [0, 1], [1, -1], [1, 0], [1, 1]])\n",
    "\n",
    "print(f\"r_start = {r_start}\")\n",
    "print(f\"r_end   = {r_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Define and unit-test methods used to calculate Lp measures and norms\n",
    "###\n",
    "\n",
    "\n",
    "def Lp_norm(v, p=2) :\n",
    "    '''\n",
    "    Calculate Lp norm of vector v, defined as [sum_i v_i^p]^(1/p). If np.isfinite(p) returns False then\n",
    "    calculate the L-infinity norm, which just returns the highest mod-vector-component.\n",
    "    Inputs:\n",
    "      > v, np.ndarray of any shape >= 1D\n",
    "        vector to calculate the norm of\n",
    "      > p, float, default=2\n",
    "        exponent of the Lp norm\n",
    "    Return:\n",
    "      > float, the Lp norm\n",
    "    '''\n",
    "    ##  If type(v) is not numpy array then try to cast it to one\n",
    "    if type(v) != np.ndarray :\n",
    "        v = np.array(v)\n",
    "    ##  Return L-infinity norm if p is not a real number\n",
    "    if not np.isfinite(p) :\n",
    "        return np.fabs(v).max()\n",
    "    ##  Use np.power method to return Lp norm if p is finite\n",
    "    return np.power(np.power(v, p).sum(), 1./p)\n",
    "  \n",
    "    \n",
    "def Lp_distance(v1, v2, p=2) :\n",
    "    '''\n",
    "    Calculate Lp distance between vectors v1 and v2 by calling Lp_norm(v2-v1).\n",
    "    Inputs:\n",
    "      > v1, np.ndarray of any shape >= 1D\n",
    "        first vector\n",
    "      > v2, np.ndarray of same shape as v1\n",
    "        second vector\n",
    "      > p, float, default=2\n",
    "        exponent of the Lp-distance\n",
    "    Return:\n",
    "      > float, the Lp distance\n",
    "    '''\n",
    "    ##  If v1 or v2 are not numpy arrays then try to cast them\n",
    "    if type(v1) != np.ndarray :\n",
    "        v1 = np.array(v1)\n",
    "    if type(v2) != np.ndarray :\n",
    "        v2 = np.array(v2)\n",
    "    ##  Return the norm of the difference between v1 and v2\n",
    "    return Lp_norm(v2 - v1, p=p)\n",
    "\n",
    "\n",
    "V1 = np.array([2, 4, 5, 7])\n",
    "V2 = np.array([5, 3, 6, 4])\n",
    "dV = V2 - V1\n",
    "\n",
    "print(f\"UNIT TEST: Lp_norm({V1}, p=2) = {Lp_norm(V1, 2):.3f}  [expect {np.sqrt((V1**2).sum()):.3f}]\")\n",
    "print(f\"UNIT TEST: Lp_norm({V1}, p=2.0) = {Lp_norm(V1, 2.0):.3f}  [expect {np.sqrt((V1**2).sum()):.3f}]\")\n",
    "print(f\"UNIT TEST: Lp_norm({V1}, p=np.inf) = {Lp_norm(V1, np.inf):.3f}  [expect {np.fabs(V1).max():.3f}]\")\n",
    "\n",
    "print(f\"UNIT TEST: Lp_distance({V1}, {V2}, p=2) = {Lp_distance(V1, V2, 2):.3f}  [expect {np.sqrt((dV**2).sum()):.3f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15affde",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Define and unit-test environment methods\n",
    "###\n",
    "\n",
    "\n",
    "def perform_action(r_agent, action, base_reward=reward_per_turn, boundary_reward=lambda_b, \n",
    "                   dr_reward_factor=lambda_r, verbose=False, agent_is_normed=False) :\n",
    "    '''\n",
    "    Given the current environment and agent states, perform the specified action and return the reward \n",
    "    obtaine along with the new agent state.\n",
    "    Inputs:\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        (x,y) position of agent at initial timestep\n",
    "      > action, np.ndarray of shape (2,)\n",
    "        (dx,dy) of action to be performed, each component expected to be one of {-1, 0, +1}\n",
    "      > base_reward, float, default=-2.\n",
    "        basic reward returned every turn (expected -ve)\n",
    "      > boundary_reward, float, default=lambda_b\n",
    "        reward received when encountering the edge of the game board (expected -ve)\n",
    "      > dr_reward_factor, float, default=lambda_r\n",
    "        factor multiplied by change-in-distance to calculate movement reward (expected +ve)\n",
    "    Returns:\n",
    "      > float\n",
    "        reward obtained by performing action\n",
    "      > np.ndarray of shape (2,)\n",
    "        (x,y) position of agent at iterated timestep\n",
    "    '''\n",
    "    ##  Undo agent feature normalisation if needed\n",
    "    if agent_is_normed :\n",
    "        r_agent = r_agent.copy() * r_norm\n",
    "    ##  Make sure initial state is valid to protect against unexpected behaviour\n",
    "    if is_terminal(r_agent) :\n",
    "        raise RuntimeError(f\"Agent state is terminal, so no actions may be performed\")\n",
    "    if is_out_of_bounds(r_agent) :\n",
    "        raise RuntimeError(f\"Agent state {r_agent} is out of bounds, so no actions may be performed\")\n",
    "    ##  Get initial distance of agent from the end\n",
    "    d_agent = Lp_distance(r_agent, r_end)\n",
    "    ##  Iterate agent position\n",
    "    ##  - if agent hits a wall then add an appropriate penalty and return agent to original position\n",
    "    r_agent_p = r_agent + action\n",
    "    reward_b  = 0\n",
    "    if r_agent_p[0] < 0 or r_agent_p[0] >= horizontal_max or r_agent_p[1] < 0 or r_agent_p[1] >= vertical_max :\n",
    "        reward_b  = boundary_reward\n",
    "        r_agent_p = r_agent.copy()\n",
    "    ##  Get distance-based reward\n",
    "    d_agent_p = Lp_distance(r_agent_p, r_end)\n",
    "    reward_r  = dr_reward_factor * (d_agent - d_agent_p) / np.sqrt(2)\n",
    "    ##  Calculate total reward by summing the base, boundary and distance rewards\n",
    "    reward = base_reward + reward_b + reward_r\n",
    "    if verbose :\n",
    "        print(f\"perform_action: agent {r_agent} action {action} --> agent {r_agent_p} reward {reward:.2f}  [{base_reward:.2f} (base) + {reward_b:.2f} (b) + {reward_r:.2f} (r)]\")\n",
    "    ##  Return reward and new agent state\n",
    "    return reward, r_agent_p\n",
    "\n",
    "\n",
    "def is_terminal(r_agent) :\n",
    "    '''\n",
    "    Return True if the agent is in the terminal state and False otherwise.\n",
    "    Inputs:\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        agent position as (x,y)-coordinates\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether the agent is in the terminal state\n",
    "    '''\n",
    "    if r_agent[0] == r_end[0] and r_agent[1] == r_end[1] :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_out_of_bounds(r_agent) :\n",
    "    '''\n",
    "    Return True if the agent isout of bounds and False otherwise.\n",
    "    Inputs:\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        agent position as (x,y)-coordinates\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether the agent is out of bounds.\n",
    "    '''\n",
    "    if r_agent[0] <  0 : return True\n",
    "    if r_agent[1] <  0 : return True\n",
    "    if r_agent[0] >= horizontal_max : return True\n",
    "    if r_agent[1] >= vertical_max    : return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def get_greedy_action(r_agent, *q_models) :\n",
    "    '''\n",
    "    Sample a greedy action from the Q-value models provided. If multiple models provided then use their mean.\n",
    "    Inputs:\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        agent position as (x,y)-coordinates\n",
    "      > q_models, list of tf.keras Model class, each with inputs [agent position, action] = [Input(2), Input(2)]\n",
    "        list of Keras Q(s,a) models\n",
    "    Returns:\n",
    "      > np.ndarray of shape (2,)\n",
    "        action defined by greedy policy over the model(s) at this agent position\n",
    "      > list of np.ndarray of shape (9,)\n",
    "        action values in the same order as action_list, in list of models provided\n",
    "    '''\n",
    "    r_agents            = np.array([r_agent/r_norm for i in range(9)])\n",
    "    model_args          = [r_agents, action_list]\n",
    "    model_action_values = [model.predict(model_args) for model in q_models]\n",
    "    action_values       = np.mean(model_action_values, axis=0)\n",
    "    best_action         = action_list[np.argmax(action_values)]\n",
    "    return best_action, model_action_values\n",
    "\n",
    "\n",
    "def get_exploration_action(num=1) :\n",
    "    '''\n",
    "    Generate uniformly random actions from the 9 available.\n",
    "    Input:\n",
    "      > num, int, default=1\n",
    "        number of random actions to generate\n",
    "    Returns :\n",
    "      > np.ndarray of size (num,2)\n",
    "        list of actions generated\n",
    "    '''\n",
    "    return action_list[np.random.randint(low=0, high=8, size=(num,))]\n",
    "    \n",
    "\n",
    "\n",
    "r_agent, action = np.array([0, 5]), np.array([-1, 0])\n",
    "print(f\"UNIT TEST: perform_action({r_agent}, {action}, verbose=True) [expect boundary penalty]\")\n",
    "perform_action(r_agent, action, verbose=True)\n",
    "\n",
    "r_agent, action = np.array([5, 0]), np.array([0, -1])\n",
    "print(f\"\\nUNIT TEST: perform_action({r_agent}, {action}, verbose=True) [expect boundary penalty]\")\n",
    "perform_action(r_agent, action, verbose=True)\n",
    "\n",
    "r_agent = r_end + np.array([-1,1])\n",
    "print(f\"\\nUNIT TEST: perform_action({r_agent}, :, verbose=True) [expect cyclic r rewards]\")\n",
    "for action in action_list :\n",
    "    perform_action(r_agent, action, verbose=True)\n",
    "\n",
    "print(f\"\\nUNIT TEST: is_terminal({r_start}) --> {is_terminal(r_start)} [expect False]\")\n",
    "\n",
    "print(f\"\\nUNIT TEST: is_terminal({r_end}) --> {is_terminal(r_end)} [expect True]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8475c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Method for creating action-value model\n",
    "###\n",
    "\n",
    "def create_action_value_model(name=None) :\n",
    "    '''\n",
    "    Create a network for the action-value model.\n",
    "    Inputs:\n",
    "      > name, str, default=None\n",
    "        model name, if None then keras default is used\n",
    "    Returns:\n",
    "      > keras Model: uncompiled keras model (must be trained using custom loop)\n",
    "    '''\n",
    "    input_layer_r = Input ((2,))\n",
    "    input_layer_a = Input ((2,))\n",
    "    next_layer_r  = Dense(25, activation=\"relu\")(input_layer_r)\n",
    "    next_layer_a  = Dense(25, activation=\"relu\")(input_layer_a)\n",
    "    next_layer    = Concatenate()([next_layer_r, next_layer_a])\n",
    "    next_layer    = Dense(200, activation=\"relu\")(next_layer)\n",
    "    next_layer    = Dense(500, activation=\"relu\")(next_layer)\n",
    "    next_layer    = Dense(500, activation=\"relu\")(next_layer)\n",
    "    next_layer    = Dense(100, activation=\"relu\")(next_layer)\n",
    "    output_layer  = Dense(1, activation=\"linear\")(next_layer)\n",
    "    #output_layer  = Rescaling(-1.)(next_layer)\n",
    "    model         = Model([input_layer_r, input_layer_a], output_layer, name=name)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directory_for_file_path(fname, print_msg_on_dir_creation=True) :\n",
    "    \"\"\"\n",
    "    Create the directory structure needed to place file fname. Call this before fig.savefig(fname, ...) to \n",
    "    make sure fname can be created without a FileNotFoundError\n",
    "    Input:\n",
    "       - fname: str\n",
    "                name of file you want to create a tree of directories to enclose\n",
    "                also create directory at this path if fname ends in '/'\n",
    "       - print_msg_on_dir_creation: bool, default = True\n",
    "                                    if True then print a message whenever a new directory is created\n",
    "    \"\"\"\n",
    "    while \"//\" in fname :\n",
    "        fname = fname.replace(\"//\", \"/\")\n",
    "    dir_tree = fname.split(\"/\")\n",
    "    dir_tree = [\"/\".join(dir_tree[:i]) for i in range(1,len(dir_tree))]\n",
    "    dir_path = \"\"\n",
    "    for dir_path in dir_tree :\n",
    "        if len(dir_path) == 0 : continue\n",
    "        if not os.path.exists(dir_path) :\n",
    "            os.mkdir(dir_path)\n",
    "            if print_msg_on_dir_creation :\n",
    "                print(f\"Directory {dir_path} created\")\n",
    "            continue\n",
    "        if os.path.isdir(dir_path) : \n",
    "            continue\n",
    "        raise RuntimeError(f\"Cannot create directory {dir_path} because it already exists and is not a directory\")\n",
    "    \n",
    "    \n",
    "def create_greedy_policy_plot(*q_models, epoch_idx=-1, verbose=False, show=False, close=False, save=\"\",\n",
    "                              do_value_function=True) :\n",
    "    '''\n",
    "    Create a plt.Figure instance visualising the greedy policy defined by the average of the q-value models \n",
    "    provided. Allows for plot to be shown, saved and/or closed using plt interface. Returns the plot figure\n",
    "    and axis objects so they can continue to be manipulated, but note that objects will no longer be in scope\n",
    "    if we have called plt.close(fig).\n",
    "    Inputs:\n",
    "      > q_models, list of keras Model class\n",
    "        list of q-value models to define the greedy policy\n",
    "      > epoch_idx, int, default=-1\n",
    "        if positive then draw a text box displaying how many epochs have been performed\n",
    "      > verbose, bool, default=False\n",
    "        if True then print some text to display progress as we evaluate the models for every state/action pair\n",
    "      > show, bool, default=False\n",
    "        if True then call plt.show(fig)\n",
    "      > close, bool, default=False\n",
    "        if True then call plt.close(fig)\n",
    "      > save, str, default=\"\"\n",
    "        if string provided then call fig.savefig(save, ...), creating any required subdirectories if needed\n",
    "    Returns:\n",
    "      > plt.Figure instance\n",
    "      > plt.Axes instance\n",
    "    '''\n",
    "     \n",
    "    #  Keep track of how long plotting takes, to help inform how often to call this function    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #  Set up plot\n",
    "    fig = plt.figure(figsize=(12*horizontal_max/11.5,12*vertical_max/11.5))\n",
    "    ax  = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xlim(-0.5, horizontal_max-0.5)\n",
    "    ax.set_ylim(-0.5, vertical_max-0.5)\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=16)\n",
    "    \n",
    "    #  Draw grid lines manually\n",
    "    for x in range(horizontal_max-1) :\n",
    "        ax.axvline(x+0.5, ls=\"-\", c=\"gray\")\n",
    "    for y in range(vertical_max-1) :\n",
    "        ax.axhline(y+0.5, ls=\"-\", c=\"gray\")\n",
    "    \n",
    "    #  Draw arrows by looping over states and finding greedy action according to q-models\n",
    "    is_first_arrow = True\n",
    "    for x in range(horizontal_max) :\n",
    "        for y in range(vertical_max) :\n",
    "            if x == r_end[0] and y == r_end[1] : continue\n",
    "            r_agent  = np.array([x,y])\n",
    "            dr_agent = Lp_distance(r_agent, r_end)\n",
    "            if verbose :\n",
    "                sys.stdout.write(f\"\\rEvaluating greedy policy for agent state ({x}, {y})\".ljust(100))\n",
    "            action, qs = get_greedy_action(r_agent, *q_models)\n",
    "            q1s, q2s   = qs\n",
    "            r_agent_p  = r_agent + action\n",
    "            dr_agent_p = Lp_distance(r_agent_p, r_end)\n",
    "            color      = \"darkblue\" if (dr_agent_p < dr_agent and not is_out_of_bounds(r_agent_p)) else \"darkred\"\n",
    "            alpha      = 0.4 if do_value_function else 1\n",
    "            dx, dy     = action\n",
    "            if dx == 0 and dy == 0 :\n",
    "                ax.plot(x, y, \"o\", markersize=8, c=color, alpha=alpha)\n",
    "            else :\n",
    "                ax.arrow(x - 0.3*dx, y - 0.3*dy, 0.6*dx, 0.6*dy, head_width=0.25, length_includes_head=True,\n",
    "                         color=color, alpha=alpha)\n",
    "                is_first_arrow = False\n",
    "            if do_value_function :\n",
    "                ax.text(x-0.47, y+0.47, f\"{-q1s.min():.1f}\", fontsize=10.5, ha=\"left\" , va=\"top\"   , weight=\"bold\", alpha=0.7)  \n",
    "                ax.text(x+0.47, y+0.47, f\"{-q1s.max():.1f}\", fontsize=10.5, ha=\"right\", va=\"top\"   , weight=\"bold\", alpha=0.7)  \n",
    "                ax.text(x-0.47, y-0.47, f\"{-q2s.min():.1f}\", fontsize=10.5, ha=\"left\" , va=\"bottom\", weight=\"bold\", alpha=0.7)  \n",
    "                ax.text(x+0.47, y-0.47, f\"{-q2s.max():.1f}\", fontsize=10.5, ha=\"right\", va=\"bottom\", weight=\"bold\", alpha=0.7)       \n",
    "                \n",
    "    #  Draw accompanying plot objects\n",
    "    ax.fill_between([r_start[0]-0.5, r_start[0]+0.5], r_start[1]-0.5, r_start[1]+0.5, color=\"g\", alpha=0.2, label=\"Start\")\n",
    "    ax.fill_between([r_end  [0]-0.5, r_end  [0]+0.5], r_end  [1]-0.5, r_end  [1]+0.5, color=\"r\", alpha=0.2, label=\"Finish\")\n",
    "    ax.legend(loc=(0.38,1.008), ncol=3, fontsize=16, frameon=False)\n",
    "    \n",
    "    #  Draw text boxes displaying title and num. epochs\n",
    "    ax.text(0, 1.01, \"Min/max $q(s,a)$ and greedy policy per $s$\", transform=ax.transAxes, \n",
    "            fontsize=18, weight=\"bold\", ha=\"left\", va=\"bottom\")\n",
    "    if epoch_idx >= 0 :\n",
    "        ax.text(1, 1.01, f\"After {epoch_idx} epochs\", ha=\"right\", va=\"bottom\", weight=\"bold\", \n",
    "                transform=ax.transAxes, fontsize=16)\n",
    "    \n",
    "    #  Verbose messaging\n",
    "    if verbose :\n",
    "        sys.stdout.write(f\"\\nPlot created in {time.time()-start_time:.2f}s\".ljust(100)+\"\\n\")\n",
    "       \n",
    "    #  Save / show / close\n",
    "    if len(save) > 0 :\n",
    "        generate_directory_for_file_path(save)\n",
    "        plt.savefig(save, bbox_inches=\"tight\")\n",
    "    if show :\n",
    "        plt.show(fig)\n",
    "    if close :\n",
    "        plt.close(fig)\n",
    "        \n",
    "    #  Return figure and axis\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def create_training_curves_plot(loss_record, ref_loss_record, maxQ_record, show=False, close=False, save=\"\") :\n",
    "    '''\n",
    "    Create a plt.Figure instance visualising the training curves. Allows for plot to be shown, saved and/or \n",
    "    closed using plt interface. Returns the plot figure and axis objects so they can continue to be \n",
    "    manipulated, but note that objects will no longer be in scope if we have called plt.close(fig).\n",
    "    Inputs:\n",
    "      > q_models, list of keras Model class\n",
    "        list of q-value models to define the greedy policy\n",
    "      > verbose, bool, default=False\n",
    "        if True then print some text to display progress as we evaluate the models for every state/action pair\n",
    "      > show, bool, default=False\n",
    "        if True then call plt.show(fig)\n",
    "      > close, bool, default=False\n",
    "        if True then call plt.close(fig)\n",
    "      > save, str, default=\"\"\n",
    "        if string provided then call fig.savefig(save, ...), creating any required subdirectories if needed\n",
    "    Returns:\n",
    "      > plt.Figure instance\n",
    "      > plt.Axes instance (axis corresponding to loss curves)\n",
    "      > plt.Axes instance (axis corresponding to ref_loss curves)\n",
    "      > plt.Axes instance (axis corresponding to maxQ curves)\n",
    "    '''\n",
    "    \n",
    "    def draw_curve(ax, container, m, c, label) :\n",
    "        ax.plot([x for x,y in container], [y for x,y in container], m, ms=7, c=c, alpha=1.0, label=label)\n",
    "        for ((x1,y1), (x2,y2)) in zip(container[:-1], container[1:]) :\n",
    "            if np.fabs(x2-x1) > 1.5 : continue\n",
    "            ax.plot([x1, x2], [y1, y2], \"-\", c=c, lw=2, alpha=0.7)\n",
    "            \n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    \n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax1.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax1.set_title(r\"Mean loss per batch [$(1-\\lambda)\\cdot$batch + $\\lambda\\cdot$ref]\", fontsize=30)\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    draw_curve(ax1, loss_record[\"Q1\"], \"o\", \"r\", \"$q_1$\")\n",
    "    draw_curve(ax1, loss_record[\"Q2\"], \"x\", \"b\", \"$q_2$\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.legend(loc=(0,1.02), fontsize=30, ncol=3, title_fontsize=30)\n",
    "    ax1.grid()\n",
    "    \n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax2.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax2.set_title(r\"Mean loss per batch [ref only]\", fontsize=30)\n",
    "    ax2.xaxis.set_ticklabels([])\n",
    "    draw_curve(ax2, ref_loss_record[\"Q1\"], \"o\", \"r\", \"$q_1$\")\n",
    "    draw_curve(ax2, ref_loss_record[\"Q2\"], \"x\", \"b\", \"$q_2$\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.grid()\n",
    "    \n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax3.set_title(r\"Max $|q(s,a)|$ over all batches\", fontsize=30)\n",
    "    ax3.set_xlabel(r\"Epoch index\", labelpad=15, fontsize=30)\n",
    "    draw_curve(ax3, maxQ_record[\"Q1\"], \"o\", \"r\", \"$q_1$\")\n",
    "    draw_curve(ax3, maxQ_record[\"Q2\"], \"x\", \"b\", \"$q_2$\")\n",
    "    ax3.axhline(0, ls=\"--\", lw=2, c=\"gray\")\n",
    "    true_max = max([horizontal_pad+horizontal_size, vertical_pad+vertical_size]) + np.fabs(lambda_b)\n",
    "    ax3.axhline(true_max, ls=\"--\", lw=2, c=\"gray\")\n",
    "    ax3.text(0, 0.99*true_max, \"True maximum\", ha=\"left\", va=\"top\", fontsize=30, c=\"gray\")\n",
    "    ax3.grid()\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.2)\n",
    "    \n",
    "    if len(save) > 0 :\n",
    "        generate_directory_for_file_path(save)\n",
    "        plt.savefig(save, bbox_inches=\"tight\")\n",
    "    if show :\n",
    "        plt.show(fig)\n",
    "    if close :\n",
    "        plt.close(fig)\n",
    "        \n",
    "    return fig, ax1, ax2, ax3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7214634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_models(r_agent, *q_models) :\n",
    "    r_agents       = np.array([r_agent/r_norm for i in range(9)])\n",
    "    model_args     = [r_agents, action_list]\n",
    "    action_values  = np.mean([model.predict(model_args) for model in q_models], axis=0).flatten()\n",
    "    str_actions    = '  '.join([f\"{a}\".ljust(8) for a in action_list])\n",
    "    str_values     = '  '.join([f\"{q:.3f}\".ljust(8) for q in action_values])\n",
    "    print(f\"TEST: values at {r_agent}\")\n",
    "    print(f\"TEST: {str_actions}\")\n",
    "    print(f\"TEST: {str_values}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Identify priority state/action pairs\n",
    "###\n",
    "\n",
    "priority_states, priority_actions, priority_returns = [], [], []\n",
    "for action in action_list :\n",
    "    r_initial = r_end - action\n",
    "    if is_terminal(r_initial) : continue\n",
    "    reward, _ = perform_action(r_initial, action)\n",
    "    priority_states .append(r_initial / r_norm)\n",
    "    priority_actions.append(action)\n",
    "    priority_returns.append(reward)\n",
    "    \n",
    "priority_states  = np.array(priority_states )\n",
    "priority_actions = np.array(priority_actions)\n",
    "priority_returns = np.array(priority_returns)\n",
    "\n",
    "print(f\"Found {len(priority_returns)} priority state-action pairs with returns:  {'  '.join([f'{x:.2f}' for x in priority_returns])}\")\n",
    "\n",
    "def print_priority_values(q1_model, q2_model) :\n",
    "    model_args    = [priority_states, priority_actions]\n",
    "    q1_values     = q1_model.predict(model_args).flatten()\n",
    "    q2_values     = q2_model.predict(model_args).flatten()\n",
    "    str_returns   = '  '.join([f\"{q:.3f}\".ljust(8) for q in priority_returns])\n",
    "    str_q1_values = '  '.join([f\"{q:.3f}\".ljust(8) for q in q1_values])\n",
    "    str_q2_values = '  '.join([f\"{q:.3f}\".ljust(8) for q in q2_values])\n",
    "    print(f\"EXP: {str_returns}\")\n",
    "    print(f\"Q1 : {str_q1_values}\")\n",
    "    print(f\"Q2 : {str_q2_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d080463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(config_fname, q1_model, q2_model, to_stdout=True) :\n",
    "    '''\n",
    "    Print environment, training and model configurations to file config_fname. Also print environment and\n",
    "    training configurations to sys.stdout if requested, but do not print model summaries as they are verbose.\n",
    "    Inputs:\n",
    "      > config_fname, str\n",
    "        name of config file to create\n",
    "      > q1_model, keras Model\n",
    "        first q-value model\n",
    "      > q2_model, keras Model\n",
    "        second q-value model\n",
    "      > to_stdout, bool, default=True\n",
    "        if True then repeat environment and training configurations to sys.stdout\n",
    "    Returns:\n",
    "      > None\n",
    "    '''\n",
    "    # Create message as list of strings\n",
    "    config_message = []\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    config_message.append(f\"Environment config:\\n\")\n",
    "    config_message.append(f\"> horizontal_size: {horizontal_size}\\n\")\n",
    "    config_message.append(f\"> vertical_size: {vertical_size}\\n\")\n",
    "    config_message.append(f\"> horizontal_pad: {horizontal_pad}\\n\")\n",
    "    config_message.append(f\"> vertical_pad: {vertical_pad}\\n\")\n",
    "    config_message.append(f\"> horizontal_max: {horizontal_max}\\n\")\n",
    "    config_message.append(f\"> vertical_max: {vertical_max}\\n\")\n",
    "    config_message.append(f\"> reward_per_turn: {reward_per_turn:.6f}\\n\")\n",
    "    config_message.append(f\"> lambda_r: {lambda_r:.6f}\\n\")\n",
    "    config_message.append(f\"> lambda_b: {lambda_b:.6f}\\n\")\n",
    "    config_message.append(f\"> gamma: {gamma:.6f}\\n\")\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    config_message.append(f\"Training config:\\n\")\n",
    "    config_message.append(f\"> Using bootstrap method: {bootstrap_method}\\n\")\n",
    "    config_message.append(f\"> Using epochs of length {num_states}\\n\")\n",
    "    config_message.append(f\"> Updating gradient every batch of size {batch_size}\\n\")\n",
    "    config_message.append(f\"> Using optimizer_q1 {optimizer_q1} with learning rate {learning_rate:.6}\\n\")\n",
    "    config_message.append(f\"> Using optimizer_q2 {optimizer_q2} with learning rate {learning_rate:.6}\\n\")\n",
    "    config_message.append(f\"> Testing every {test_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Plotting policy every {plot_policy_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Plotting monitors every {plot_monitors_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Swapping q1 and q2 every {switch_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Cloning q2 from q1 every {clone_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Assigning a weight of {priority_weight} to anchoring state/action pairs\\n\")\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    # Make sure directory exists for file\n",
    "    generate_directory_for_file_path(config_fname, print_msg_on_dir_creation=True)\n",
    "    # Open file and print messages, also to stdout if configured\n",
    "    # - also print q-model summaries, only to file\n",
    "    with open(config_fname, \"w\") as config_file :\n",
    "        for line in config_message :\n",
    "            config_file.write(line)\n",
    "            if not to_stdout : continue\n",
    "            sys.stdout.write(line)\n",
    "        config_file.write(\"\\nModel configs:\\n\\n\")\n",
    "        q1_model.summary(print_fn=lambda x: config_file.write(x + '\\n'))\n",
    "        config_file.write(\"\\n\")\n",
    "        q2_model.summary(print_fn=lambda x: config_file.write(x + '\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb2b35",
   "metadata": {},
   "source": [
    "- Use two optimisers so momentum from q1 does not propagate onto q2 or vice versa. Downside is that momentum skips into the future. Also we do nothing to solve the instability which comes from a constantly shifting objective\n",
    "- Prefer to study rewards of \"-1 per turn\" instead of \"reward for moving towards goal\" because the first one is more generalisable and forces us to find techniques which optimise long-term return, whereas the second objective can be achieved by simply optimising immediate rewards and so it is not clear to what degree we are driven by long-term rather than short-term returns\n",
    "- currently learning without momentum (SGD) as I'm worried about the impact that momentum might have on encouraging divergence (could be studied)\n",
    "- What is happening when instantaneous divergences occur? Seems like things are going well then suddenly both batch loss and reference loss both explode\n",
    "- Even though we know all returns must be negative definite, we still use a linear output layer for the network so the initial returns are symmetric about 0, so initial bootstraps do not start propagating chains of negative numbers which may encourage divergence (could be studied)\n",
    "- Greedy policy exists in regions of aligned policies, reflecting smoothness of value function in (NxMx9) space\n",
    "- Not using validation split because don't want there to be state/action pairs which agent does not have access to during training\n",
    "- To what degree do we want each model to converge on the train data + current bootstrap target before we swap, thus updating the bootstrap target for the next model (and so on)\n",
    "- Idea to improve convergence?\n",
    "- Could get stable solution then study variations such as\n",
    "    - changing optimizer (momentum and learning rate)\n",
    "    - changing output layer (linear vs -1*relu)\n",
    "    - reduce number of batch updates before switching, so each model does not converge before being used as bootstrap target\n",
    "    - priority weight high vs low\n",
    "    - -1 per turn vs dr per turn\n",
    "    - small NN model\n",
    "    - return clipping to reduce divergence\n",
    "    - with/without boundary penalty\n",
    "- Could derive optimal value function and use as absolute measure of quality\n",
    "- Idea: form a validation set of state/action pairs and track their q-values to study divergence. E.g. all 9 actions at five different states (2 on boundary, 1 near middle)\n",
    "- Idea: monitoring plot: mean q(s,a) across a for every q (maybe as heatmap, see how value function is evolving across state space. Suspect divergence driven by uniform raising of expected values leading to runaway process, not a slow development of a cone with a peak-value around r_end and more negative values concentrically around it.\n",
    "- Use true DQN/double-Q method and freeze the same network for the bootstraps?\n",
    "- Idea: Could make simple 1D function update example to show where divergence comes from, and study how to control it. E.g. line walk with boundary (same as current example but in 1D, actions are L or R)\n",
    "- Important note: previous experiments impacted by bug in which y was profiled up to horizontal_max instead of vertical_max, ruining everything\n",
    "- I think using two networks increases variance but is less biased\n",
    "- Some kind of adaptive learning_rate would be nice\n",
    "- Expect high variance to trigger divergence because we increase the distance between the function estimates and the truth\n",
    "- Whether using two networks or a frozen network, having two players inherently causes instability because of the constantly shifting objective function, making it difficult to converge because each time we switch bootstrap function we change the objective (could be mitigated with slower learning?)\n",
    "- We do not prioritise accuracy of states near to finish, and these get pulled around by nearby datapoints due to function approximation. These instabilities then propagate through the state space to \"earlier\" states, because we are using bootstrapping. If these final states never converge, then the function may constantly fluctuate-and-propagate forever without converging towards a stable solution. This is not necessarily a variance issue, rather that the function approximation allows all states to pull on all other ones, although variance may also contribute. Maybe solution is to assign high weight to priority states? This reduces learning rate elsewhere but may more effectively anchor the function in these important states, so the information which propagates back to other states (through bootstrapping) is good information. It does not fully solve the problem though, because functional approximations still lead to propagating mis-information in the rest of the state space. I expect that the further you get from the terminal state, the worse this problem becomes, because the amount of misinformation compounds over distance. When we compare the action-value-estimates at each epoch with the max-Q at that stage in training, we see that the final few states are fluctuating a lot. I don't see how this problem can be avoided without curating a training schedule to focus on and freeze states in a custom manner, which requires even more expert knowledge (on top of the priority state definitions already provided) when defeats the whole purpose.\n",
    "- Why does off-policy behaviour contribute to divergence? I guess we are biasing the distribution of visited state-action pairs to give too much weight to actions which are not important, forcing the q-model to focus on these. Maybe if we behaved on-policy then we could find a steady-state solution which, although not necessarily exact (still using function approximation), is at least convergent?\n",
    "- Bootstrapping does not do anything to correct the functional dependence which is already there. E.g. if the true q-function has a positive gradient, and the estimate has a negative gradient, then bootstrapping will continue to provide q-models with a negative gradient - unless we use a tailored reward signal (which has the two negative effects (i) introduces expert knowledge and (ii) biases the learned policy towards those the expert already knows). If we use a reward signal of e.g. '-1 per turn', the only learning pressure towards correcting the functional form comes from the states which do not bootstrap and so must be fixed in scale to their immediate reward, i.e. those next to terminal states. However for large state spaces, without priority weighting, these state-action pairs only carry a small weight and so provide a very small learning pressure. This means that the larger pressure - to reinforce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09635bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Configure\n",
    "\n",
    "num_epochs                 = -1\n",
    "batch_size                 = 50\n",
    "learning_rate              = 1e-3\n",
    "plot_policy_after_epochs   = 50\n",
    "plot_monitors_after_epochs = 50\n",
    "switch_after_epochs        = -1\n",
    "clone_after_epochs         = 1\n",
    "test_after_epochs          = 50\n",
    "priority_weight            = 0.3\n",
    "bootstrap_method           = \"clone\"       # [\"clone\", \"self\", \"other\"]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Set up\n",
    "\n",
    "loss_fcn     = tf.keras.losses.MeanSquaredError()\n",
    "optimizer_q1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer_q2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "q1_model     = create_action_value_model(name=\"action_value_model_1\")\n",
    "q2_model     = q1_model if bootstrap_method == \"self\" else create_action_value_model(name=\"action_value_model_2\")\n",
    "states       = []\n",
    "for x in range(horizontal_max) :  # range(r_end[0]-2, r_end[0]+3) :       # range(horizontal_max) : \n",
    "    for y in range(vertical_max) :  # range(r_end[1]-2, r_end[1]+3) :   # range(horizontal_max) : \n",
    "        r_agent = np.array([x,y])\n",
    "        if is_terminal(r_agent) : continue\n",
    "        states.append( r_agent / r_norm )\n",
    "num_states = len(states)\n",
    "if bootstrap_method not in [\"clone\", \"self\", \"other\"] :\n",
    "    raise NotImplementedError(f\"Bootstrap method {bootstrap_method} not implemented\")\n",
    "\n",
    "\n",
    "## Print config to file and screen (model summaries only to file because they are verbose)\n",
    "create_config(\"figures/Helicopter_NB1/config.txt\", q1_model, q2_model, to_stdout=True)\n",
    "\n",
    "\n",
    "## Start training\n",
    "\n",
    "start_time = time.time()\n",
    "model_key_q1, model_key_q2 = \"Q1\", \"Q2\"  # used to keep track of which model is being traing each epoch\n",
    "loss_record, ref_loss_record, maxQ_record = {\"Q1\":[], \"Q2\":[]}, {\"Q1\":[], \"Q2\":[]}, {\"Q1\":[], \"Q2\":[]}\n",
    "\n",
    "epoch_idx = 0\n",
    "while epoch_idx < num_epochs or num_epochs < 0 :\n",
    "    \n",
    "    # Determine whether to print test information\n",
    "    if test_after_epochs > 0 and epoch_idx % test_after_epochs == 0 :\n",
    "        #test_models(r_start, q1_model, q2_model)\n",
    "        #test_models(r_end + np.array([-1,1]), q1_model, q2_model)\n",
    "        print_priority_values(q1_model, q2_model)\n",
    "    \n",
    "    # Determine whether to plot current greedy policy\n",
    "    if plot_policy_after_epochs > 0 and epoch_idx % plot_policy_after_epochs == 0 :\n",
    "        create_greedy_policy_plot(q1_model, q2_model, epoch_idx=epoch_idx, verbose=True, show=True, close=True,\n",
    "                                  do_value_function=True,\n",
    "                                  save=f\"figures/Helicopter_NB1/greedy_policy_epoch{epoch_idx}.pdf\")\n",
    "    \n",
    "    # Determine whether to plot training curves\n",
    "    if plot_monitors_after_epochs > 0 and epoch_idx > 0 and epoch_idx % plot_monitors_after_epochs == 0 :\n",
    "        create_training_curves_plot(loss_record, ref_loss_record, maxQ_record, show=True, close=True,\n",
    "                                    save=\"figures/Helicopter_NB1/training_curves.pdf\")\n",
    "        \n",
    "    # Determine whether to switch q1 and q2\n",
    "    # - keeping this irregular reduces the risk of feedback function<->bootstrap loops which drive divergence\n",
    "    if bootstrap_method == \"other\" and switch_after_epochs > 0 and epoch_idx > 0 and epoch_idx % switch_after_epochs == 0 :\n",
    "        model_key_q1, model_key_q2 = model_key_q2, model_key_q1\n",
    "        q1_model, q2_model         = q2_model, q1_model\n",
    "        optimizer_q1, optimizer_q2 = optimizer_q2, optimizer_q1\n",
    "        \n",
    "    # Determine whether to copy q1 to q2\n",
    "    if bootstrap_method == \"clone\" and clone_after_epochs > 0 and epoch_idx % clone_after_epochs == 0 :\n",
    "        q2_model.set_weights(q1_model.get_weights()) \n",
    "    \n",
    "    sys.stdout.write(f\"Epoch {epoch_idx+1} / {num_epochs}  [t={time.time()-start_time:.2f}s]\")\n",
    "    \n",
    "    # Shuffle states and loop over batches, performing one gradient update per batch\n",
    "    # - this has lower variance than applying one gradient update per sample, and also allows parallelisation\n",
    "    np.random.shuffle(states)\n",
    "    epoch_losses, ref_losses, max_abs_q_values = [], [], []\n",
    "    for batch_idx in range(math.ceil(num_states/batch_size)) :\n",
    "        # Resolve sample indices to be used for this batch update\n",
    "        batch_idx_low, batch_idx_high = batch_idx*batch_size, min((batch_idx+1)*batch_size, num_states)\n",
    "        actual_batch_size = batch_idx_high - batch_idx_low\n",
    "        if actual_batch_size == 0 : continue\n",
    "        \n",
    "        # Update the current epoch message to keep track of batch progress \n",
    "        sys.stdout.write(f\"\\rEpoch {epoch_idx+1} / {num_epochs} batch indices ({batch_idx_low}, {batch_idx_high}) / {num_states}  [t={time.time()-start_time:.2f}s]\")\n",
    "        \n",
    "        # Get batch of states and generate a random exploration action for each\n",
    "        r_agents = np.array(states[batch_idx_low:batch_idx_high])\n",
    "        actions  = get_exploration_action(num=actual_batch_size)\n",
    "        \n",
    "        # For each state/action pair, apply the action to get the immediate reward, and also find the\n",
    "        # greedy action in the next state from which to calculate bootstraps\n",
    "        rewards, r_agents_p, actions_p, agent_p_is_terminal = [], [], [], []\n",
    "        for r_agent, action in zip(r_agents, actions) :\n",
    "            reward, r_agent_p = perform_action(r_agent, action, agent_is_normed=True)\n",
    "            agent_p_is_terminal.append(True if is_terminal(r_agent_p) else False)\n",
    "            action_p, _ = get_greedy_action(r_agent_p, q1_model)\n",
    "            r_agent_p /= r_norm\n",
    "            rewards   .append(reward)\n",
    "            r_agents_p.append(r_agent_p)\n",
    "            actions_p .append(action_p)\n",
    "        rewards, r_agents_p, actions_p = np.array(rewards), np.array(r_agents_p), np.array(actions_p)\n",
    "        \n",
    "        # Calculate obs_returns using the observed immediate rewards plut gamma * bootstrap values\n",
    "        bootstrap_values = q2_model.predict([r_agents_p, actions_p]).flatten()\n",
    "        bootstrap_values = np.array([0. if is_term else q for is_term,q in zip(agent_p_is_terminal,bootstrap_values)])\n",
    "        obs_returns      = rewards + gamma * bootstrap_values\n",
    "        \n",
    "        # Calculate weights to apply to regular batch and priority samples\n",
    "        sample_weights   = (1.-priority_weight) * np.full(shape=(len(r_agents),), fill_value=1./len(r_agents))\n",
    "        priority_weights = priority_weight * np.full(shape=(len(priority_states),), fill_value=1./len(priority_states))\n",
    "        \n",
    "        # Concatenate regular batch and priority samples\n",
    "        if priority_weight > 0 and priority_weight <= 1 :\n",
    "            sample_weights   = np.concatenate([sample_weights , priority_weights]).flatten()\n",
    "            r_agents         = np.concatenate([r_agents       , priority_states ])\n",
    "            actions          = np.concatenate([actions        , priority_actions])\n",
    "            obs_returns      = np.concatenate([obs_returns    , priority_returns]).flatten()\n",
    "        \n",
    "        # When using sample weights, we have to be careful with the object shapes. The loss function will\n",
    "        # expect y_pred of shape (N,1) and y_true of shape (N,1), when the output shape is (1,), and sample\n",
    "        # weights of shape (N,). When not using sample weights we can get away with being lazy on the shapes\n",
    "        # of y_pred and y_true, but if we do this here then it will not correctly apply the correct sample\n",
    "        # weight to the correct sample. Furthermore, MSE with sample weights will calculate mean(sw*res**2)\n",
    "        # rather than normalising according to sum(sw*res**2)/sum(sw). This means we must also multiply sw \n",
    "        # by a factor of len(sw)/sum(sw) if we are to recover the MSE value that we expect. Note that we have\n",
    "        # sum(sw)=1 by our construction above, but I still write it explicitly so the general idea is clear.\n",
    "        sample_weights = sample_weights * len(sample_weights) / sample_weights.sum()\n",
    "        obs_returns    = obs_returns.reshape((len(obs_returns),1))\n",
    "        \n",
    "        # ref_loss is the loss over only the priority state/action pairs, before gradient updates for\n",
    "        # consistency with regular loss record. Since regular loss mixes (i) how close the priority samples\n",
    "        # are to their correct values and (ii) how close other points are to their bootstrap-biased values,\n",
    "        # the ref_loss removes the bootstrap samples so we can see whether these points are diverging\n",
    "        ref_returns = q1_model([priority_states, priority_actions], training=False)\n",
    "        ref_loss    = loss_fcn(priority_returns.reshape((len(ref_returns),1)), ref_returns.numpy())\n",
    "        ref_losses.append(ref_loss)\n",
    "                                \n",
    "        # Apply gradient updates\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_returns     = q1_model([r_agents, actions], training=True)\n",
    "            loss_value       = loss_fcn(obs_returns, pred_returns, sample_weight=sample_weights)\n",
    "            grads            = tape.gradient(loss_value, q1_model.trainable_weights)\n",
    "            optimizer_q1.apply_gradients(zip(grads, q1_model.trainable_weights))\n",
    "            epoch_losses.append(loss_value.numpy())\n",
    "            np_returns = np.fabs(pred_returns.numpy().flatten())\n",
    "            max_q_idx  = np.argmax(np_returns)\n",
    "            #sys.stdout.write(f\"\\nMax batch return at index {max_q_idx} state {r_agents[max_q_idx]*r_norm} action {actions[max_q_idx]} with q-value {np_returns[max_q_idx]:.1f}\\n\")\n",
    "            max_abs_q_values.append(np_returns.max())\n",
    "                                \n",
    "    # Print epoch summary and end stdout line to keep this on screen\n",
    "    epoch_mean_loss, epoch_mean_ref_loss, epoch_maxQ = np.mean(epoch_losses), np.mean(ref_losses), np.max(max_abs_q_values)\n",
    "    sys.stdout.write(f\"\\rEpoch {epoch_idx+1} / {num_epochs}  [t={time.time()-start_time:.2f}s]  <loss = {epoch_mean_loss:.5f}, ref_loss = {epoch_mean_ref_loss:.5f}, max_Q = {epoch_maxQ:.1f}>\".ljust(100)+\"\\n\")\n",
    "    loss_record    [model_key_q1].append((epoch_idx, epoch_mean_loss    ))\n",
    "    ref_loss_record[model_key_q1].append((epoch_idx, epoch_mean_ref_loss))\n",
    "    maxQ_record    [model_key_q1].append((epoch_idx, epoch_maxQ         ))\n",
    "                              \n",
    "    # Manually iterate epoch index and make sure stdout not lagging\n",
    "    epoch_idx += 1\n",
    "    sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d32573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec9bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
