{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643c6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ste/miniforge3/envs/tf-sandbox-py3p9/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has found devices:\n",
      "-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "#  Required imports\n",
    "\n",
    "import math, os, sys, time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Concatenate, Dense, Dropout, Flatten, Input, MaxPooling2D, Rescaling\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"TensorFlow has found devices:\")\n",
    "for device in tf.config.list_physical_devices() :\n",
    "    print(f\"-  {device}\")\n",
    "    \n",
    "#  Use non-interactive backend to prevent memory leak from creation and non-deletion of background GUI objects\n",
    "#  > interactive backends such as 'inline' cause plt.close(fig) to not release memory, causing massive data leak\n",
    "#  > plt.clf() followed by plt.close(fig) still works okay but still small data leak from GUI objects\n",
    "#  > non-interactive backend allows plt.close(fig) to work normally with no leak, beacuse no GUI objects are\n",
    "#    created (this means we cannot use things like plt.show(fig))\n",
    "%matplotlib agg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6fd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Global constants\n",
    "###\n",
    "\n",
    "#  Initially we will just run on a game board of fixed size, to avoid building an architecture to handle \n",
    "#  variable board sizes, so let's configure this here\n",
    "\n",
    "horizontal_size = 8\n",
    "vertical_size   = 5\n",
    "horizontal_pad  = 2\n",
    "vertical_pad    = 2\n",
    "horizontal_max  = horizontal_size + 2*horizontal_pad\n",
    "vertical_max    = vertical_size   + 2*vertical_pad\n",
    "r_norm          = max(horizontal_max, vertical_max)\n",
    "storm_radius    = 2.\n",
    "num_storms      = 2\n",
    "\n",
    "reward_per_turn = -1.\n",
    "lambda_r        = 0.\n",
    "lambda_b        = -2.\n",
    "lambda_w        = -15.\n",
    "gamma           = .98\n",
    "\n",
    "r_start = np.array([horizontal_pad, vertical_pad+vertical_size-1])\n",
    "r_end   = np.array([horizontal_pad+horizontal_size-1, vertical_pad]) \n",
    "\n",
    "action_list = np.array([[-1, -1], [-1, 0], [-1, 1], [0, -1], [0, 0], [0, 1], [1, -1], [1, 0], [1, 1]])\n",
    "\n",
    "print(f\"r_start = {r_start}\")\n",
    "print(f\"r_end   = {r_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b030b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Define and unit-test methods used to calculate Lp measures and norms\n",
    "###\n",
    "\n",
    "\n",
    "def Lp_norm(v, p=2) :\n",
    "    '''\n",
    "    Calculate Lp norm of vector v, defined as [sum_i v_i^p]^(1/p). If np.isfinite(p) returns False then\n",
    "    calculate the L-infinity norm, which just returns the highest mod-vector-component.\n",
    "    Inputs:\n",
    "      > v, np.ndarray of any shape >= 1D\n",
    "        vector to calculate the norm of\n",
    "      > p, float, default=2\n",
    "        exponent of the Lp norm\n",
    "    Return:\n",
    "      > float, the Lp norm\n",
    "    '''\n",
    "    ##  If type(v) is not numpy array then try to cast it to one\n",
    "    if type(v) != np.ndarray :\n",
    "        v = np.array(v)\n",
    "    ##  Return L-infinity norm if p is not a real number\n",
    "    if not np.isfinite(p) :\n",
    "        return np.fabs(v).max()\n",
    "    ##  Use np.power method to return Lp norm if p is finite\n",
    "    return np.power(np.power(v, p).sum(), 1./p)\n",
    "  \n",
    "    \n",
    "def Lp_distance(v1, v2, p=2) :\n",
    "    '''\n",
    "    Calculate Lp distance between vectors v1 and v2 by calling Lp_norm(v2-v1).\n",
    "    Inputs:\n",
    "      > v1, np.ndarray of any shape >= 1D\n",
    "        first vector\n",
    "      > v2, np.ndarray of same shape as v1\n",
    "        second vector\n",
    "      > p, float, default=2\n",
    "        exponent of the Lp-distance\n",
    "    Return:\n",
    "      > float, the Lp distance\n",
    "    '''\n",
    "    ##  If v1 or v2 are not numpy arrays then try to cast them\n",
    "    if type(v1) != np.ndarray :\n",
    "        v1 = np.array(v1)\n",
    "    if type(v2) != np.ndarray :\n",
    "        v2 = np.array(v2)\n",
    "    ##  Return the norm of the difference between v1 and v2\n",
    "    return Lp_norm(v2 - v1, p=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15affde",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Define and unit-test environment methods\n",
    "###\n",
    "\n",
    "\n",
    "def create_weather_map(_num_storms=num_storms, _storm_radius=storm_radius) :\n",
    "    x_storms = np.random.uniform(low=0, high=horizontal_max-1, size=(_num_storms,))\n",
    "    y_storms = np.random.uniform(low=0, high=vertical_max  -1, size=(_num_storms,))\n",
    "    r_storms = np.array([x_storms, y_storms]).transpose()\n",
    "    weather_map = np.zeros(shape=(horizontal_max, vertical_max, 1))\n",
    "    for x in range(horizontal_max) :\n",
    "        for y in range(vertical_max) :\n",
    "            r = np.array([x,y])\n",
    "            intensity = 0\n",
    "            for x_storm, y_storm in zip(x_storms, y_storms) :\n",
    "                r_storm    = np.array([x_storm,y_storm])\n",
    "                dr         = Lp_distance(r,r_storm)\n",
    "                intensity += 1. / (1 + dr/_storm_radius)\n",
    "            weather_map[x, y, 0] = intensity\n",
    "    return weather_map\n",
    "\n",
    "\n",
    "def perform_action(weather_map, r_agent, action, base_reward=reward_per_turn, boundary_reward=lambda_b, \n",
    "                   dr_reward_factor=lambda_r, w_reward_factor=lambda_w, verbose=False) :\n",
    "    '''\n",
    "    Given the current environment and agent states, perform the specified action and return the reward \n",
    "    obtaine along with the new agent state.\n",
    "    Inputs:\n",
    "      > weather_map, np.ndarray of shape (horizontal_max, vertical_max, 1)\n",
    "        intensity of weather at every pixel in the map\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        (x,y) position of agent at initial timestep\n",
    "      > action, np.ndarray of shape (2,)\n",
    "        (dx,dy) of action to be performed, each component expected to be one of {-1, 0, +1}\n",
    "      > base_reward, float, default=-2.\n",
    "        basic reward returned every turn (expected -ve)\n",
    "      > boundary_reward, float, default=lambda_b\n",
    "        reward received when encountering the edge of the game board (expected -ve)\n",
    "      > dr_reward_factor, float, default=lambda_r\n",
    "        factor multiplied by change-in-distance to calculate movement reward (expected +ve)\n",
    "    Returns:\n",
    "      > float\n",
    "        reward obtained by performing action\n",
    "      > np.ndarray of shape (2,)\n",
    "        (x,y) position of agent at iterated timestep\n",
    "    '''\n",
    "    ##  Make sure initial state is valid to protect against unexpected behaviour\n",
    "    if is_terminal(r_agent) :\n",
    "        raise RuntimeError(f\"Agent state is terminal, so no actions may be performed\")\n",
    "    if is_out_of_bounds(r_agent) :\n",
    "        raise RuntimeError(f\"Agent state {r_agent} is out of bounds, so no actions may be performed\")\n",
    "    ##  Get initial distance of agent from the end\n",
    "    d_agent = Lp_distance(r_agent, r_end)\n",
    "    ##  Iterate agent position\n",
    "    ##  - if agent hits a wall then add an appropriate penalty and return agent to original position\n",
    "    r_agent_p = r_agent + action\n",
    "    reward_b  = 0\n",
    "    if r_agent_p[0] < 0 or r_agent_p[0] >= horizontal_max or r_agent_p[1] < 0 or r_agent_p[1] >= vertical_max :\n",
    "        reward_b  = boundary_reward\n",
    "        r_agent_p = r_agent.copy()\n",
    "    ##  Get distance-based reward\n",
    "    d_agent_p = Lp_distance(r_agent_p, r_end)\n",
    "    reward_r  = dr_reward_factor * (d_agent - d_agent_p) / np.sqrt(2)\n",
    "    ##  Get weather-based reward\n",
    "    reward_w  = w_reward_factor * weather_map[int(r_agent[0]), int(r_agent[1]), 0]\n",
    "    ##  Calculate total reward by summing the base, boundary, distance and weather rewards\n",
    "    reward = 0. if is_terminal(r_agent_p) else base_reward + reward_b + reward_r + reward_w\n",
    "    if verbose :\n",
    "        print(f\"perform_action: agent {r_agent} action {action} --> agent {r_agent_p} reward {reward:.2f}  [{base_reward:.2f} (base) + {reward_b:.2f} (b) + {reward_r:.2f} (r) + {reward_w:.2f} (w)]\")\n",
    "    ##  Return reward and new agent state\n",
    "    return reward, r_agent_p\n",
    "\n",
    "\n",
    "def is_terminal(r_agent) :\n",
    "    '''\n",
    "    Return True if the agent is in the terminal state and False otherwise.\n",
    "    Inputs:\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        agent position as (x,y)-coordinates\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether the agent is in the terminal state\n",
    "    '''\n",
    "    if r_agent[0] == r_end[0] and r_agent[1] == r_end[1] :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_out_of_bounds(r_agent) :\n",
    "    '''\n",
    "    Return True if the agent isout of bounds and False otherwise.\n",
    "    Inputs:\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        agent position as (x,y)-coordinates\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether the agent is out of bounds.\n",
    "    '''\n",
    "    if r_agent[0] <  0 : return True\n",
    "    if r_agent[1] <  0 : return True\n",
    "    if r_agent[0] >= horizontal_max : return True\n",
    "    if r_agent[1] >= vertical_max   : return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def get_greedy_action(weather_map, r_agent, *q_models) :\n",
    "    '''\n",
    "    Sample a greedy action from the Q-value models provided. If multiple models provided then use their mean.\n",
    "    Inputs:\n",
    "      > weather_map, np.ndarray of shape (horizontal_max, vertical_max)\n",
    "        intensity of weather at every pixel in the map\n",
    "      > r_agent, np.ndarray of shape (2,)\n",
    "        agent position as (x,y)-coordinates\n",
    "      > q_models, list of tf.keras Model class, each with inputs [agent position, action] = [Input(2), Input(2)]\n",
    "        list of Keras Q(s,a) models\n",
    "    Returns:\n",
    "      > np.ndarray of shape (2,)\n",
    "        action defined by greedy policy over the model(s) at this agent position\n",
    "      > list of np.ndarray of shape (9,)\n",
    "        action values in the same order as action_list, in list of models provided\n",
    "    '''\n",
    "    weather_maps        = np.array([weather_map for i in range(9)])\n",
    "    r_agents            = np.array([r_agent for i in range(9)])\n",
    "    model_args          = [weather_maps, r_agents, action_list]\n",
    "    model_action_values = [model.predict(model_args) for model in q_models]\n",
    "    action_values       = np.mean(model_action_values, axis=0)\n",
    "    best_action         = action_list[np.argmax(action_values)]\n",
    "    return best_action, model_action_values\n",
    "\n",
    "\n",
    "def get_exploration_action(num=1) :\n",
    "    '''\n",
    "    Generate uniformly random actions from the 9 available.\n",
    "    Input:\n",
    "      > num, int, default=1\n",
    "        number of random actions to generate\n",
    "    Returns :\n",
    "      > np.ndarray of size (num,2)\n",
    "        list of actions generated\n",
    "    '''\n",
    "    return action_list[np.random.randint(low=0, high=8, size=(num,))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8475c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Method for creating action-value model\n",
    "###\n",
    "\n",
    "def create_action_value_model(name=None) :\n",
    "    '''\n",
    "    Create a network for the action-value model.\n",
    "    Inputs:\n",
    "      > name, str, default=None\n",
    "        model name, if None then keras default is used\n",
    "    Returns:\n",
    "      > keras Model: uncompiled keras model (must be trained using custom loop)\n",
    "    '''\n",
    "    input_layer_w = Input((horizontal_max, vertical_max, 1))\n",
    "    input_layer_r = Input((2,))\n",
    "    input_layer_a = Input((2,))\n",
    "    \n",
    "    next_layer_w  = Conv2D(20, kernel_size=(2,2), activation=\"relu\")(input_layer_w)\n",
    "    next_layer_w  = MaxPooling2D(pool_size=(2,2))(next_layer_w)\n",
    "    next_layer_w  = BatchNormalization()(next_layer_w)\n",
    "    next_layer_w  = Conv2D(20, kernel_size=(2,2), activation=\"relu\")(next_layer_w)\n",
    "    next_layer_w  = Flatten()(next_layer_w)\n",
    "    next_layer_w  = BatchNormalization()(next_layer_w)\n",
    "    next_layer_w  = Dense(50, activation=\"relu\")(next_layer_w)\n",
    "    next_layer_w  = BatchNormalization()(next_layer_w)\n",
    "    \n",
    "    next_layer_r  = Rescaling(scale=2./r_norm, offset=-0.5*r_norm)(input_layer_r)\n",
    "    next_layer_r  = Dense(50, activation=\"relu\")(next_layer_r)\n",
    "    next_layer_r  = BatchNormalization()(next_layer_r)\n",
    "    \n",
    "    next_layer_a  = Dense(50, activation=\"relu\")(input_layer_a)\n",
    "    next_layer_a  = BatchNormalization()(next_layer_a)\n",
    "    \n",
    "    next_layer    = Concatenate()([next_layer_w, next_layer_r, next_layer_a])\n",
    "    next_layer    = Dense(500, activation=\"relu\")(next_layer)\n",
    "    next_layer    = BatchNormalization()(next_layer)\n",
    "    next_layer    = Dense(500, activation=\"linear\")(next_layer)\n",
    "    output_layer  = Dense(1, activation=\"linear\")(next_layer)\n",
    "    model         = Model([input_layer_w, input_layer_r, input_layer_a], output_layer, name=name)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directory_for_file_path(fname, print_msg_on_dir_creation=True) :\n",
    "    \"\"\"\n",
    "    Create the directory structure needed to place file fname. Call this before fig.savefig(fname, ...) to \n",
    "    make sure fname can be created without a FileNotFoundError\n",
    "    Input:\n",
    "       - fname: str\n",
    "                name of file you want to create a tree of directories to enclose\n",
    "                also create directory at this path if fname ends in '/'\n",
    "       - print_msg_on_dir_creation: bool, default = True\n",
    "                                    if True then print a message whenever a new directory is created\n",
    "    \"\"\"\n",
    "    while \"//\" in fname :\n",
    "        fname = fname.replace(\"//\", \"/\")\n",
    "    dir_tree = fname.split(\"/\")\n",
    "    dir_tree = [\"/\".join(dir_tree[:i]) for i in range(1,len(dir_tree))]\n",
    "    dir_path = \"\"\n",
    "    for dir_path in dir_tree :\n",
    "        if len(dir_path) == 0 : continue\n",
    "        if not os.path.exists(dir_path) :\n",
    "            os.mkdir(dir_path)\n",
    "            if print_msg_on_dir_creation :\n",
    "                print(f\"Directory {dir_path} created\")\n",
    "            continue\n",
    "        if os.path.isdir(dir_path) : \n",
    "            continue\n",
    "        raise RuntimeError(f\"Cannot create directory {dir_path} because it already exists and is not a directory\")\n",
    "    \n",
    "    \n",
    "def create_greedy_policy_plot(weather_map, *q_models, epoch_idx=-1, verbose=False, show=False, close=False, save=\"\") :\n",
    "    '''\n",
    "    Create a plt.Figure instance visualising the greedy policy defined by the average of the q-value models \n",
    "    provided. Allows for plot to be shown, saved and/or closed using plt interface. Returns the plot figure\n",
    "    and axis objects so they can continue to be manipulated, but note that objects will no longer be in scope\n",
    "    if we have called plt.close(fig).\n",
    "    Inputs:\n",
    "      > weather_map, np.ndarray of size (horizontal_max, vertical_max)\n",
    "        weather intensity at every state\n",
    "      > q_models, list of keras Model class\n",
    "        list of q-value models to define the greedy policy\n",
    "      > epoch_idx, int, default=-1\n",
    "        if positive then draw a text box displaying how many epochs have been performed\n",
    "      > verbose, bool, default=False\n",
    "        if True then print some text to display progress as we evaluate the models for every state/action pair\n",
    "      > show, bool, default=False\n",
    "        if True then call plt.show(fig)\n",
    "      > close, bool, default=False\n",
    "        if True then call plt.close(fig)\n",
    "      > save, str, default=\"\"\n",
    "        if string provided then call fig.savefig(save, ...), creating any required subdirectories if needed\n",
    "    Returns:\n",
    "      > plt.Figure instance\n",
    "      > plt.Axes instance\n",
    "    '''\n",
    "     \n",
    "    #  Keep track of how long plotting takes, to help inform how often to call this function    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #  Set up plot\n",
    "    fig = plt.figure(figsize=(12*horizontal_max/11.5,12*vertical_max/11.5))\n",
    "    ax  = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xlim(-0.5, horizontal_max-0.5)\n",
    "    ax.set_ylim(-0.5, vertical_max-0.5)\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=16)\n",
    "       \n",
    "    #  Draw weather!\n",
    "    ax.imshow(weather_map[:,:,0].transpose(), origin=\"lower\", alpha=0.5, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "    \n",
    "    #  Draw arrows by looping over states and finding greedy action according to q-models\n",
    "    is_first_arrow = True\n",
    "    for x in range(horizontal_max) :\n",
    "        for y in range(vertical_max) :\n",
    "            if x == r_end[0] and y == r_end[1] : continue\n",
    "            if verbose :\n",
    "                sys.stdout.write(f\"\\rEvaluating greedy policy for agent state ({x}, {y})\".ljust(100))\n",
    "            r_agent      = np.array([x,y])\n",
    "            (dx, dy), qs = get_greedy_action(weather_map, r_agent, *q_models)\n",
    "            q1s, q2s     = qs\n",
    "            if dx == 0 and dy == 0 :\n",
    "                ax.plot(x, y, \"o\", markersize=8, c=\"b\", alpha=1)\n",
    "            else :\n",
    "                ax.arrow(x - 0.3*dx, y - 0.3*dy, 0.6*dx, 0.6*dy, head_width=0.25, length_includes_head=True, color=\"b\")\n",
    "                is_first_arrow = False\n",
    "                \n",
    "    #  Draw accompanying plot objects\n",
    "    ax.fill_between([r_start[0]-0.5, r_start[0]+0.5], r_start[1]-0.5, r_start[1]+0.5, color=\"g\", alpha=0.2, label=\"Start\")\n",
    "    ax.fill_between([r_end  [0]-0.5, r_end  [0]+0.5], r_end  [1]-0.5, r_end  [1]+0.5, color=\"r\", alpha=0.2, label=\"Finish\")\n",
    "    ax.legend(loc=(0.,1.002), ncol=3, fontsize=16, frameon=False)\n",
    "    \n",
    "    #  Draw text boxes displaying title and num. epochs\n",
    "    ax.text(0, 1.07, \"Weather intensity and greedy policy per $s$\", transform=ax.transAxes, \n",
    "            fontsize=18, weight=\"bold\", ha=\"left\", va=\"bottom\")\n",
    "    if epoch_idx >= 0 :\n",
    "        ax.text(1, 1.01, f\"After {epoch_idx} epochs\", ha=\"right\", va=\"bottom\", weight=\"bold\", \n",
    "                transform=ax.transAxes, fontsize=16)\n",
    "    \n",
    "    #  Verbose messaging\n",
    "    '''if verbose :\n",
    "        sys.stdout.write(f\"\\nPlot created in {time.time()-start_time:.2f}s\".ljust(100)+\"\\n\")'''\n",
    "       \n",
    "    #  Save / show / close\n",
    "    if len(save) > 0 :\n",
    "        generate_directory_for_file_path(save)\n",
    "        plt.savefig(save, bbox_inches=\"tight\")\n",
    "    if show :\n",
    "        plt.show(fig)\n",
    "    if close :\n",
    "        plt.clf()\n",
    "        plt.close(fig)\n",
    "        \n",
    "    #  Return figure and axis\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def draw_training_curve(ax, container, m, c, label) :\n",
    "    ax.plot([x for x,y in container], [y for x,y in container], m, ms=7, c=c, alpha=1.0, label=label)\n",
    "    for ((x1,y1), (x2,y2)) in zip(container[:-1], container[1:]) :\n",
    "        if np.fabs(x2-x1) > 1.5 : continue\n",
    "        ax.plot([x1, x2], [y1, y2], \"-\", c=c, lw=2, alpha=0.7)\n",
    "            \n",
    "            \n",
    "def create_training_curves_plot(loss_record, ref_loss_record, maxQ_record, show=False, close=False, save=\"\") :\n",
    "    '''\n",
    "    Create a plt.Figure instance visualising the training curves. Allows for plot to be shown, saved and/or \n",
    "    closed using plt interface. Returns the plot figure and axis objects so they can continue to be \n",
    "    manipulated, but note that objects will no longer be in scope if we have called plt.close(fig).\n",
    "    Inputs:\n",
    "      > q_models, list of keras Model class\n",
    "        list of q-value models to define the greedy policy\n",
    "      > verbose, bool, default=False\n",
    "        if True then print some text to display progress as we evaluate the models for every state/action pair\n",
    "      > show, bool, default=False\n",
    "        if True then call plt.show(fig)\n",
    "      > close, bool, default=False\n",
    "        if True then call plt.close(fig)\n",
    "      > save, str, default=\"\"\n",
    "        if string provided then call fig.savefig(save, ...), creating any required subdirectories if needed\n",
    "    Returns:\n",
    "      > plt.Figure instance\n",
    "      > plt.Axes instance (axis corresponding to loss curves)\n",
    "      > plt.Axes instance (axis corresponding to ref_loss curves)\n",
    "      > plt.Axes instance (axis corresponding to maxQ curves)\n",
    "    '''\n",
    "            \n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    \n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax1.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax1.set_title(r\"Mean loss per batch [$(1-\\lambda)\\cdot$batch + $\\lambda\\cdot$ref]\", fontsize=30)\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    draw_training_curve(ax1, loss_record[\"Q1\"], \"o\", \"r\", \"$q_1$\")\n",
    "    draw_training_curve(ax1, loss_record[\"Q2\"], \"x\", \"b\", \"$q_2$\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.legend(loc=(0,1.02), fontsize=30, ncol=3, title_fontsize=30)\n",
    "    \n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax2.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax2.set_title(r\"Mean loss per batch [ref only]\", fontsize=30)\n",
    "    ax2.xaxis.set_ticklabels([])\n",
    "    draw_training_curve(ax2, ref_loss_record[\"Q1\"], \"o\", \"r\", \"$q_1$\")\n",
    "    draw_training_curve(ax2, ref_loss_record[\"Q2\"], \"x\", \"b\", \"$q_2$\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax3.set_title(r\"Max $|q(s,a)|$ over all batches\", fontsize=30)\n",
    "    ax3.set_xlabel(r\"Epoch\", labelpad=15, fontsize=30)\n",
    "    draw_training_curve(ax3, maxQ_record[\"Q1\"], \"o\", \"r\", \"$q_1$\")\n",
    "    draw_training_curve(ax3, maxQ_record[\"Q2\"], \"x\", \"b\", \"$q_2$\")\n",
    "    ax3.axhline(0, ls=\"--\", lw=2, c=\"gray\")\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.2)\n",
    "    \n",
    "    if len(save) > 0 :\n",
    "        generate_directory_for_file_path(save)\n",
    "        plt.savefig(save, bbox_inches=\"tight\")\n",
    "    if show :\n",
    "        plt.show(fig)\n",
    "    if close :\n",
    "        plt.clf()\n",
    "        plt.close(fig)\n",
    "        \n",
    "    return fig, ax1, ax2, ax3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e96e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Identify priority state/action pairs\n",
    "###\n",
    "\n",
    "priority_states, priority_actions, priority_returns = [], [], []\n",
    "for action in action_list :\n",
    "    r_initial = r_end - action\n",
    "    if is_terminal(r_initial) : continue\n",
    "    priority_states .append(r_initial)\n",
    "    priority_actions.append(action)\n",
    "    priority_returns.append(0.)\n",
    "    \n",
    "priority_states  = np.array(priority_states )\n",
    "priority_actions = np.array(priority_actions)\n",
    "priority_returns = np.array(priority_returns)\n",
    "\n",
    "print(f\"Found {len(priority_returns)} priority state-action pairs with returns:  {'  '.join([f'{x:.2f}' for x in priority_returns])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49969928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(config_fname, q1_model, q2_model, to_stdout=True) :\n",
    "    '''\n",
    "    Print environment, training and model configurations to file config_fname. Also print environment and\n",
    "    training configurations to sys.stdout if requested, but do not print model summaries as they are verbose.\n",
    "    Inputs:\n",
    "      > config_fname, str\n",
    "        name of config file to create\n",
    "      > q1_model, keras Model\n",
    "        first q-value model\n",
    "      > q2_model, keras Model\n",
    "        second q-value model\n",
    "      > to_stdout, bool, default=True\n",
    "        if True then repeat environment and training configurations to sys.stdout\n",
    "    Returns:\n",
    "      > None\n",
    "    '''\n",
    "    # Create message as list of strings\n",
    "    config_message = []\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    config_message.append(f\"Environment config:\\n\")\n",
    "    config_message.append(f\"> horizontal_size: {horizontal_size}\\n\")\n",
    "    config_message.append(f\"> vertical_size: {vertical_size}\\n\")\n",
    "    config_message.append(f\"> horizontal_pad: {horizontal_pad}\\n\")\n",
    "    config_message.append(f\"> vertical_pad: {vertical_pad}\\n\")\n",
    "    config_message.append(f\"> horizontal_max: {horizontal_max}\\n\")\n",
    "    config_message.append(f\"> vertical_max: {vertical_max}\\n\")\n",
    "    config_message.append(f\"> reward_per_turn: {reward_per_turn:.6f}\\n\")\n",
    "    config_message.append(f\"> lambda_r: {lambda_r:.6f}\\n\")\n",
    "    config_message.append(f\"> lambda_b: {lambda_b:.6f}\\n\")\n",
    "    config_message.append(f\"> lambda_w: {lambda_w:.6f}\\n\")\n",
    "    config_message.append(f\"> gamma: {gamma:.6f}\\n\")\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    config_message.append(f\"Training config:\\n\")\n",
    "    config_message.append(f\"> Using bootstrap method: {bootstrap_method}\\n\")\n",
    "    config_message.append(f\"> Using epochs of length {num_train}\\n\")\n",
    "    config_message.append(f\"> Updating gradient every batch of size {batch_size}\\n\")\n",
    "    config_message.append(f\"> Using optimizer_q1 {optimizer_q1} with learning rate {learning_rate:.6}\\n\")\n",
    "    config_message.append(f\"> Using optimizer_q2 {optimizer_q2} with learning rate {learning_rate:.6}\\n\")\n",
    "    config_message.append(f\"> Plotting policy every {plot_policy_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Plotting monitors every {plot_monitors_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Swapping q1 and q2 every {switch_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Cloning q2 from q1 every {clone_after_epochs} epochs\\n\")\n",
    "    config_message.append(f\"> Assigning a weight of {priority_weight} to anchoring state/action pairs\\n\")\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    # Make sure directory exists for file\n",
    "    generate_directory_for_file_path(config_fname, print_msg_on_dir_creation=True)\n",
    "    # Open file and print messages, also to stdout if configured\n",
    "    # - also print q-model summaries, only to file\n",
    "    with open(config_fname, \"w\") as config_file :\n",
    "        for line in config_message :\n",
    "            config_file.write(line)\n",
    "            if not to_stdout : continue\n",
    "            sys.stdout.write(line)\n",
    "        config_file.write(\"\\nModel configs:\\n\\n\")\n",
    "        q1_model.summary(print_fn=lambda x: config_file.write(x + '\\n'))\n",
    "        config_file.write(\"\\n\")\n",
    "        q2_model.summary(print_fn=lambda x: config_file.write(x + '\\n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Configure\n",
    "\n",
    "num_epochs                 = -1\n",
    "batch_size                 = 25\n",
    "learning_rate              = 1e-4\n",
    "plot_policy_after_epochs   = 10\n",
    "plot_monitors_after_epochs = 2\n",
    "switch_after_epochs        = -1\n",
    "clone_after_epochs         = 2\n",
    "priority_weight            = 0.3\n",
    "bootstrap_method           = \"clone\"       # [\"clone\", \"self\", \"other\"]\n",
    "num_emp_steps              = 1\n",
    "\n",
    "\n",
    "## Set up\n",
    "\n",
    "loss_fcn     = tf.keras.losses.MeanSquaredError()\n",
    "optimizer_q1 = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "optimizer_q2 = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "q1_model     = create_action_value_model(name=\"action_value_model_1\")\n",
    "q2_model     = q1_model if bootstrap_method == \"self\" else create_action_value_model(name=\"action_value_model_2\")\n",
    "state_action_pairs = []\n",
    "for x in range(horizontal_max) : \n",
    "    for y in range(vertical_max) : \n",
    "        if is_terminal(np.array([x,y])) : continue\n",
    "        for a in action_list :\n",
    "            state_action_pairs.append([x,y,a[0],a[1]])\n",
    "np.random.shuffle(state_action_pairs)\n",
    "state_action_pairs = np.array(state_action_pairs)\n",
    "num_train = len(state_action_pairs)\n",
    "\n",
    "if bootstrap_method not in [\"clone\", \"self\", \"other\"] :\n",
    "    raise NotImplementedError(f\"Bootstrap method {bootstrap_method} not implemented\")\n",
    "\n",
    "\n",
    "## Print config to file and screen (model summaries only to file because they are verbose)\n",
    "\n",
    "create_config(\"figures/Helicopter_NB2/config.txt\", q1_model, q2_model, to_stdout=True)\n",
    "\n",
    "\n",
    "## Set up monitors for training start\n",
    "\n",
    "model_key_q1, model_key_q2 = \"Q1\", \"Q2\"  # used to keep track of which model is being traing each epoch\n",
    "loss_record, ref_loss_record, maxQ_record = {\"Q1\":[], \"Q2\":[]}, {\"Q1\":[], \"Q2\":[]}, {\"Q1\":[], \"Q2\":[]}\n",
    "\n",
    "epoch_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a289f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_q_samples = q1_model.predict([np.array([create_weather_map() for i in range(20)]), \n",
    "                  state_action_pairs[:20,0:2], \n",
    "                  state_action_pairs[:20,2:4]]).flatten()\n",
    "print(f\"A sample of initial q-values are {initial_q_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb30a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 16:15:33.653056: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.47059593],\n",
       "       [0.5246563 ],\n",
       "       [0.5206993 ],\n",
       "       [0.47516003],\n",
       "       [0.5044229 ],\n",
       "       [0.51498663],\n",
       "       [0.49184936],\n",
       "       [0.5258844 ],\n",
       "       [0.57368016],\n",
       "       [0.4764757 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd92aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "snapshot_epoch201  = tracemalloc.take_snapshot()\n",
    "display_top(snapshot_epoch201, limit=20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "snapshot_epoch154  = tracemalloc.take_snapshot()\n",
    "display_top(snapshot_epoch154, limit=20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "snapshot_epoch93 = tracemalloc.take_snapshot()\n",
    "display_top(snapshot_epoch93, limit=20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d126373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "num_validation_games = 20\n",
    "\n",
    "for game_idx in range(num_validation_games) :\n",
    "    create_greedy_policy_plot(create_weather_map(), q1_model, q2_model, epoch_idx=epoch_idx, verbose=True,\n",
    "                              show=True, close=True, save=f\"figures/Helicopter_NB2/greedy_policy_val{epoch_idx}.pdf\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1439ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ac982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compare_snapshots(snapshot1, snapshot2, key_type='lineno', limit=10):\n",
    "    print(\"Filtering snapshot 1\")\n",
    "    snapshot1 = snapshot1.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    \n",
    "    print(\"Filtering snapshot 2\")\n",
    "    snapshot2 = snapshot2.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    \n",
    "    print(\"Getting top stats\")\n",
    "    top_stats1 = snapshot1.statistics(key_type)\n",
    "    top_stats2 = snapshot2.statistics(key_type)\n",
    "    total_mem1 = sum(stat.size for stat in top_stats1)\n",
    "    total_mem2 = sum(stat.size for stat in top_stats2)\n",
    "    print(f\"total_mem1 = {total_mem1/1024/1024:.3f} MB\")\n",
    "    print(f\"total_mem2 = {total_mem2/1024/1024:.3f} MB\")\n",
    "    print(f\"change in mem = {total_mem2 - total_mem1}\")\n",
    "    \n",
    "    print(\"Getting memory 1\")\n",
    "    memory_dict1 = {}\n",
    "    for index, stat in enumerate(top_stats1[:limit], 1):\n",
    "        key, size = stat.traceback[0].filename, stat.size / 1024 / 1024\n",
    "        memory_dict1[key] = size\n",
    "        \n",
    "    print(\"Getting memory 2\")\n",
    "    memory_dict2 = {}\n",
    "    for index, stat in enumerate(top_stats2[:limit], 1):\n",
    "        key, size = stat.traceback[0].filename, stat.size / 1024 / 1024\n",
    "        memory_dict2[key] = size\n",
    "        \n",
    "    print(\"Getting memory change\")\n",
    "    memory_increase = []\n",
    "    for key, mem1 in memory_dict1.items() :\n",
    "        if key not in memory_dict2 : continue\n",
    "        memory_increase.append((key, memory_dict2[key] - mem1))\n",
    "    for key, mem2 in memory_dict2.items() :\n",
    "        if key in memory_dict1 : continue\n",
    "        memory_increase.append((key, mem2))\n",
    "        \n",
    "    print(\"Sorting list\")\n",
    "    memory_increase.sort(key = lambda x : x[1], reverse=True)\n",
    "    \n",
    "    print(\"Printing results\")\n",
    "    for key, delta_mem in memory_increase[:limit] :\n",
    "        print(f\"{key}    {delta_mem:.3f} MB\")\n",
    "        \n",
    "        \n",
    "compare_snapshots(snapshot_epoch154, snapshot_epoch201)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466d3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "top_stats_compare = snapshot_end.compare_to(snapshot_start, 'lineno')\n",
    "\n",
    "display_top(top_stats_compare)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"plt\", sys.getsizeof(plt))\n",
    "\n",
    "print(\"loss_fcn\", sys.getsizeof(loss_fcn))\n",
    "print(\"optimizer_q1\", sys.getsizeof(optimizer_q1))\n",
    "print(\"optimizer_q2\", sys.getsizeof(optimizer_q2))\n",
    "print(\"q1_model\", sys.getsizeof(q1_model))\n",
    "print(\"q2_model\", sys.getsizeof(q2_model))\n",
    "\n",
    "print(\"states\", sys.getsizeof(states))\n",
    "print(\"loss_record\", sys.getsizeof(loss_record))\n",
    "print(\"ref_loss_record\", sys.getsizeof(ref_loss_record))\n",
    "\n",
    "print(\"maxQ_record\", sys.getsizeof(maxQ_record))\n",
    "print(\"epoch_losses\", sys.getsizeof(epoch_losses))\n",
    "print(\"ref_losses\", sys.getsizeof(ref_losses))\n",
    "\n",
    "print(\"max_abs_q_values\", sys.getsizeof(max_abs_q_values))\n",
    "print(\"r_agents\", sys.getsizeof(r_agents))\n",
    "print(\"actions\", sys.getsizeof(actions))\n",
    "\n",
    "print(\"weather_maps\", sys.getsizeof(weather_maps))\n",
    "print(\"rewards\", sys.getsizeof(rewards))\n",
    "print(\"r_agents_p\", sys.getsizeof(r_agents_p))\n",
    "print(\"actions_p\", sys.getsizeof(actions_p))\n",
    "print(\"weather_map\", sys.getsizeof(weather_map))\n",
    "print(\"r_agent_p\", sys.getsizeof(r_agent_p))\n",
    "\n",
    "print(\"action_p\", sys.getsizeof(action_p))\n",
    "print(\"bootstrap_values\", sys.getsizeof(bootstrap_values))\n",
    "print(\"obs_returns\", sys.getsizeof(obs_returns))\n",
    "print(\"sample_weights\", sys.getsizeof(sample_weights))\n",
    "print(\"priority_weights\", sys.getsizeof(priority_weights))\n",
    "\n",
    "print(\"priority_weather\", sys.getsizeof(priority_weather))\n",
    "print(\"ref_returns\", sys.getsizeof(ref_returns))\n",
    "print(\"ref_loss\", sys.getsizeof(ref_loss))\n",
    "print(\"tape\", sys.getsizeof(tape))\n",
    "\n",
    "print(\"pred_returns\", sys.getsizeof(pred_returns))\n",
    "print(\"loss_value\", sys.getsizeof(loss_value))\n",
    "print(\"grads\", sys.getsizeof(grads))\n",
    "\n",
    "print(\"np_returns\", sys.getsizeof(np_returns))\n",
    "print(\"epoch_mean_loss\", sys.getsizeof(epoch_mean_loss))\n",
    "print(\"epoch_mean_ref_loss\", sys.getsizeof(epoch_mean_ref_loss))\n",
    "print(\"epoch_maxQ\", sys.getsizeof(epoch_maxQ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321aa655",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for key, desc in locals().items() :\n",
    "    print(key)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afd830",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''for key, desc in globals().items() :\n",
    "    print(key)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9093d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b58855a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  4],\n",
       "       [ 7,  5],\n",
       "       [ 3,  8],\n",
       "       [11,  0],\n",
       "       [ 4,  6],\n",
       "       [11,  5],\n",
       "       [ 5,  4],\n",
       "       [10,  1],\n",
       "       [ 2,  8],\n",
       "       [ 3,  3],\n",
       "       [11,  4],\n",
       "       [11,  5],\n",
       "       [11,  5],\n",
       "       [ 0,  7],\n",
       "       [10,  6],\n",
       "       [ 7,  8],\n",
       "       [ 7,  7],\n",
       "       [ 9,  8],\n",
       "       [ 9,  1],\n",
       "       [ 7,  7]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " state_action_pairs[:20,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e43db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 1,  1],\n",
       "       [-1, -1],\n",
       "       [-1,  1],\n",
       "       [-1, -1],\n",
       "       [ 1,  0],\n",
       "       [ 1,  0],\n",
       "       [-1, -1],\n",
       "       [ 0,  1],\n",
       "       [ 1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 1, -1],\n",
       "       [ 1, -1],\n",
       "       [ 1,  0],\n",
       "       [ 1,  0],\n",
       "       [ 1,  1],\n",
       "       [ 0,  0],\n",
       "       [ 0,  1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_pairs[:20,2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09635bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Start or continue training\n",
    "\n",
    "start_time = time.time()\n",
    "while epoch_idx < num_epochs or num_epochs < 0 :\n",
    "    \n",
    "    # Determine whether to plot current greedy policy\n",
    "    if plot_policy_after_epochs > 0 and epoch_idx % plot_policy_after_epochs == 0 :\n",
    "        create_greedy_policy_plot(create_weather_map(), q1_model, q2_model, epoch_idx=epoch_idx, verbose=True,\n",
    "                                  show=False, close=True, save=f\"figures/Helicopter_NB2/greedy_policy_epoch{epoch_idx}.pdf\")\n",
    "    \n",
    "    # Determine whether to plot training curves\n",
    "    if plot_monitors_after_epochs > 0 and epoch_idx > 0 and epoch_idx % plot_monitors_after_epochs == 0 :\n",
    "        create_training_curves_plot(loss_record, ref_loss_record, maxQ_record, show=False, close=True,\n",
    "                                    save=\"figures/Helicopter_NB2/training_curves.pdf\")\n",
    "        \n",
    "    # Determine whether to switch q1 and q2\n",
    "    if bootstrap_method == \"other\" and switch_after_epochs > 0 and epoch_idx > 0 and epoch_idx % switch_after_epochs == 0 :\n",
    "        model_key_q1, model_key_q2 = model_key_q2, model_key_q1\n",
    "        q1_model, q2_model         = q2_model, q1_model\n",
    "        optimizer_q1, optimizer_q2 = optimizer_q2, optimizer_q1\n",
    "        \n",
    "    # Determine whether to copy q1 to q2\n",
    "    if bootstrap_method == \"clone\" and clone_after_epochs > 0 and epoch_idx % clone_after_epochs == 0 :\n",
    "        q2_model.set_weights(q1_model.get_weights()) \n",
    "    \n",
    "    sys.stdout.write(f\"\\rEpoch {epoch_idx+1} / {num_epochs}  [t={time.time()-start_time:.2f}s]\".ljust(110))\n",
    "    \n",
    "    # Perform one gradient update per batch\n",
    "    epoch_losses, ref_losses, max_abs_q_values = [], [], []\n",
    "    for batch_idx in range(math.ceil(num_train/batch_size)) :\n",
    "        \n",
    "        # Resolve sample indices to be used for this batch update\n",
    "        batch_idx_low, batch_idx_high = batch_idx*batch_size, min((batch_idx+1)*batch_size, num_train)\n",
    "        actual_batch_size = batch_idx_high - batch_idx_low\n",
    "        if actual_batch_size == 0 : continue\n",
    "        \n",
    "        # Update the current epoch message to keep track of batch progress \n",
    "        sys.stdout.write(f\"\\rEpoch {epoch_idx+1} / {num_epochs} batch indices ({batch_idx_low}, {batch_idx_high}) / {num_train}  [t={time.time()-start_time:.2f}s]\".ljust(110))\n",
    "        \n",
    "        # Get batch of states and generate a random exploration action for each\n",
    "        r_agents = state_action_pairs[batch_idx_low:batch_idx_high,0:2]\n",
    "        actions  = state_action_pairs[batch_idx_low:batch_idx_high,2:4]\n",
    "        \n",
    "        # Generate some unique weather maps to avoid over-training if we re-use maps\n",
    "        weather_maps = np.array([create_weather_map() for i in range(actual_batch_size)])\n",
    "        \n",
    "        # For each state/action pair, apply the action to get the immediate reward, and also find the\n",
    "        # greedy action in the next state from which to calculate bootstraps\n",
    "        rewards, r_agents_p, actions_p, agent_p_is_terminal = [], [], [], []\n",
    "        for r_agent, action, weather_map in zip(r_agents, actions, weather_maps) :\n",
    "            r_agent_p, action_p, step_y, emp_return = r_agent, action, 1., 0.\n",
    "            for step_idx in range(num_emp_steps) :\n",
    "                if is_terminal(r_agent_p) : continue\n",
    "                reward, r_agent_p = perform_action(weather_map, r_agent, action)\n",
    "                action_p, _       = get_greedy_action(weather_map, r_agent_p, q1_model)\n",
    "                emp_return       += step_y * reward\n",
    "                step_y           *= gamma\n",
    "            rewards   .append(emp_return)\n",
    "            r_agents_p.append(r_agent_p)\n",
    "            actions_p .append(action_p)\n",
    "            agent_p_is_terminal.append(is_terminal(r_agent_p))\n",
    "        rewards, r_agents_p, actions_p = np.array(rewards), np.array(r_agents_p), np.array(actions_p)\n",
    "        \n",
    "        # Calculate obs_returns using the observed immediate rewards plut gamma * bootstrap values\n",
    "        bootstrap_values = q2_model.predict([weather_maps, r_agents_p, actions_p]).flatten()\n",
    "        bootstrap_values = np.array([0. if is_term else q for is_term,q in zip(agent_p_is_terminal,bootstrap_values)])\n",
    "        obs_returns      = rewards + (gamma**num_emp_steps) * bootstrap_values\n",
    "        \n",
    "        # Calculate weights to apply to regular batch and priority samples\n",
    "        sample_weights  = (1.-priority_weight) * np.full(shape=(len(r_agents),), fill_value=1./len(r_agents))\n",
    "        priority_weights = priority_weight * np.full(shape=(len(priority_states),), fill_value=1./len(priority_states))\n",
    "        \n",
    "        # Calculate weather for priority samples (changes each epoch)\n",
    "        priority_weather = np.array([create_weather_map() for i in range(len(priority_states))])\n",
    "        \n",
    "        # Concatenate regular batch and priority samples\n",
    "        if priority_weight > 0 and priority_weight <= 1 :\n",
    "            sample_weights   = np.concatenate([sample_weights , priority_weights]).flatten()\n",
    "            r_agents         = np.concatenate([r_agents       , priority_states ])\n",
    "            actions          = np.concatenate([actions        , priority_actions])\n",
    "            weather_maps     = np.concatenate([weather_maps   , priority_weather])\n",
    "            obs_returns      = np.concatenate([obs_returns    , priority_returns]).flatten()\n",
    "        \n",
    "        # When using sample weights, we have to be careful with the object shapes. The loss function will\n",
    "        # expect y_pred of shape (N,1) and y_true of shape (N,1), when the output shape is (1,), and sample\n",
    "        # weights of shape (N,). When not using sample weights we can get away with being lazy on the shapes\n",
    "        # of y_pred and y_true, but if we do this here then it will not correctly apply the correct sample\n",
    "        # weight to the correct sample. Furthermore, MSE with sample weights will calculate mean(sw*res**2)\n",
    "        # rather than normalising according to sum(sw*res**2)/sum(sw). This means we must also multiply sw \n",
    "        # by a factor of len(sw)/sum(sw) if we are to recover the MSE value that we expect. Note that we have\n",
    "        # sum(sw)=1 by our construction above, but I still write it explicitly so the general idea is clear.\n",
    "        sample_weights = sample_weights * len(sample_weights) / sample_weights.sum()\n",
    "        obs_returns    = obs_returns.reshape((len(obs_returns),1))\n",
    "        \n",
    "        # ref_loss is the loss over only the priority state/action pairs, before gradient updates for\n",
    "        # consistency with regular loss record. Since regular loss mixes (i) how close the priority samples\n",
    "        # are to their correct values and (ii) how close other points are to their bootstrap-biased values,\n",
    "        # the ref_loss removes the bootstrap samples so we can see whether these points are diverging\n",
    "        ref_returns = q1_model([priority_weather, priority_states, priority_actions], training=False)\n",
    "        ref_loss    = loss_fcn(priority_returns.reshape((len(ref_returns),1)), ref_returns.numpy())\n",
    "        ref_losses.append(ref_loss)\n",
    "                                \n",
    "        # Apply gradient updates\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_returns     = q1_model([weather_maps, r_agents, actions], training=True)\n",
    "            loss_value       = loss_fcn(obs_returns, pred_returns, sample_weight=sample_weights)\n",
    "            grads            = tape.gradient(loss_value, q1_model.trainable_weights)\n",
    "            optimizer_q1.apply_gradients(zip(grads, q1_model.trainable_weights))\n",
    "            epoch_losses.append(loss_value.numpy())\n",
    "            np_returns = np.fabs(pred_returns.numpy().flatten())\n",
    "            max_q_idx  = np.argmax(np_returns)\n",
    "            max_abs_q_values.append(np_returns.max())\n",
    "                                            \n",
    "    # Print epoch summary and end stdout line to keep this on screen\n",
    "    epoch_mean_loss, epoch_mean_ref_loss, epoch_maxQ = np.mean(epoch_losses), np.mean(ref_losses), np.max(max_abs_q_values)\n",
    "    sys.stdout.write(f\"\\rEpoch {epoch_idx+1} / {num_epochs}  [t={time.time()-start_time:.2f}s]  <loss = {epoch_mean_loss:.5f}, ref_loss = {epoch_mean_ref_loss:.5f}, max_Q = {epoch_maxQ:.1f}>\\n\".ljust(110))\n",
    "    loss_record    [model_key_q1].append((epoch_idx, epoch_mean_loss    ))\n",
    "    ref_loss_record[model_key_q1].append((epoch_idx, epoch_mean_ref_loss))\n",
    "    maxQ_record    [model_key_q1].append((epoch_idx, epoch_maxQ         ))\n",
    "                      \n",
    "    # Manually iterate epoch index and make sure stdout not lagging\n",
    "    epoch_idx += 1\n",
    "    sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f739b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6a2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
