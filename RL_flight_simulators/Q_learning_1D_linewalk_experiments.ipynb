{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e8af00",
   "metadata": {},
   "source": [
    "# Linewalk Q-learning experiments\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we use a simple 1D linewalk to experiment with the properties of tabular Q-learning. Our aim is to understand the dynamics of Q-learning when the current value estimates are wrong in different ways. The three types of wrong-ness we want to consider are when\n",
    "- Q-learning iterates in the wrong direction because of bootstrapping.\n",
    "- Q-learning iterates in the wrong direction because of an incorrect action choice.\n",
    "- The value estimates are a larger scale than the immediate rewards, causing the bootstrap contributions to be more important than the rewards when calculating the (possibly multi-step) error.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Q-learning we have two primary components: bootstrapping, and a \"max-over-action\" operation to implement a form of off-policy learning. We do not use function approximation here, because we want to avoid conflating typical Q-learning behaviours with those of function approximation.\n",
    "\n",
    "The learning objective is to minimise $\\mathbb{E}_{s,a \\sim \\mu}\\left[~|r_{s,a} + \\gamma \\cdot \\mathrm{max}_{a'}q(s',a') - q(s,a)|^2~\\right]$, where the expectation is over all state-action pairs selected according to the exploration policy (for us this will be uniform over all pairs). We may also extend the empirical returns to multiple steps, but the greedy-policy must still be performed over the estimated value function. This means that, whilst multi-step returns reduce the contribution of the possibly-mis-modelled bootstrap target in favour of empirical returns, they do not address the impact of _selecting the wrong action_ because of the bad value-function. This bad action-selection process therefore contributes regardless of whether we bootstrap after one step or many. We therefore have two different drivers of possibly-pathological learning dynamics: bootstrapping with a mis-modelled function, and action-selection with a mis-modelled function. I expect there to be an interplay between these two effects.\n",
    "\n",
    "In particular, I want to initialise the value function in different ways (e.g. with correct/incorrect gradients, or correct/incorrect action-choices, or all-zeros), to see how these lead to different training trajectories.\n",
    "\n",
    "In this simple example, it is relatively easy to construct the optimal policy along with its true value. We may therefore using three metrics to track our distance from our goal:\n",
    "1. the MSE comparing our value function with the optimal one for all state-action pairs\n",
    "2. the MSE comparing our value function with the optimal one using only the optimal state-action pairs\n",
    "3. the \"accuracy\" of the greedy policy define using our value function, defined as the fraction of states in which the greedy action is correctly modelled\n",
    "As well as being able to plot the estimated and true value functions as we progress through training.\n",
    "\n",
    "Furthermore, we will track the MSE loss, and MSE loss for winning state-action pairs, and the maximum $|q(s,a)|$ over every epoch. These are the metrics we will actually have access to when training without access knowledge of the true optimal value function.\n",
    "\n",
    "We may consider how these experiments are affected when we introduce the following\n",
    "- multi-step returns\n",
    "- large learning rate to reduce absolute variance on gradient updates \n",
    "- double Q-learning using cloning and alternative models\n",
    "- a tailored reward signal (introduces expert knowledge and biases solution towards a given form, so does not necessarily generalise well, but forces good behaviour)\n",
    "\n",
    "When using function approximation, we may add a term to the loss function which anchors the value estimates to priority state/action-pairs. This helps to enforce the absolute scale of value function, because bootstrapping only requires the value target to satisfy a relative relationship to nearby state-action pairs - specifically those chosen by the current possibly-mis-modelled greedy policy. This approach is not needed in a tabular setting, since there is no function to anchor (rather the value of every state-action pair is updated in isolation to every other).\n",
    "\n",
    "N.B. this approach would require expert knowledge to define the priority state-action pairs. The impact of anchoring becomes smaller the further we are from the anchor points, since all mis-modelling in between may compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4199c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Import packages\n",
    "###\n",
    "\n",
    "import os, sys, time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca1c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_pad           = 25\n",
      "right_pad          = 4\n",
      "reward_per_turn    = -1.0\n",
      "reward_per_dx      = 0.0\n",
      "reward_at_boundary = -1.0\n",
      "x_min              = 0\n",
      "x_max              = 29\n",
      "x_terminal         = 25\n",
      "x_range            = 29\n",
      "state_list         = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "num_states         = 30\n",
      "action_list        = [-1, 1]\n",
      "num_actions        = 2\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Configure environment variables\n",
    "###\n",
    "\n",
    "left_pad   = 25                          # Amount of line to the left of the terminal state\n",
    "right_pad  = 4                           # Amount of line to the right of the terminal state\n",
    "\n",
    "reward_per_turn    = -1.                 # Reward obtained per movement\n",
    "reward_per_dx      = 0.                  # Reward multiplier for change in distance to goal (+ve def)\n",
    "reward_at_boundary = -1.                 # Reward obtained when encountering the line boundary\n",
    "discount_factor    = 1.                  # Discount factor\n",
    "\n",
    "\n",
    "###\n",
    "###  Non-configurable calculations\n",
    "###\n",
    "\n",
    "x_min      = 0                           # Minimum x index is always 0  <<<  do not change\n",
    "x_terminal = left_pad                    # Terminal state location\n",
    "x_max      = left_pad + right_pad        # Max x index, = num padding +1 (terminal state) -1 (idcs start @ 0)\n",
    "x_range    = x_max - x_min               # Range of x indices, equal to x_max when x_min = 0\n",
    "state_list = np.arange(x_range + 1)\n",
    "num_states = len(state_list)\n",
    "\n",
    "action_list = [-1, 1]                    # Allowed actions in units of dx\n",
    "num_actions = len(action_list)\n",
    "\n",
    "\n",
    "###\n",
    "###  Print environment summary\n",
    "###\n",
    "\n",
    "print(f\"left_pad           = {left_pad}\")\n",
    "print(f\"right_pad          = {right_pad}\")\n",
    "print(f\"reward_per_turn    = {reward_per_turn}\")\n",
    "print(f\"reward_per_dx      = {reward_per_dx}\")\n",
    "print(f\"reward_at_boundary = {reward_at_boundary}\")\n",
    "\n",
    "print(f\"x_min              = {x_min}\")\n",
    "print(f\"x_max              = {x_max}\")\n",
    "print(f\"x_terminal         = {x_terminal}\")\n",
    "print(f\"x_range            = {x_range}\")\n",
    "print(f\"state_list         = {state_list}\")\n",
    "print(f\"num_states         = {num_states}\")\n",
    "print(f\"action_list        = {action_list}\")\n",
    "print(f\"num_actions        = {num_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537e99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Define environment methods\n",
    "###\n",
    "\n",
    "\n",
    "def is_terminal(state) :\n",
    "    '''\n",
    "    Return True if state is the terminal state and False otherwise.\n",
    "    Inputs:\n",
    "      > state, int [x_min, x_max]\n",
    "        x position to evaluate\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether the state is in the terminal state\n",
    "    '''\n",
    "    if state == x_terminal :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_out_of_bounds(state) :\n",
    "    '''\n",
    "    Return True if state is out of bounds and False otherwise.\n",
    "    Inputs:\n",
    "      > state, int [x_min, x_max]\n",
    "        x position to evaluate\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether the state is out of bounds\n",
    "    '''\n",
    "    if state < x_min : return True\n",
    "    if state > x_max : return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_valid_agent_state(state) :\n",
    "    '''\n",
    "    Return True if state is within bounds and not terminal, and False otherwise.\n",
    "    Inputs:\n",
    "      > state, int [x_min, x_max]\n",
    "        x position to evaluate\n",
    "    Returns:\n",
    "      > bool\n",
    "        whether an agent may exist in this state\n",
    "    '''\n",
    "    if is_terminal     (state) : return False\n",
    "    if is_out_of_bounds(state) : return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def perform_action(state, action, base_reward=None, boundary_reward=None, dx_reward=None) :\n",
    "    '''\n",
    "    Given the current agent state, perform the specified action and return the reward obtained along with \n",
    "    the new agent state.\n",
    "    Inputs:\n",
    "      > state, int [x_min, x_max]\n",
    "        x position of agent at initial timestep\n",
    "      > action, int in [0, num_actions-1]\n",
    "        index of action_list to dereference\n",
    "      > base_reward, float, default=reward_per_turn\n",
    "        basic reward returned every turn (expected -ve)\n",
    "      > boundary_reward, float, default=reward_at_boundary\n",
    "        reward received when encountering the edge of the game board (expected -ve)\n",
    "      > dx_reward, float, default=reward_per_dx\n",
    "        factor multiplied by change-in-distance to calculate movement reward (expected +ve def)\n",
    "    Returns:\n",
    "      > float\n",
    "        reward obtained by performing action\n",
    "      > int [0, horizontal_max)\n",
    "        x position of agent at iterated timestep\n",
    "    '''\n",
    "    ##\n",
    "    if type(base_reward)     == type(None) : base_reward     = reward_per_turn\n",
    "    if type(boundary_reward) == type(None) : boundary_reward = reward_at_boundary\n",
    "    if type(dx_reward)       == type(None) : dx_reward       = reward_per_dx\n",
    "    ##  Make sure initial state is valid to protect against unexpected behaviour\n",
    "    if not is_valid_agent_state(state) :\n",
    "        raise RuntimeError(f\"Agent position ({state}) is not a valid agent state\")\n",
    "    ##  Make sure action is valid to protect against unexpected behaviour\n",
    "    if action < 0 or action >= num_actions :\n",
    "        raise RuntimeError(f\"Action index ({action}) not found in allowed range [0, {num_actions})\")\n",
    "    ##  Get initial distance of agent from the end\n",
    "    dx_agent = np.fabs(state - x_terminal)\n",
    "    ##  Iterate agent state (if agent hits boundary then add penalty and return to original position)\n",
    "    state_p   = state + action_list[action]\n",
    "    reward_b  = 0\n",
    "    if is_out_of_bounds(state_p) :\n",
    "        reward_b = boundary_reward\n",
    "        state_p  = state\n",
    "    ##  Get distance-based reward\n",
    "    dx_agent_p = np.fabs(state_p - x_terminal)\n",
    "    reward_dx  = dx_reward * (dx_agent - dx_agent_p)\n",
    "    ##  Calculate total reward by summing the base, boundary, distance and weather rewards\n",
    "    reward = base_reward + reward_b + reward_dx\n",
    "    ##  Return reward and new agent state\n",
    "    return reward, state_p\n",
    "    \n",
    "\n",
    "def get_greedy_action(state, *q_models) :\n",
    "    '''\n",
    "    Sample a greedy action from the q-value models provided. If multiple models provided then use their mean.\n",
    "    Inputs:\n",
    "      > state, int [x_min, x_max]\n",
    "        x position of agent at initial timestep\n",
    "      > q_models, list of np.ndarray shape=(num_states,num_actions)\n",
    "        list of estimated value functions\n",
    "    Returns:\n",
    "      > int in action_list\n",
    "        index of action defined by greedy policy over the model(s) at this agent position\n",
    "    '''\n",
    "    action_values = 0\n",
    "    for q_model in q_models : action_values += q_model[state,:]\n",
    "    action_values /= len(q_models)\n",
    "    action_max, best_actions = -np.inf, []\n",
    "    for a, q in enumerate(action_values) :\n",
    "        if q < action_max : continue\n",
    "        if q == action_max :\n",
    "            best_actions.append(a)\n",
    "            continue\n",
    "        action_max, best_actions = q, [a]\n",
    "    return np.random.choice(best_actions)\n",
    "\n",
    "\n",
    "def get_true_action_values() :\n",
    "    '''\n",
    "    Create the true action value function for the environment configured\n",
    "    '''\n",
    "    action_values = np.zeros(shape=(num_states,num_actions))\n",
    "    for s in range(x_min, 1+x_max) :\n",
    "        for a in range(num_actions) :\n",
    "            if is_terminal(s) :\n",
    "                action_values[s,a] = np.nan\n",
    "                continue\n",
    "            g, sp = perform_action(s, a)\n",
    "            step_discount = discount_factor\n",
    "            while not is_terminal(sp) :\n",
    "                if sp > x_terminal : ap = 0\n",
    "                else               : ap = 1\n",
    "                r, sp          = perform_action(sp, ap)\n",
    "                g             += step_discount*r\n",
    "                step_discount *= discount_factor\n",
    "            action_values[s,a] = g\n",
    "    return action_values\n",
    "\n",
    "\n",
    "def print_action_values(action_values) :\n",
    "    '''\n",
    "    Print a text version of the action values\n",
    "    '''\n",
    "    print(\"       | State\")\n",
    "    print(\"Action | \" + \" | \".join([f'{s}'.ljust(6) for s in state_list])) \n",
    "    print(\"-------+\" + \"-\"*(9*num_states-1))\n",
    "    for a_idx, a in enumerate(action_list) :\n",
    "        print(f\"{a}\".ljust(6) + \" | \" +  \" | \".join([f'{action_values[s,a_idx]:.2f}'.ljust(6) for s in state_list])) \n",
    "        print(\"-------+\" + \"-\"*(9*num_states-1))\n",
    "\n",
    "        \n",
    "def evaluate_model_accuracy(q_model_true, q_model_eval) :\n",
    "    '''\n",
    "    Return the accuracy of the greedy policy obtained using the action value estimates.\n",
    "    '''\n",
    "    num_correct, num_total = 0, 0\n",
    "    for state in state_list :\n",
    "        if is_terminal(state) : continue\n",
    "        true_a = get_greedy_action(state, q_model_true)\n",
    "        eval_a = get_greedy_action(state, q_model_eval)\n",
    "        num_total += 1\n",
    "        if true_a == eval_a :\n",
    "            num_correct += 1\n",
    "    return num_correct / num_total\n",
    "    \n",
    "    \n",
    "def get_mean_abs_error_between_models(q_model_1, q_model_2, squared=True) :\n",
    "    '''\n",
    "    Return mean absolute error between the two models, weighting equally over all state-action pairs. Instead\n",
    "    return the MSE if squared=True.\n",
    "    '''\n",
    "    q_residual = (q_model_1 - q_model_2).flatten()\n",
    "    q_residual = np.where(np.isfinite(q_residual), q_residual, 0.)\n",
    "    q_residual = np.fabs(q_residual)\n",
    "    if squared :\n",
    "        q_residual = q_residual**2\n",
    "    return q_residual.sum() / len(q_residual)\n",
    "\n",
    "\n",
    "def get_all_state_action_pairs() :\n",
    "    '''\n",
    "    Create new numpy array of all state-action pairs\n",
    "    '''\n",
    "    get_state_action_pairs = []\n",
    "    for state in range(num_states) : \n",
    "        if not is_valid_agent_state(state) : continue\n",
    "        for action in range(num_actions) : \n",
    "            state_action_pairs.append((state, action))\n",
    "    state_action_pairs = np.array(state_action_pairs)\n",
    "    return state_action_pairs\n",
    "\n",
    "\n",
    "def get_q_target(state, action, q_model_eval, q_model_bs, num_emp_steps=1, gamma=None) :\n",
    "    '''\n",
    "    Return multi-step target for given state-action pair.\n",
    "    '''\n",
    "    state_p, action_p, emp_return, step_discount = state, action, 0., 1.\n",
    "    if type(gamma) == type(None) : gamma = discount_factor\n",
    "    for step_idx in range(num_emp_steps) :\n",
    "        if is_terminal(state_p) : continue\n",
    "        r, state_p     = perform_action(state_p, action_p)\n",
    "        action_p       = get_greedy_action(state_p, q_model_eval)\n",
    "        emp_return    += step_discount * r\n",
    "        step_discount *= gamma\n",
    "    bootstrap_return = 0. if is_terminal(state_p) else q_model_bs[state_p, action_p]\n",
    "    q_target = emp_return + step_discount * bootstrap_return\n",
    "    return q_target\n",
    "\n",
    "\n",
    "def get_q_target_function(q_model_eval, q_model_bs, num_emp_steps=1, gamma=None) :\n",
    "    '''\n",
    "    Return multi-step target for all state-action pairs.\n",
    "    '''\n",
    "    q_target = np.full(shape=(num_states, num_actions), fill_value=np.nan)\n",
    "    for s in range(num_states) :\n",
    "        if is_terminal(s) : continue\n",
    "        for a in range(num_actions) :\n",
    "            q_target[s,a] = get_q_target(s, a, q_model_eval, q_model_bs, num_emp_steps=num_step_returns, gamma=gamma)\n",
    "    return q_target\n",
    "\n",
    "\n",
    "def get_empirical_error(state, action, q_model_eval, q_model_bs, num_emp_steps=1, gamma=None) :\n",
    "    '''\n",
    "    Return multi-step error for given state-action pair.\n",
    "    '''\n",
    "    q_target = get_q_target(state, action, q_model_eval, q_model_bs, num_emp_steps=num_emp_steps, gamma=gamma)\n",
    "    return q_target - q_model_eval[state, action]\n",
    "\n",
    "\n",
    "def get_empirical_error_function(q_model_eval, q_model_bs, num_emp_steps=1, gamma=None) :\n",
    "    '''\n",
    "    Return multi-step error for all state-action pairs.\n",
    "    '''\n",
    "    q_target = get_q_target_function(q_model_eval, q_model_bs, num_emp_steps=num_emp_steps, gamma=gamma)\n",
    "    return q_target - q_model_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b0edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action values of optimal policy are:\n",
      "\n",
      "       | State\n",
      "Action | 0      | 1      | 2      | 3      | 4      | 5      | 6      | 7      | 8      | 9      | 10     | 11     | 12     | 13     | 14     | 15     | 16     | 17     | 18     | 19     | 20     | 21     | 22     | 23     | 24     | 25     | 26     | 27     | 28     | 29    \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "-1     | -27.00 | -26.00 | -25.00 | -24.00 | -23.00 | -22.00 | -21.00 | -20.00 | -19.00 | -18.00 | -17.00 | -16.00 | -15.00 | -14.00 | -13.00 | -12.00 | -11.00 | -10.00 | -9.00  | -8.00  | -7.00  | -6.00  | -5.00  | -4.00  | -3.00  | nan    | -1.00  | -2.00  | -3.00  | -4.00 \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1      | -25.00 | -24.00 | -23.00 | -22.00 | -21.00 | -20.00 | -19.00 | -18.00 | -17.00 | -16.00 | -15.00 | -14.00 | -13.00 | -12.00 | -11.00 | -10.00 | -9.00  | -8.00  | -7.00  | -6.00  | -5.00  | -4.00  | -3.00  | -2.00  | -1.00  | nan    | -3.00  | -4.00  | -5.00  | -6.00 \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " with a maximum |q(s,a)| of 27.0\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Evaluate and print true action-values for this environment configuration\n",
    "###\n",
    "\n",
    "true_action_values, true_max_abs_q = None, np.nan\n",
    "\n",
    "def update_true_action_values() :\n",
    "    global true_action_values, true_max_abs_q\n",
    "    true_action_values = get_true_action_values()\n",
    "    true_max_abs_q     = np.nanmax(np.fabs(true_action_values))\n",
    "    print(\"Action values of optimal policy are:\\n\")\n",
    "    print_action_values(true_action_values)\n",
    "    print(f\"\\n with a maximum |q(s,a)| of {true_max_abs_q}\")\n",
    "\n",
    "update_true_action_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83fbd499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy state-action pairs are: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 0), (27, 0), (28, 0), (29, 0)]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Find greedy action for every state\n",
    "###\n",
    "\n",
    "greedy_state_action_pairs = []\n",
    "for s in range(num_states) :\n",
    "    if is_terminal(s) : continue\n",
    "    if s > x_terminal : a = 0\n",
    "    else              : a = 1\n",
    "    greedy_state_action_pairs.append((s,a))\n",
    "    \n",
    "print(f\"Greedy state-action pairs are: {greedy_state_action_pairs}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8747cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###  Configure initial action value estimates\n",
    "###\n",
    "#\n",
    "# Value function format: np array of shape (num_states,num_actions)\n",
    "# For convenience this includes the terminal state, for which the contained value will be ignored\n",
    "#\n",
    "\n",
    "# Create list to store initial action value estimates for different experiments\n",
    "# List items are pairs of the form (description of experiment, initial action value estimates)\n",
    "experiment_configs = []\n",
    "\n",
    "# First experiment: all zeros\n",
    "experiment_configs.append((\"All_zeros\", np.zeros(shape=(num_states,num_actions))))\n",
    "\n",
    "# Second experiment: inverted (so wrong action choice and dependence at every step)\n",
    "experiment_configs.append((\"Wrong_action_and_dependence\", -.7*true_action_values))\n",
    "\n",
    "# Third experiment: inverted and scaled\n",
    "experiment_configs.append((\"Wrong_action_and_dependence_scaled_up\", -5.*true_action_values))\n",
    "\n",
    "# Fourth experiment: correct dependence and action choice, but scaled up\n",
    "experiment_configs.append((\"Over_scaled\", 5.*true_action_values))\n",
    "\n",
    "# Fifth experiment: correct dependence and action choice, but scaled down\n",
    "experiment_configs.append((\"Under_scaled\", .2*true_action_values))\n",
    "\n",
    "# Sixth experiment: correct dependence and action choice, but shifted up\n",
    "experiment_configs.append((\"Up_shifted\", true_action_values + true_max_abs_q + 3.))\n",
    "\n",
    "# Seventh experiment: correct dependence and action choice, but shifted down\n",
    "experiment_configs.append((\"Down_shifted\", true_action_values - true_max_abs_q - 3.))\n",
    "\n",
    "# Eighth experiment: incorrect dependence but correct action choice\n",
    "experiment_configs.append((\"Wrong_dependence\", .7*np.array([-true_action_values[:,1], -true_action_values[:,0]]).transpose()))\n",
    "\n",
    "# Nineth experiment: incorrect dependence but correct action choice\n",
    "experiment_configs.append((\"Wrong_dependence_scaled_up\", 5.*np.array([-true_action_values[:,1], -true_action_values[:,0]]).transpose()))\n",
    "\n",
    "# Tenth experiment: correct dependence but incorrect action choice\n",
    "experiment_configs.append((\"Wrong_actions\", .7*np.array([true_action_values[:,1], true_action_values[:,0]]).transpose()))\n",
    "\n",
    "# Eleventh experiment: correct dependence but incorrect action choice\n",
    "experiment_configs.append((\"Wrong_actions_scaled_up\", 5.*np.array([true_action_values[:,1], true_action_values[:,0]]).transpose()))\n",
    "              \n",
    "# Twelth experiment: random values with different magnitudes\n",
    "experiment_configs.append((\"Random_values_small_v1\", np.random.normal(size=(num_states,num_actions), scale=0.1)))\n",
    "experiment_configs.append((\"Random_values_small_v2\", np.random.normal(size=(num_states,num_actions), scale=0.1)))\n",
    "\n",
    "experiment_configs.append((\"Random_values_medium_v1\", np.random.normal(size=(num_states,num_actions), scale=1)))\n",
    "experiment_configs.append((\"Random_values_medium_v2\", np.random.normal(size=(num_states,num_actions), scale=1)))\n",
    "\n",
    "experiment_configs.append((\"Random_values_large_v1\", np.random.normal(size=(num_states,num_actions), scale=5)))\n",
    "experiment_configs.append((\"Random_values_large_v2\", np.random.normal(size=(num_states,num_actions), scale=5)))\n",
    "\n",
    "experiment_configs.append((\"Random_values_very_large_v1\", np.random.normal(size=(num_states,num_actions), scale=20)))\n",
    "experiment_configs.append((\"Random_values_very_large_v2\", np.random.normal(size=(num_states,num_actions), scale=20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7821bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directory_for_file_path(fname, print_msg_on_dir_creation=True) :\n",
    "    \"\"\"\n",
    "    Create the directory structure needed to place file fname. Call this before fig.savefig(fname, ...) to \n",
    "    make sure fname can be created without a FileNotFoundError\n",
    "    Input:\n",
    "       - fname: str\n",
    "                name of file you want to create a tree of directories to enclose\n",
    "                also create directory at this path if fname ends in '/'\n",
    "       - print_msg_on_dir_creation: bool, default = True\n",
    "                                    if True then print a message whenever a new directory is created\n",
    "    \"\"\"\n",
    "    while \"//\" in fname :\n",
    "        fname = fname.replace(\"//\", \"/\")\n",
    "    dir_tree = fname.split(\"/\")\n",
    "    dir_tree = [\"/\".join(dir_tree[:i]) for i in range(1,len(dir_tree))]\n",
    "    dir_path = \"\"\n",
    "    for dir_path in dir_tree :\n",
    "        if len(dir_path) == 0 : continue\n",
    "        if not os.path.exists(dir_path) :\n",
    "            os.mkdir(dir_path)\n",
    "            if print_msg_on_dir_creation :\n",
    "                print(f\"Directory {dir_path} created\")\n",
    "            continue\n",
    "        if os.path.isdir(dir_path) : \n",
    "            continue\n",
    "        raise RuntimeError(f\"Cannot create directory {dir_path} because it already exists and is not a directory\")\n",
    "    \n",
    "\n",
    "def create_config(config_fname, to_stdout=True) :\n",
    "    '''\n",
    "    Print environment, training and model configurations to file config_fname. Also print environment and\n",
    "    training configurations to sys.stdout if requested, but do not print model summaries as they are verbose.\n",
    "    Inputs:\n",
    "      > config_fname, str\n",
    "        name of config file to create\n",
    "      > to_stdout, bool, default=True\n",
    "        if True then repeat environment and training configurations to sys.stdout\n",
    "    Returns:\n",
    "      > None\n",
    "    '''\n",
    "    # Create message as list of strings\n",
    "    config_message = []\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    config_message.append(f\"Environment config:\\n\")\n",
    "    config_message.append(f\"> left_pad: {left_pad}\\n\")\n",
    "    config_message.append(f\"> right_pad: {right_pad}\\n\")\n",
    "    config_message.append(f\"> reward_per_turn: {reward_per_turn}\\n\")\n",
    "    config_message.append(f\"> reward_per_dx: {reward_per_dx}\\n\")\n",
    "    config_message.append(f\"> reward_at_boundary: {reward_at_boundary}\\n\")\n",
    "    config_message.append(f\"> discount_factor: {discount_factor}\\n\")\n",
    "    config_message.append(f\"> x_min: {x_min}\\n\")\n",
    "    config_message.append(f\"> x_max: {x_max}\\n\")\n",
    "    config_message.append(f\"> x_terminal: {x_terminal}\\n\")\n",
    "    config_message.append(f\"> x_range: {x_range}\\n\")\n",
    "    config_message.append(f\"> state_list: {state_list}\\n\")\n",
    "    config_message.append(f\"> num_states: {num_states}\\n\")\n",
    "    config_message.append(f\"> action_list: {action_list}\\n\")\n",
    "    config_message.append(f\"> num_actions: {num_actions}\\n\")\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    config_message.append(f\"Training config:\\n\")\n",
    "    config_message.append(f\"> max_epochs: {max_epochs}\\n\")\n",
    "    config_message.append(f\"> end_at_MSE_true: {end_at_MSE_true}\\n\")\n",
    "    config_message.append(f\"> learning_rate: {learning_rate}\\n\")\n",
    "    config_message.append(f\"> plot_estimate_after_epochs: {plot_estimate_after_epochs}\\n\")\n",
    "    config_message.append(f\"> plot_monitors_after_epochs: {plot_monitors_after_epochs}\\n\")\n",
    "    config_message.append(f\"> switch_after_epochs: {switch_after_epochs}\\n\")\n",
    "    config_message.append(f\"> clone_after_epochs: {clone_after_epochs}\\n\")\n",
    "    config_message.append(f\"> bootstrap_method: {bootstrap_method}\\n\")\n",
    "    config_message.append(f\"> num_step_returns: {num_step_returns}\\n\")\n",
    "    config_message.append(f\"=\"*114 + \"\\n\")\n",
    "    # Make sure directory exists for file\n",
    "    generate_directory_for_file_path(config_fname, print_msg_on_dir_creation=True)\n",
    "    # Open file and print messages, also to stdout if configured\n",
    "    # - also print q-model summaries, only to file\n",
    "    with open(config_fname, \"w\") as config_file :\n",
    "        for line in config_message :\n",
    "            config_file.write(line)\n",
    "            if not to_stdout : continue\n",
    "            sys.stdout.write(line)\n",
    "        \n",
    "        \n",
    "def create_value_estimate_plot(true_q_model, q_model, q_model_bs, q_target, epoch_idx=-1, \n",
    "                               show=False, close=False, save=\"\", dpi=200) :\n",
    "    '''\n",
    "    Create a plt.Figure instance visualising the greedy policy defined by the average of the q-value models \n",
    "    provided. Allows for plot to be shown, saved and/or closed using plt interface. Returns the plot figure\n",
    "    and axis objects so they can continue to be manipulated, but note that objects will no longer be in scope\n",
    "    if we have called plt.close(fig).\n",
    "    Inputs:\n",
    "      > true_q_model, np.ndarray shape (num_states, num_actions)\n",
    "        true q-values\n",
    "      > q_model, np.ndarray shape (num_states, num_actions)\n",
    "        current q-value estimates\n",
    "      > q_model_bs, np.ndarray shape (num_states, num_actions)\n",
    "        current bootstrap q-values\n",
    "      > q_target, np.ndarray shape (num_states, num_actions)\n",
    "        current target for q-values\n",
    "      > epoch_idx, int, default=-1\n",
    "        if positive then draw a text box displaying how many epochs have been performed\n",
    "      > show, bool, default=False\n",
    "        if True then call plt.show(fig)\n",
    "      > close, bool, default=False\n",
    "        if True then call plt.close(fig)\n",
    "      > save, str, default=\"\"\n",
    "        if string provided then call fig.savefig(save, ...), creating any required subdirectories if needed\n",
    "    Returns:\n",
    "      > plt.Figure instance\n",
    "        Figure object\n",
    "      > plt.Axes instance\n",
    "        Left-hand axis object\n",
    "      > plt.Axes instance\n",
    "        Right-hand axis object\n",
    "    '''\n",
    "     \n",
    "    #  Keep track of how long plotting takes, to help inform how often to call this function    \n",
    "    start_time = time.time()\n",
    "\n",
    "    #  Make plot\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    fig.set_facecolor(\"white\")\n",
    "    fig.set_alpha(1)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=12)\n",
    "    ax1.plot(state_list, q_model     [:,0], \"o-\" , c=\"r\"         , ms=5, lw=3, alpha=0.5, label=\"Estimated $q(s,a)$\")\n",
    "    ax1.plot(state_list, q_model_bs  [:,0], \"x-\" , c=\"b\"         , ms=5, lw=3, alpha=0.5, label=\"Bootstrap\")\n",
    "    ax1.plot(state_list, q_target    [:,0], \"x-\" , c=\"darkorange\", ms=5, lw=3, alpha=0.5, label=\"Target\")\n",
    "    ax1.plot(state_list, true_q_model[:,0], \".--\", c=\"gray\"      , ms=5, lw=3, alpha=0.5, label=\"True\")\n",
    "    ax1.grid(True, which='both')\n",
    "    ax1.set_xlabel(\"$x$\", labelpad=15, fontsize=14)\n",
    "    \n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=12)\n",
    "    ax2.plot(state_list, q_model     [:,1], \"o-\" , c=\"r\"         , ms=5, lw=3, alpha=0.5, label=\"Estimated $q(s,a)$\")\n",
    "    ax2.plot(state_list, q_model_bs  [:,1], \"x-\" , c=\"b\"         , ms=5, lw=3, alpha=0.5, label=\"Bootstrap\")\n",
    "    ax2.plot(state_list, q_target    [:,1], \"x-\" , c=\"darkorange\", ms=5, lw=3, alpha=0.5, label=\"Target\")\n",
    "    ax2.plot(state_list, true_q_model[:,1], \".--\", c=\"gray\"      , ms=5, lw=3, alpha=0.5, label=\"True\")\n",
    "    ax2.grid(True, which='both')\n",
    "    ax2.set_xlabel(\"$x$\", labelpad=15, fontsize=14)\n",
    "             \n",
    "    #  Draw accompanying plot objects\n",
    "    ax1.legend(loc=(0.7,1.06), ncol=5, fontsize=14, frameon=False)\n",
    "    ax1.axhline(0, lw=1, c=\"k\", ls=\"-\")\n",
    "    ax2.axhline(0, lw=1, c=\"k\", ls=\"-\")\n",
    "    ax1.text(0.01, 1.01, f\"Action: left\" , ha=\"left\", va=\"bottom\", weight=\"bold\", transform=ax1.transAxes, \n",
    "             alpha=0.8, fontsize=12, c=\"k\")\n",
    "    ax2.text(0.01, 1.01, f\"Action: right\", ha=\"left\", va=\"bottom\", weight=\"bold\", transform=ax2.transAxes, \n",
    "             alpha=0.8, fontsize=12, c=\"k\")\n",
    "    \n",
    "    #  Draw greedy policies\n",
    "    model_greedy_policy_str, bs_greedy_policy_str, true_greedy_policy_str = \"\", \"\", \"\"\n",
    "    for state in state_list :\n",
    "        if is_terminal(state) : continue\n",
    "        model_greedy_policy_str += \"L  \" if get_greedy_action(state, q_model     ) == 0 else \"R  \"\n",
    "        bs_greedy_policy_str    += \"L  \" if get_greedy_action(state, q_model_bs  ) == 0 else \"R  \"\n",
    "        true_greedy_policy_str  += \"L  \" if get_greedy_action(state, true_q_model) == 0 else \"R  \"\n",
    "    #ax2.text(1, -0.20, f\"Model policy:  {model_greedy_policy_str}\"    , ha=\"right\", va=\"top\", weight=\"bold\", fontsize=16, transform=ax2.transAxes)\n",
    "    ax2.text(1, -0.20, f\"Bootstrap policy:  {bs_greedy_policy_str}\", ha=\"right\", va=\"top\", weight=\"bold\", fontsize=16, transform=ax2.transAxes)\n",
    "    ax2.text(1, -0.30, f\"True policy:  {true_greedy_policy_str}\"     , ha=\"right\", va=\"top\", weight=\"bold\", fontsize=16, transform=ax2.transAxes)\n",
    "        \n",
    "    #  Figure out and set y-axis ranges\n",
    "    y_min   = np.min([0, np.nanmin(q_model), np.nanmin(q_model_bs), np.nanmin(true_q_model)])\n",
    "    y_max   = np.max([0, np.nanmax(q_model), np.nanmax(q_model_bs), np.nanmax(true_q_model)])\n",
    "    y_range = y_max - y_min\n",
    "    y_pad   = 0.1\n",
    "    y_lim   = [y_min - y_pad*y_range, y_max + y_pad*y_range]\n",
    "    ax1.set_ylim(y_lim)\n",
    "    ax2.set_ylim(y_lim)\n",
    "    \n",
    "    #  Draw text boxes displaying title and num. epochs\n",
    "    if epoch_idx >= 0 :\n",
    "        ax1.text(0., 1.08, f\"After {epoch_idx} epochs\", ha=\"left\", va=\"bottom\", weight=\"bold\", \n",
    "                 transform=ax1.transAxes, fontsize=14)\n",
    "       \n",
    "    #  Save / show / close\n",
    "    if len(save) > 0 :\n",
    "        generate_directory_for_file_path(save)\n",
    "        plt.savefig(save, bbox_inches=\"tight\", dpi=dpi, transparent=False)\n",
    "    if show :\n",
    "        plt.show(fig)\n",
    "    if close :\n",
    "        plt.close(fig)\n",
    "        \n",
    "    #  Return figure and axis\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "        \n",
    "def create_training_curves_plot(epochs_record, MSE_record, ref_MSE_record, true_MSE_record, accuracy_record, max_abs_q_record, \n",
    "                                true_max_abs_q=np.nan, show=False, close=False, save=\"\", dpi=300) :\n",
    "    '''\n",
    "    Create a plt.Figure instance visualising the training curves. Allows for plot to be shown, saved and/or \n",
    "    closed using plt interface. Returns the plot figure and axis objects so they can continue to be \n",
    "    manipulated, but note that objects will no longer be in scope if we have called plt.close(fig).\n",
    "    Inputs:\n",
    "      > show, bool, default=False\n",
    "        if True then call plt.show(fig)\n",
    "      > close, bool, default=False\n",
    "        if True then call plt.close(fig)\n",
    "      > save, str, default=\"\"\n",
    "        if string provided then call fig.savefig(save, ...), creating any required subdirectories if needed\n",
    "    Returns:\n",
    "      > plt.Figure instance\n",
    "      > plt.Axes instance (axis corresponding to MSE curves)\n",
    "      > plt.Axes instance (axis corresponding to ref_MSE curves)\n",
    "      > plt.Axes instance (axis corresponding to true_MSE curves)\n",
    "      > plt.Axes instance (axis corresponding to accuracy curves)\n",
    "      > plt.Axes instance (axis corresponding to max_abs_q curves)\n",
    "    '''\n",
    "            \n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "    fig.set_facecolor(\"white\")\n",
    "    fig.set_alpha(1)\n",
    "    \n",
    "    ax1 = fig.add_subplot(5, 1, 1)\n",
    "    ax1.grid(True, which='both')\n",
    "    ax1.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax1.set_title(r\"$\\mathbb{E}_{s,a}\\left[ | q(s,a) - q_{target}(s,a) |^2 \\right]$\", fontsize=30)\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    ax1.plot(epochs_record, MSE_record, \"o-\", c=\"r\", ms=5, lw=3)\n",
    "    ax1.set_yscale(\"log\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(5, 1, 2)\n",
    "    ax2.grid(True, which='both')\n",
    "    ax2.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax2.set_title(r\"$\\mathbb{E}_{\\mathrm{true greedy}~s,a}\\left[ | q(s,a) - q_{target}(s,a) |^2 \\right]$\", fontsize=30)\n",
    "    ax2.xaxis.set_ticklabels([])\n",
    "    ax2.plot(epochs_record, ref_MSE_record, \"o-\", c=\"r\", ms=5, lw=3)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    ax3 = fig.add_subplot(5, 1, 3)\n",
    "    ax3.grid(True, which='both')\n",
    "    ax3.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax3.set_title(r\"$\\mathbb{E}_{s,a}\\left[ | q(s,a) - q_{true}(s,a) |^2 \\right]$\", fontsize=30)\n",
    "    ax3.xaxis.set_ticklabels([])\n",
    "    ax3.plot(epochs_record, true_MSE_record, \"o-\", c=\"r\", ms=5, lw=3)\n",
    "    ax3.set_yscale(\"log\")\n",
    "    \n",
    "    ax4 = fig.add_subplot(5, 1, 4)\n",
    "    ax4.grid(True, which='both')\n",
    "    ax4.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax4.set_title(r\"Accuracy of greedy policy\", fontsize=30)\n",
    "    ax4.xaxis.set_ticklabels([])\n",
    "    ax4.plot(epochs_record, accuracy_record, \"o-\", c=\"r\", ms=5, lw=3)\n",
    "    ax4.axhline(0, ls=\"--\", lw=2, c=\"gray\")\n",
    "    ax4.axhline(1, ls=\"--\", lw=2, c=\"gray\")\n",
    "    \n",
    "    ax5 = fig.add_subplot(5, 1, 5)\n",
    "    ax5.grid(True, which='both')\n",
    "    ax5.tick_params(axis=\"both\", which=\"both\", right=True, top=True, direction=\"in\", labelsize=30)\n",
    "    ax5.set_title(r\"Max $|q(s,a)|$\", fontsize=30)\n",
    "    ax5.plot(epochs_record, max_abs_q_record, \"o-\", c=\"r\", ms=5, lw=3)\n",
    "    ax5.set_xlabel(r\"Epoch\", labelpad=15, fontsize=30)\n",
    "    ax5.axhline(0, ls=\"--\", lw=2, c=\"gray\")\n",
    "    if np.isfinite(true_max_abs_q) :\n",
    "        ax5.axhline(true_max_abs_q, ls=\"--\", lw=2, c=\"gray\")\n",
    "        ax5.text(0, true_max_abs_q, \"True maximum\", fontsize=20, ha=\"left\", va=\"top\", c=\"k\")\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.2)\n",
    "    \n",
    "    if len(save) > 0 :\n",
    "        generate_directory_for_file_path(save)\n",
    "        plt.savefig(save, bbox_inches=\"tight\", dpi=dpi, transparent=False)\n",
    "    if show :\n",
    "        plt.show(fig)\n",
    "    if close :\n",
    "        plt.close(fig)\n",
    "        \n",
    "    return fig, ax1, ax2, ax3, ax4, ax5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937eeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure\n",
    "\n",
    "max_epochs                  = np.inf\n",
    "end_at_MSE_true             = 0.01\n",
    "learning_rate               = 0.1\n",
    "plot_estimate_after_epochs  = 5\n",
    "plot_monitors_after_epochs  = -1\n",
    "switch_after_epochs         = -1\n",
    "clone_after_epochs          = -1\n",
    "bootstrap_method            = \"self\"       # [\"clone\", \"self\", \"other\"]\n",
    "num_step_returns            = 1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if bootstrap_method not in [\"clone\", \"self\", \"other\"] :\n",
    "    raise NotImplementedError(f\"Bootstrap method {bootstrap_method} not implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90c137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(run_tag, initial_q_model, show_curves=True) :\n",
    "    \n",
    "    ## Set up models\n",
    "    q_model  = initial_q_model.copy()\n",
    "    for a in range(num_actions) : q_model[x_terminal,a] = np.nan\n",
    "    bs_model = q_model.copy()\n",
    "\n",
    "    ## Print config to file and screen (model summaries only to file because they are verbose)\n",
    "    create_config(f\"figures/Q_learning_1D_linewalk/{run_tag}/config.txt\", to_stdout=False)\n",
    "\n",
    "    ## Create containers and method to monitor progress\n",
    "    \n",
    "    MSE_record, ref_MSE_record, true_MSE_record, accuracy_record, max_abs_q_record = [], [], [], [], []\n",
    "    epochs_record, q_record = [], []\n",
    "    \n",
    "    def calculate_monitors() :\n",
    "        empirical_errors = np.zeros(shape=(num_states, num_actions))\n",
    "        for s in range(num_states) :\n",
    "            if is_terminal(s) : continue\n",
    "            for a in range(num_actions) :\n",
    "                empirical_errors[s,a] = get_empirical_error(s, a, q_model, bs_model, num_emp_steps=num_step_returns)\n",
    "        empirical_errors_sq = empirical_errors ** 2\n",
    "        MSE       = np.mean(empirical_errors_sq)\n",
    "        ref_MSE   = np.mean([empirical_errors_sq[s,a] for s,a in greedy_state_action_pairs])\n",
    "        true_MSE  = get_mean_abs_error_between_models(q_model, true_action_values, squared=True)\n",
    "        max_abs_q = np.nanmax(np.fabs(q_model))\n",
    "        accuracy  = evaluate_model_accuracy(true_action_values, q_model)\n",
    "        return MSE, ref_MSE, true_MSE, accuracy, max_abs_q\n",
    "        \n",
    "    def record_progress(epoch_idx, q_model, MSE, ref_MSE, true_MSE, accuracy, max_abs_q) :\n",
    "        epochs_record   .append(epoch_idx)\n",
    "        q_record        .append(q_model.copy())\n",
    "        MSE_record      .append(MSE)\n",
    "        ref_MSE_record  .append(ref_MSE)\n",
    "        true_MSE_record .append(true_MSE)\n",
    "        accuracy_record .append(accuracy)\n",
    "        max_abs_q_record.append(max_abs_q)\n",
    "        \n",
    "    MSE, ref_MSE, true_MSE, accuracy, max_abs_q = calculate_monitors()\n",
    "    record_progress(0, q_model, MSE, ref_MSE, true_MSE, accuracy, max_abs_q)\n",
    "    \n",
    "    ## Start training\n",
    "\n",
    "    sys.stdout.write(f\"Starting learning\")\n",
    "    epoch_idx, start_time = 0, time.time()\n",
    "    value_function_fignames = []\n",
    "    while (epoch_idx < max_epochs or max_epochs < 0) and true_MSE_record[-1] > end_at_MSE_true :\n",
    "\n",
    "        # Determine whether to plot training curves\n",
    "        if plot_monitors_after_epochs > 0 and epoch_idx > 0 and epoch_idx % plot_monitors_after_epochs == 0 :\n",
    "            create_training_curves_plot(epochs_record, MSE_record, ref_MSE_record, true_MSE_record, accuracy_record, max_abs_q_record, true_max_abs_q,\n",
    "                                        show=False, close=True, save=f\"figures/Q_learning_1D_linewalk/{run_tag}/training_curves.pdf\")\n",
    "            \n",
    "        # Determine whether to bootstrap from self\n",
    "        if bootstrap_method == \"self\" :\n",
    "            bs_model = q_model.copy()\n",
    "        \n",
    "        # Determine whether to switch q1 and q2\n",
    "        if bootstrap_method == \"other\" and switch_after_epochs > 0 and epoch_idx > 0 and epoch_idx % switch_after_epochs == 0 :\n",
    "            q_model, bs_model = bs_model, q_model\n",
    "\n",
    "        # Determine whether to copy q1 to q2\n",
    "        if bootstrap_method == \"clone\" and clone_after_epochs > 0 and epoch_idx % clone_after_epochs == 0 :\n",
    "            bs_model = q_model.copy()\n",
    "            \n",
    "        # For each state/action pair, find the empirical error\n",
    "        q_target            = get_q_target_function(q_model, bs_model, num_emp_steps=num_step_returns)\n",
    "        empirical_errors    = q_target - q_model\n",
    "        empirical_errors_sq = empirical_errors ** 2\n",
    "        \n",
    "        # Determine whether to plot value function estimates (do now because we have the q_target values)\n",
    "        if plot_estimate_after_epochs > 0 and epoch_idx % plot_estimate_after_epochs == 0 :\n",
    "            figname = f\"figures/Q_learning_1D_linewalk/{run_tag}/value_estimates_epoch{epoch_idx}.png\"\n",
    "            create_value_estimate_plot(true_action_values, q_model, bs_model, q_target, epoch_idx=epoch_idx,\n",
    "                                       show=False, close=True, save=figname)\n",
    "            value_function_fignames.append(figname)\n",
    "                \n",
    "        # Update the action-values\n",
    "        q_model = q_model + learning_rate * empirical_errors\n",
    "        \n",
    "        # Get monitor values\n",
    "        MSE, ref_MSE, true_MSE, accuracy, max_abs_q = calculate_monitors()\n",
    "        sys.stdout.write(f\"\\rEpoch {epoch_idx+1} / {max_epochs} [t={time.time()-start_time:.2f}s] <MSE: {MSE:.4f}, ref_MSE: {ref_MSE:.4f}, true_MSE: {true_MSE:.4f}, accuracy: {accuracy:.2f}, max_abs_q: {max_abs_q:.1f}>\".ljust(110))\n",
    "\n",
    "        # Manually iterate epoch index, record monitors\n",
    "        epoch_idx += 1\n",
    "        record_progress(epoch_idx, q_model, MSE, ref_MSE, true_MSE, accuracy, max_abs_q)\n",
    "\n",
    "        \n",
    "    ## Terminate this line of stdout\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    sys.stdout.flush()\n",
    "       \n",
    "    ## Plot final training curves\n",
    "    create_training_curves_plot(epochs_record, MSE_record, ref_MSE_record, true_MSE_record, accuracy_record, max_abs_q_record, true_max_abs_q,\n",
    "                                show=show_curves, close=True, save=f\"figures/Q_learning_1D_linewalk/{run_tag}/training_curves.pdf\")\n",
    "\n",
    "    ## Plot final value functions\n",
    "    q_target = get_q_target_function(q_model, bs_model, num_emp_steps=num_step_returns)\n",
    "    figname  = f\"figures/Q_learning_1D_linewalk/{run_tag}/value_estimates_epoch{epoch_idx}.png\"\n",
    "    create_value_estimate_plot(true_action_values, q_model, bs_model, q_target, epoch_idx=epoch_idx,\n",
    "                                show=show_curves, close=True, save=figname)\n",
    "    value_function_fignames.append(figname)\n",
    "    imageio.mimsave(f\"figures/Q_learning_1D_linewalk/{run_tag}/value_estimates_animated.gif\", \n",
    "                [imageio.v2.imread(fname) for fname in value_function_fignames], fps=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672e5927",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for exp_tag, initial_q_values in experiment_configs :\\n    print(f\"\\nRUNNING EXPERIMENT TAG: {exp_tag} {initial_q_values.shape}\\n\")\\n    run_experiment(exp_tag, initial_q_values, show_curves=False)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for exp_tag, initial_q_values in experiment_configs :\n",
    "    print(f\"\\nRUNNING EXPERIMENT TAG: {exp_tag} {initial_q_values.shape}\\n\")\n",
    "    run_experiment(exp_tag, initial_q_values, show_curves=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a85ae",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. All of these experiments converge nicely! I conclude that the diverging / slowly-converging behaviour is only present when function approximation is present (demonstrate by running NB0 with a similar environment config\n",
    "\n",
    "2. Convergence occurs by first getting the 'near to end' states into position, since these are not biased by the bootstrapping and want to move directly towards their correct values. This information then gradually propagates outwards from the next-to-terminal states to all others. At any given time close to convergence, the accuracy decreases as we move away from the near-terminal states, because the bootstrapping bias compounds with every step that q_model != r + y * q_bs\n",
    "\n",
    "3. Using the MSE as a metric is flawed because it only measures the distance between the current value estimates and the target ones. However, the target is biased by bootstrapping with incorrect functional-dependence and/or incorrect action-choice. Even with a good target it is bounded by the scale of expected immediate rewards. This metric really tells us about the speed of learning, which can therefore fluctuate up/down, and does not necessarily tell us about convergence. If it falls to very small values then we probably have reached convergence, but this is may not be observable in a stochastic setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3794162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Configure multi-step experiments\\n\\nnum_step_returns = 3\\n\\nexperiment_configs = []\\nexperiment_configs.append((\"Wrong_action_and_dependence_multistep\", -.7*true_action_values))\\nexperiment_configs.append((\"Wrong_action_and_dependence_multistep_scaled_up\", -5.*true_action_values))\\nexperiment_configs.append((\"Wrong_dependence_multistep\", .7*np.array([-true_action_values[:,1], -true_action_values[:,0]]).transpose()))\\nexperiment_configs.append((\"Wrong_dependence_multistep_scaled_up\", 5.*np.array([-true_action_values[:,1], -true_action_values[:,0]]).transpose()))\\nexperiment_configs.append((\"Wrong_actions_multistep\", .7*np.array([true_action_values[:,1], true_action_values[:,0]]).transpose()))\\nexperiment_configs.append((\"Wrong_actions_multistep_scaled_up\", 5.*np.array([true_action_values[:,1], true_action_values[:,0]]).transpose()))\\nexperiment_configs.append((\"Random_values_multistep_small_v1\", np.random.normal(size=(num_states,num_actions), scale=0.1)))\\nexperiment_configs.append((\"Random_values_multistep_medium_v1\", np.random.normal(size=(num_states,num_actions), scale=1)))\\nexperiment_configs.append((\"Random_values_multistep_large_v1\", np.random.normal(size=(num_states,num_actions), scale=5)))\\n\\nfor exp_tag, initial_q_values in experiment_configs :\\n    print(f\"\\nRUNNING EXPERIMENT TAG: {exp_tag} {initial_q_values.shape}\\n\")\\n    run_experiment(exp_tag, initial_q_values, show_curves=False)\\n    \\n## Return num-steps back to 1\\n    \\nnum_step_returns = 3\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## Configure multi-step experiments\n",
    "\n",
    "num_step_returns = 3\n",
    "\n",
    "experiment_configs = []\n",
    "experiment_configs.append((\"Wrong_action_and_dependence_multistep\", -.7*true_action_values))\n",
    "experiment_configs.append((\"Wrong_action_and_dependence_multistep_scaled_up\", -5.*true_action_values))\n",
    "experiment_configs.append((\"Wrong_dependence_multistep\", .7*np.array([-true_action_values[:,1], -true_action_values[:,0]]).transpose()))\n",
    "experiment_configs.append((\"Wrong_dependence_multistep_scaled_up\", 5.*np.array([-true_action_values[:,1], -true_action_values[:,0]]).transpose()))\n",
    "experiment_configs.append((\"Wrong_actions_multistep\", .7*np.array([true_action_values[:,1], true_action_values[:,0]]).transpose()))\n",
    "experiment_configs.append((\"Wrong_actions_multistep_scaled_up\", 5.*np.array([true_action_values[:,1], true_action_values[:,0]]).transpose()))\n",
    "experiment_configs.append((\"Random_values_multistep_small_v1\", np.random.normal(size=(num_states,num_actions), scale=0.1)))\n",
    "experiment_configs.append((\"Random_values_multistep_medium_v1\", np.random.normal(size=(num_states,num_actions), scale=1)))\n",
    "experiment_configs.append((\"Random_values_multistep_large_v1\", np.random.normal(size=(num_states,num_actions), scale=5)))\n",
    "\n",
    "for exp_tag, initial_q_values in experiment_configs :\n",
    "    print(f\"\\nRUNNING EXPERIMENT TAG: {exp_tag} {initial_q_values.shape}\\n\")\n",
    "    run_experiment(exp_tag, initial_q_values, show_curves=False)\n",
    "    \n",
    "## Return num-steps back to 1\n",
    "    \n",
    "num_step_returns = 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac41a5df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreward_per_dx_backup = reward_per_dx\\nreward_per_dx        = 1.\\nexp_tag              = \"Random_values_tailored_reward_v1\"\\nupdate_true_action_values()\\nrun_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\\nreward_per_dx        = reward_per_dx_backup\\n\\ndiscount_factor_backup = discount_factor\\ndiscount_factor        = .9\\nexp_tag                = \"Random_values_gamma_0.9\"\\nupdate_true_action_values()\\nrun_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\\ndiscount_factor        = discount_factor_backup\\n\\nbootstrap_method_backup   = bootstrap_method\\nclone_after_epochs_backup = clone_after_epochs\\nbootstrap_method          = \"clone\"\\nclone_after_epochs        = 5\\nexp_tag                   = \"Random_values_clone_5\"\\nupdate_true_action_values()\\nrun_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\\nbootstrap_method          = bootstrap_method_backup\\nclone_after_epochs        = clone_after_epochs_backup\\n               \\nupdate_true_action_values()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "reward_per_dx_backup = reward_per_dx\n",
    "reward_per_dx        = 1.\n",
    "exp_tag              = \"Random_values_tailored_reward_v1\"\n",
    "update_true_action_values()\n",
    "run_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\n",
    "reward_per_dx        = reward_per_dx_backup\n",
    "\n",
    "discount_factor_backup = discount_factor\n",
    "discount_factor        = .9\n",
    "exp_tag                = \"Random_values_gamma_0.9\"\n",
    "update_true_action_values()\n",
    "run_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\n",
    "discount_factor        = discount_factor_backup\n",
    "\n",
    "bootstrap_method_backup   = bootstrap_method\n",
    "clone_after_epochs_backup = clone_after_epochs\n",
    "bootstrap_method          = \"clone\"\n",
    "clone_after_epochs        = 5\n",
    "exp_tag                   = \"Random_values_clone_5\"\n",
    "update_true_action_values()\n",
    "run_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\n",
    "bootstrap_method          = bootstrap_method_backup\n",
    "clone_after_epochs        = clone_after_epochs_backup\n",
    "               \n",
    "update_true_action_values()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9df349bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action values of optimal policy are:\n",
      "\n",
      "       | State\n",
      "Action | 0      | 1      | 2      | 3      | 4      | 5      | 6      | 7      | 8      | 9      | 10     | 11     | 12     | 13     | 14     | 15     | 16     | 17     | 18     | 19     | 20     | 21     | 22     | 23     | 24     | 25     | 26     | 27     | 28     | 29    \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "-1     | -10.35 | -9.35  | -9.28  | -9.20  | -9.11  | -9.02  | -8.91  | -8.78  | -8.65  | -8.50  | -8.33  | -8.15  | -7.94  | -7.71  | -7.46  | -7.18  | -6.86  | -6.51  | -6.13  | -5.70  | -5.22  | -4.69  | -4.10  | -3.44  | -2.71  | nan    | -1.00  | -1.90  | -2.71  | -3.44 \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1      | -9.28  | -9.20  | -9.11  | -9.02  | -8.91  | -8.78  | -8.65  | -8.50  | -8.33  | -8.15  | -7.94  | -7.71  | -7.46  | -7.18  | -6.86  | -6.51  | -6.13  | -5.70  | -5.22  | -4.69  | -4.10  | -3.44  | -2.71  | -1.90  | -1.00  | nan    | -2.71  | -3.44  | -4.10  | -5.10 \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " with a maximum |q(s,a)| of 10.353891811077338\n",
      "Epoch 250 / inf [t=15.81s] <MSE: 0.0007, ref_MSE: 0.0005, true_MSE: 0.0099, accuracy: 1.00, max_abs_q: 10.0> \n",
      "Action values of optimal policy are:\n",
      "\n",
      "       | State\n",
      "Action | 0      | 1      | 2      | 3      | 4      | 5      | 6      | 7      | 8      | 9      | 10     | 11     | 12     | 13     | 14     | 15     | 16     | 17     | 18     | 19     | 20     | 21     | 22     | 23     | 24     | 25     | 26     | 27     | 28     | 29    \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "-1     | -27.00 | -26.00 | -25.00 | -24.00 | -23.00 | -22.00 | -21.00 | -20.00 | -19.00 | -18.00 | -17.00 | -16.00 | -15.00 | -14.00 | -13.00 | -12.00 | -11.00 | -10.00 | -9.00  | -8.00  | -7.00  | -6.00  | -5.00  | -4.00  | -3.00  | nan    | -1.00  | -2.00  | -3.00  | -4.00 \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1      | -25.00 | -24.00 | -23.00 | -22.00 | -21.00 | -20.00 | -19.00 | -18.00 | -17.00 | -16.00 | -15.00 | -14.00 | -13.00 | -12.00 | -11.00 | -10.00 | -9.00  | -8.00  | -7.00  | -6.00  | -5.00  | -4.00  | -3.00  | -2.00  | -1.00  | nan    | -3.00  | -4.00  | -5.00  | -6.00 \n",
      "-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " with a maximum |q(s,a)| of 27.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "discount_factor_backup = discount_factor\n",
    "discount_factor        = .9\n",
    "exp_tag                = \"Random_values_gamma_0.9\"\n",
    "update_true_action_values()\n",
    "run_experiment(exp_tag, np.random.normal(size=(num_states,num_actions)), show_curves=False)\n",
    "discount_factor        = discount_factor_backup\n",
    "update_true_action_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
