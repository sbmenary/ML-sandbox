{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba5c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"AdamW optimizer implementation.\"\"\"\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "from keras import backend_config\n",
    "from keras.optimizers.legacy import optimizer_v2\n",
    "\n",
    "# isort: off\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "@keras_export(\n",
    "    \"keras.optimizers.legacy.AdamW\",\n",
    "    v1=[\"keras.optimizers.AdamW\", \"keras.optimizers.legacy.AdamW\"],\n",
    ")\n",
    "class AdamW(optimizer_v2.OptimizerV2):\n",
    "    r\"\"\"Optimizer that implements the AdamW algorithm.\n",
    "    AdamW optimization is a stochastic gradient descent method that is based on\n",
    "    adaptive estimation of first-order and second-order moments, with weight-decay.\n",
    "    Args:\n",
    "      learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
    "        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
    "        that takes no arguments and returns the actual value to use, The\n",
    "        learning rate. Defaults to 0.001.\n",
    "      weight_decay: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use, The\n",
    "        weight decay rate. Defaults to 0.01.\n",
    "      beta_1: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n",
    "      beta_2: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use, The\n",
    "        exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n",
    "      epsilon: A small constant for numerical stability. This epsilon is\n",
    "        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
    "        1e-7.\n",
    "      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n",
    "        the paper \"On the Convergence of AdamW and beyond\". Defaults to `False`.\n",
    "      name: Optional name for the operations created when applying gradients.\n",
    "        Defaults to `\"AdamW\"`.\n",
    "      **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n",
    "        `clipnorm`, `global_clipnorm`.\n",
    "        If `clipvalue` (float) is set, the gradient of each weight\n",
    "        is clipped to be no higher than this value.\n",
    "        If `clipnorm` (float) is set, the gradient of each weight\n",
    "        is individually clipped so that its norm is no higher than this value.\n",
    "        If `global_clipnorm` (float) is set the gradient of all weights is\n",
    "        clipped so that their global norm is no higher than this value.\n",
    "    Usage:\n",
    "    >>> opt = tf.keras.optimizers.legacy.AdamW(learning_rate=0.1)\n",
    "    >>> var1 = tf.Variable(10.0)\n",
    "    >>> loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "    >>> step_count = opt.minimize(loss, [var1]).numpy()\n",
    "    >>> # The first step is `-learning_rate*sign(grad)`\n",
    "    >>> var1.numpy()\n",
    "    9.9\n",
    "    \"\"\"\n",
    "\n",
    "    _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate = 0.001,\n",
    "        weight_decay  = 0.01,\n",
    "        beta_1        = 0.9,\n",
    "        beta_2        = 0.999,\n",
    "        epsilon       = 1e-7,\n",
    "        amsgrad       = False,\n",
    "        name=\"AdamW\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "        #self._set_hyper(\"weight_decay\" , kwargs.get(\"wd\", weight_decay))\n",
    "        self._set_hyper(\"decay\"        , self._initial_decay)\n",
    "        self._set_hyper(\"beta_1\"       , beta_1)\n",
    "        self._set_hyper(\"beta_2\"       , beta_2)\n",
    "        self.epsilon      = epsilon or backend_config.epsilon()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.amsgrad      = amsgrad\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        # Create slots for the first and second moments.\n",
    "        # Separate for-loops to respect the ordering of slot variables from v1.\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"v\")\n",
    "        if self.amsgrad:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, \"vhat\")\n",
    "\n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super()._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "        local_step   = tf.cast(self.iterations + 1, var_dtype)\n",
    "        #wd_t         = tf.identity(self._get_hyper(\"wd\"    , var_dtype))\n",
    "        beta_1_t     = tf.identity(self._get_hyper(\"beta_1\", var_dtype))\n",
    "        beta_2_t     = tf.identity(self._get_hyper(\"beta_2\", var_dtype))\n",
    "        beta_1_power = tf.pow(beta_1_t, local_step)\n",
    "        beta_2_power = tf.pow(beta_2_t, local_step)\n",
    "        lr = apply_state[(var_device, var_dtype)][\"lr_t\"] * (\n",
    "            tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n",
    "        )\n",
    "        apply_state[(var_device, var_dtype)].update(\n",
    "            dict(\n",
    "                lr                 = lr,\n",
    "                #wd_t               = wd_t,\n",
    "                wd                 = tf.convert_to_tensor(self.weight_decay, var_dtype),\n",
    "                epsilon            = tf.convert_to_tensor(self.epsilon, var_dtype),\n",
    "                beta_1_t           = beta_1_t,\n",
    "                beta_1_power       = beta_1_power,\n",
    "                one_minus_beta_1_t = 1 - beta_1_t,\n",
    "                beta_2_t           = beta_2_t,\n",
    "                beta_2_power       = beta_2_power,\n",
    "                one_minus_beta_2_t = 1 - beta_2_t,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        # If the weights are generated by Keras V1 optimizer, it includes vhats\n",
    "        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n",
    "        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[: len(params)]\n",
    "        super().set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = (apply_state or {}).get(\n",
    "            (var_device, var_dtype)\n",
    "        ) or self._fallback_apply_state(var_device, var_dtype)\n",
    "\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        if not self.amsgrad:\n",
    "            raise NotImplementedError(\"tf.raw_ops.ResourceApplyAdamW idoes not exist\")\n",
    "            return tf.raw_ops.ResourceApplyAdamW(\n",
    "                var         = var.handle,\n",
    "                m           = m.handle,\n",
    "                v           = v.handle,\n",
    "                beta1_power = coefficients[\"beta_1_power\"],\n",
    "                beta2_power = coefficients[\"beta_2_power\"],\n",
    "                lr          = coefficients[\"lr_t\"],\n",
    "                wd          = coefficients[\"wd\"],\n",
    "                beta1       = coefficients[\"beta_1_t\"],\n",
    "                beta2       = coefficients[\"beta_2_t\"],\n",
    "                epsilon     = coefficients[\"epsilon\"],\n",
    "                grad        = grad,\n",
    "                use_locking = self._use_locking,\n",
    "            )\n",
    "        else:\n",
    "            vhat = self.get_slot(var, \"vhat\")\n",
    "            raise NotImplementedError(\"tf.raw_ops.ResourceApplyAdamWWithAmsgrad idoes not exist\")\n",
    "            return tf.raw_ops.ResourceApplyAdamWWithAmsgrad(\n",
    "                var         = var.handle,\n",
    "                m           = m.handle,\n",
    "                v           = v.handle,\n",
    "                vhat        = vhat.handle,\n",
    "                beta1_power = coefficients[\"beta_1_power\"],\n",
    "                beta2_power = coefficients[\"beta_2_power\"],\n",
    "                lr          = coefficients[\"lr_t\"],\n",
    "                wd          = coefficients[\"wd\"],\n",
    "                beta1       = coefficients[\"beta_1_t\"],\n",
    "                beta2       = coefficients[\"beta_2_t\"],\n",
    "                epsilon     = coefficients[\"epsilon\"],\n",
    "                grad        = grad,\n",
    "                use_locking = self._use_locking,\n",
    "            )\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = (apply_state or {}).get(\n",
    "            (var_device, var_dtype)\n",
    "        ) or self._fallback_apply_state(var_device, var_dtype)\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * coefficients[\"one_minus_beta_1_t\"]\n",
    "        m_t = tf.compat.v1.assign(\n",
    "            m, m * coefficients[\"beta_1_t\"], use_locking=self._use_locking\n",
    "        )\n",
    "        with tf.control_dependencies([m_t]):\n",
    "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * coefficients[\"one_minus_beta_2_t\"]\n",
    "        v_t = tf.compat.v1.assign(\n",
    "            v, v * coefficients[\"beta_2_t\"], use_locking=self._use_locking\n",
    "        )\n",
    "        with tf.control_dependencies([v_t]):\n",
    "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        lr  = coefficients[\"lr\"]\n",
    "        wd  = coefficients[\"wd\"]\n",
    "        eps = coefficients[\"epsilon\"]\n",
    "        \n",
    "        if not self.amsgrad:\n",
    "            v_sqrt = tf.sqrt(v_t)\n",
    "            var_update = tf.compat.v1.assign_sub(\n",
    "                var,\n",
    "                lr * (wd*var + m_t / (v_sqrt + eps)),\n",
    "                use_locking=self._use_locking,\n",
    "            )\n",
    "            return tf.group(*[var_update, m_t, v_t])\n",
    "        else:\n",
    "            v_hat = self.get_slot(var, \"vhat\")\n",
    "            v_hat_t = tf.maximum(v_hat, v_t)\n",
    "            with tf.control_dependencies([v_hat_t]):\n",
    "                v_hat_t = tf.compat.v1.assign(\n",
    "                    v_hat, v_hat_t, use_locking=self._use_locking\n",
    "                )\n",
    "            v_hat_sqrt = tf.sqrt(v_hat_t)\n",
    "            var_update = tf.compat.v1.assign_sub(\n",
    "                var,\n",
    "                lr * (wd*var + m_t / (v_hat_sqrt + eps)),\n",
    "                use_locking=self._use_locking,\n",
    "            )\n",
    "            return tf.group(*[var_update, m_t, v_t, v_hat_t])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\" : self._serialize_hyperparameter(\"learning_rate\"),\n",
    "                \"weight_decay\"  : self.weight_decay,\n",
    "                \"decay\"         : self._initial_decay,\n",
    "                \"beta_1\"        : self._serialize_hyperparameter(\"beta_1\"),\n",
    "                \"beta_2\"        : self._serialize_hyperparameter(\"beta_2\"),\n",
    "                \"epsilon\"       : self.epsilon,\n",
    "                \"amsgrad\"       : self.amsgrad,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db660055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x_in = Input(1)\n",
    "x    = Dense(1)(x_in)\n",
    "\n",
    "model = Model(x_in, x, name=\"test_model\")\n",
    "model.compile(loss=\"mse\", optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f76751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 20:01:16.769608: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 588, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 747, in apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 806, in _distributed_apply\n        update_op = distribution.extended.update(\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 785, in apply_grad_to_update_var  **\n        update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n    File \"/var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/ipykernel_32360/3753812170.py\", line 155, in _resource_apply_dense\n        raise NotImplementedError(\"tf.raw_ops.ResourceApplyAdamW idoes not exist\")\n\n    NotImplementedError: tf.raw_ops.ResourceApplyAdamW idoes not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(X)\n\u001b[1;32m      7\u001b[0m Y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(Y)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/__autograph_generated_filenaoj5alx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 155\u001b[0m, in \u001b[0;36mAdamW._resource_apply_dense\u001b[0;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[1;32m    152\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_slot(var, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamsgrad:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.raw_ops.ResourceApplyAdamW idoes not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mraw_ops\u001b[38;5;241m.\u001b[39mResourceApplyAdamW(\n\u001b[1;32m    157\u001b[0m         var         \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    158\u001b[0m         m           \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         use_locking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_locking,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 588, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 747, in apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 806, in _distributed_apply\n        update_op = distribution.extended.update(\n    File \"/Users/Ste/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/optimizers/legacy/optimizer_v2.py\", line 785, in apply_grad_to_update_var  **\n        update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n    File \"/var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/ipykernel_32360/3753812170.py\", line 155, in _resource_apply_dense\n        raise NotImplementedError(\"tf.raw_ops.ResourceApplyAdamW idoes not exist\")\n\n    NotImplementedError: tf.raw_ops.ResourceApplyAdamW idoes not exist\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.normal(size=(200,1))\n",
    "Y = np.random.normal(size=(200,1))\n",
    "\n",
    "X = tf.constant(X)\n",
    "Y = tf.constant(Y)\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e8a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
