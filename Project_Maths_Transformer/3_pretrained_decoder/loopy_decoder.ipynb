{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884dc59d",
   "metadata": {},
   "source": [
    "#  Loopy decoder\n",
    "\n",
    "Author: S. Menary [sbmenary@gmail.com]\n",
    "\n",
    "Date: 17/5/2023\n",
    "\n",
    "Overview: Train an `integer -> sequence` model where the output sequence is a (logit probabilities over the tokens of a) text representation of the input integer. The inputs are first embedded into a higher dimensional space using a `Fourier` positional encoding with fixed frequencies spanning a logarithmic series, allowing them to encode over many orders of magnitude. The decoder is composed of a simple decoder block, which may be applied many times in over to simulate inductive logic, followed by a feed-forward block to derive the final logits. This decoder may be frozen and used in future `encoder-decoder` networks.\n",
    "\n",
    "Note: we do not use skip connections when propagating through loops\n",
    "\n",
    "Note: think about best architecture design with fixed-vector embedding (not a sequence of feature-vectors)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Set up program\n",
    "\n",
    "###  Import\n",
    "\n",
    "All imports go here at the top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5eabcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=========================##\n",
    "##   All imports go here   ##\n",
    "##=========================##\n",
    "\n",
    "##  Import entire python stdlib packages\n",
    "import logging, os, sys\n",
    "\n",
    "##  Import entire pypi packages\n",
    "import tensorflow as tf\n",
    "\n",
    "##  Remove tensorflow INFO messages\n",
    "tf.get_logger().setLevel('WARNING')\n",
    "\n",
    "##  Add directory above this to system path to expose mathsformer package location\n",
    "sys.path.append(\"/\".join(os.getcwd().split(\"/\")[:-1]))\n",
    "\n",
    "##  Import individual modules/objects from local packages\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from mathsformer import config, data, transformers, utils\n",
    "from mathsformer import selfsupervised_learning_addition_model_backend as backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d60e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##===========================================================================##\n",
    "##   Additional imports go here - to be removed after migration to backend   ##\n",
    "##===========================================================================##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers     import Add, Average, Concatenate, Embedding, Input\n",
    "from tensorflow.keras.models     import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from mathsformer.tf_objects import (DecoderBlock, EncoderBlock, Enumerate, FeedForwardBlock, LearnableMixture, MaskedCategoricalAccuracy,\n",
    "                                    MaskedSparseCategoricalCrossentropy, PositionalEncoding)\n",
    "\n",
    "from collections.abc import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2f54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==============================##\n",
    "##   Set custom config values   ##\n",
    "##==============================##\n",
    "\n",
    "custom_config = {\n",
    "    \"global\" : {\n",
    "        \"base_seed\"        : -1,\n",
    "        \"working_dir\"      : \"baseline_decoder_[problem_tag]_[model_tag]_[date]\",\n",
    "        \"problem_tag\"      : \"int123467810\",\n",
    "        \"model_tag\"        : \"baseline\",\n",
    "        \"log_lvl_iostream\" : logging.INFO,\n",
    "        \"log_lvl_fstream\"  : logging.DEBUG,\n",
    "    },\n",
    "    \"data\" : {\n",
    "        \"train_data\" : {\n",
    "            \"int_lengths\"      : [1, 2, 3, 4, 6],\n",
    "            \"batch_size\"       : 32,\n",
    "            \"num_batches\"      : 1000,\n",
    "            \"gen_base_seed\"    : 100,\n",
    "            \"gen_reproducible\" : False, \n",
    "        },\n",
    "        \"val_data\" : {\n",
    "            \"int_lengths\"      : [5, 7],\n",
    "            \"batch_size\"       : 32,\n",
    "            \"num_batches\"      : 50,\n",
    "            \"gen_base_seed\"    : 101,\n",
    "            \"gen_reproducible\" : True,\n",
    "        },\n",
    "        \"test_data\" : {\n",
    "            \"int_lengths\"      : [8, 9, 10],\n",
    "            \"batch_size\"       : 32,\n",
    "            \"num_batches\"      : 100,\n",
    "            \"gen_base_seed\"    : 102,\n",
    "            \"gen_reproducible\" : True,\n",
    "        },\n",
    "        \"characters\"              : ['M', 'B', 'E', 'N', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-'],\n",
    "        \"mask_char\"               : 'M',\n",
    "        \"seq_start_char\"          : 'B',\n",
    "        \"seq_end_char\"            : 'E',\n",
    "        \"negative_char\"           : 'N',\n",
    "        \"dtype_int\"               : \"int64\",\n",
    "        \"dtype\"                   : \"int32\",\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"load_pretrained_model\" : None,\n",
    "        \"name\"                  : \"decoder_model\",\n",
    "        \"dtype\"                 : \"float32\",\n",
    "        \"dropout\"               : 0.1,\n",
    "        \"jit_compile\"           : False,\n",
    "        \"use_old_loss\"          : True,\n",
    "        \"optimizer\"             : Adam,\n",
    "        \"optimizer_args\"        : {\"learning_rate\":1e-4},\n",
    "        \"int_positional_encoding\" : {\n",
    "            \"num_freqs\"         : 128,\n",
    "            \"min_period\"        : 3,\n",
    "            \"max_period\"        : 1e13,\n",
    "            \"learnable\"         : True,\n",
    "        },\n",
    "        \"str_positional_encoding\" : {\n",
    "            \"num_freqs\"         : 32,\n",
    "            \"min_period\"        : 4,\n",
    "            \"max_period\"        : 400,\n",
    "            \"learnable\"         : True,\n",
    "        },\n",
    "        \"ndim_embedding\"        : 32,\n",
    "        \"pre_decoder\" : {\n",
    "            \"num_layers\"        : -1,\n",
    "            \"ndim\"              : 256,\n",
    "            \"skip_connect\"      : True,\n",
    "        },\n",
    "        \"decoder\" : {\n",
    "            \"num_blocks\"        : 6,\n",
    "            \"num_loops\"         : 1,\n",
    "            \"num_heads\"         : 8,\n",
    "            \"ndim\"              : 32,\n",
    "            \"ndim_att_hidden\"   : 128,\n",
    "            \"ndim_ff_hidden\"    : 512,\n",
    "            \"skip_connect\"      : True,\n",
    "        },\n",
    "        \"post_decoder\" : {\n",
    "            \"num_layers\"        : 3,\n",
    "            \"ndim\"              : 512,\n",
    "        },\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"train\"          : True,\n",
    "        \"max_epochs\"     : 100000,\n",
    "        \"log_after_epoch\" : {\n",
    "            \"do\"          : True,\n",
    "            \"log_lvl\"     : logging.DEBUG,\n",
    "        },\n",
    "        \"early_stopping\" : {\n",
    "            \"do\"                   : True,\n",
    "            \"patience\"             : 5,\n",
    "            \"monitor\"              : \"val_loss\",\n",
    "            \"mode\"                 : \"min\",\n",
    "            \"restore_best_weights\" : True,\n",
    "        },\n",
    "        \"model_checkpoint\" : {\n",
    "            \"do\"       : True,\n",
    "            \"filename\" : \"model_checkpoint_epoch{epoch}_val_loss_{val_loss:.5}.h5\",\n",
    "        },\n",
    "        \"layer_weights_record\" : {\n",
    "            \"do\"               : True,\n",
    "            \"batch_frequency\"  : 2000,\n",
    "            \"recursive\"        : True,\n",
    "        },\n",
    "        \"adaptive_learning_rate\" : {\n",
    "            \"do\"                 : True,\n",
    "            \"decay_factor\"       : 0.3,\n",
    "            \"monitor\"            : \"loss\",\n",
    "            \"mode\"               : \"min\",\n",
    "            \"patience\"           : 1,\n",
    "            \"log_lvl\"            : logging.DEBUG,\n",
    "        },\n",
    "    },\n",
    "    \"evaluate\" : {\n",
    "        \"num_print\"            : 20,\n",
    "        \"save_model\"           : True,\n",
    "        \"plot_weights\"         : False,\n",
    "        \"plot_training_curves\" : True,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d07307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "===   Config created   ===\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "##===================================##\n",
    "##   Load and validate full config   ##\n",
    "##===================================##\n",
    "\n",
    "##  Create config object containing default values\n",
    "cfg = config.Config(backend.DEFAULT_CONFIG)\n",
    "\n",
    "##  Override with custom values\n",
    "cfg.load_dict(custom_config)\n",
    "\n",
    "##  Validate config\n",
    "backend.validate_config(cfg)\n",
    "\n",
    "##  Print success\n",
    "print(utils.fancy_message(f\"Config created\"))\n",
    "\n",
    "##  For convenience, split configs for different sections\n",
    "cfg_global   = cfg[\"global\"  ]\n",
    "cfg_data     = cfg[\"data\"    ]\n",
    "cfg_model    = cfg[\"model\"   ]\n",
    "cfg_training = cfg[\"training\"]\n",
    "cfg_evaluate = cfg[\"evaluate\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4943616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "===   Working directory created at baseline_decoder_int123467810_baseline_2023_06_14   ===\n",
      "==========================================================================================\n",
      "   INFO initialise_logging: Begin logging on 2023-06-14 at 16:31:01\n",
      "   INFO initialise_program: Program description: unsupervised_learning_addition_model_generator (notebook)\n",
      "   INFO initialise_program: Working directory: baseline_decoder_int123467810_baseline_2023_06_14\n",
      "   INFO log_versions: ------------------------------------------------------+-----------------------------------------------------------------------------------\n",
      "   INFO log_versions:                                              PACKAGE  |  VERSION\n",
      "   INFO log_versions: ------------------------------------------------------+-----------------------------------------------------------------------------------\n",
      "   INFO log_versions:                                               Python  |  3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]\n",
      "   INFO log_versions:                                              IPython  |  8.13.2\n",
      "   INFO log_versions:                                 IPython.core.release  |  8.13.2\n",
      "   INFO log_versions:                                                  PIL  |  9.5.0\n",
      "   INFO log_versions:                                            PIL.Image  |  9.5.0\n",
      "   INFO log_versions:                                       PIL._deprecate  |  9.5.0\n",
      "   INFO log_versions:                                         PIL._version  |  9.5.0\n",
      "   INFO log_versions:                                                 _csv  |  1.0\n",
      "   INFO log_versions:                                              _ctypes  |  1.1.0\n",
      "   INFO log_versions:                                              _curses  |  b'2.2'\n",
      "   INFO log_versions:                                              decimal  |  1.70\n",
      "   INFO log_versions:                               _pydev_bundle.fsnotify  |  0.1.5\n",
      "   INFO log_versions:                 _pydevd_frame_eval.vendored.bytecode  |  0.13.0.dev\n",
      "   INFO log_versions:                                              appnope  |  0.1.3\n",
      "   INFO log_versions:                                             argparse  |  1.1\n",
      "   INFO log_versions:                                           astunparse  |  1.6.3\n",
      "   INFO log_versions:                                             backcall  |  0.2.0\n",
      "   INFO log_versions:                                              certifi  |  2023.05.07\n",
      "   INFO log_versions:                                                 cffi  |  1.15.1\n",
      "   INFO log_versions:                                   charset_normalizer  |  3.1.0\n",
      "   INFO log_versions:                           charset_normalizer.version  |  3.1.0\n",
      "   INFO log_versions:                                                 comm  |  0.1.3\n",
      "   INFO log_versions:                                                  csv  |  1.0\n",
      "   INFO log_versions:                                               ctypes  |  1.1.0\n",
      "   INFO log_versions:                                      ctypes.macholib  |  1.0\n",
      "   INFO log_versions:                                               cycler  |  0.10.0\n",
      "   INFO log_versions:                                             dateutil  |  2.8.2\n",
      "   INFO log_versions:                                              debugpy  |  1.6.7\n",
      "   INFO log_versions:                                   debugpy.public_api  |  1.6.7\n",
      "   INFO log_versions:                                              decimal  |  1.70\n",
      "   INFO log_versions:                                            decorator  |  5.1.1\n",
      "   INFO log_versions:                                           defusedxml  |  0.7.1\n",
      "   INFO log_versions:                                            distutils  |  3.10.11\n",
      "   INFO log_versions:                                            executing  |  1.2.0\n",
      "   INFO log_versions:                                    executing.version  |  1.2.0\n",
      "   INFO log_versions:                                          flatbuffers  |  23.5.9\n",
      "   INFO log_versions:                                 flatbuffers._version  |  23.5.9\n",
      "   INFO log_versions:                                      google.protobuf  |  4.23.1\n",
      "   INFO log_versions:                                                 h5py  |  3.6.0\n",
      "   INFO log_versions:                                          http.server  |  0.6\n",
      "   INFO log_versions:                                                 idna  |  3.4\n",
      "   INFO log_versions:                                        idna.idnadata  |  15.0.0\n",
      "   INFO log_versions:                                    idna.package_data  |  3.4\n",
      "   INFO log_versions:                                            ipaddress  |  1.0\n",
      "   INFO log_versions:                                            ipykernel  |  6.23.1\n",
      "   INFO log_versions:                                   ipykernel._version  |  6.23.1\n",
      "   INFO log_versions:                                                 jedi  |  0.18.2\n",
      "   INFO log_versions:                                                 json  |  2.0.9\n",
      "   INFO log_versions:                                       jupyter_client  |  8.2.0\n",
      "   INFO log_versions:                              jupyter_client._version  |  8.2.0\n",
      "   INFO log_versions:                                         jupyter_core  |  5.3.0\n",
      "   INFO log_versions:                                 jupyter_core.version  |  5.3.0\n",
      "   INFO log_versions:                                                keras  |  2.13.1rc0\n",
      "   INFO log_versions:                                            keras.src  |  2.13.1\n",
      "   INFO log_versions:                                           kiwisolver  |  1.4.4\n",
      "   INFO log_versions:                                     kiwisolver._cext  |  1.4.4\n",
      "   INFO log_versions:                                              logging  |  0.5.1.2\n",
      "   INFO log_versions:                                           matplotlib  |  3.7.1\n",
      "   INFO log_versions:                                  matplotlib._version  |  3.7.1\n",
      "   INFO log_versions:                                                numpy  |  1.23.2\n",
      "   INFO log_versions:                                           numpy.core  |  1.23.2\n",
      "   INFO log_versions:                         numpy.core._multiarray_umath  |  3.1\n",
      "   INFO log_versions:                                            numpy.lib  |  1.23.2\n",
      "   INFO log_versions:                           numpy.linalg._umath_linalg  |  0.1.5\n",
      "   INFO log_versions:                                        numpy.version  |  1.23.2\n",
      "   INFO log_versions:                                           opt_einsum  |  v3.3.0\n",
      "   INFO log_versions:                                            packaging  |  23.1\n",
      "   INFO log_versions:                                                parso  |  0.8.3\n",
      "   INFO log_versions:                                              pexpect  |  4.8.0\n",
      "   INFO log_versions:                                          pickleshare  |  0.7.5\n",
      "   INFO log_versions:                 pkg_resources._vendor.more_itertools  |  9.0.0\n",
      "   INFO log_versions:                      pkg_resources._vendor.packaging  |  23.0\n",
      "   INFO log_versions:                   pkg_resources._vendor.platformdirs  |  2.6.2\n",
      "   INFO log_versions:           pkg_resources._vendor.platformdirs.version  |  2.6.2\n",
      "   INFO log_versions:                 pkg_resources._vendor.more_itertools  |  9.0.0\n",
      "   INFO log_versions:                      pkg_resources._vendor.packaging  |  23.0\n",
      "   INFO log_versions:                   pkg_resources._vendor.platformdirs  |  2.6.2\n",
      "   INFO log_versions:                                             platform  |  1.0.8\n",
      "   INFO log_versions:                                         platformdirs  |  3.5.1\n",
      "   INFO log_versions:                                 platformdirs.version  |  3.5.1\n",
      "   INFO log_versions:                                       prompt_toolkit  |  3.0.38\n",
      "   INFO log_versions:                                               psutil  |  5.9.5\n",
      "   INFO log_versions:                                           ptyprocess  |  0.7.0\n",
      "   INFO log_versions:                                            pure_eval  |  0.2.2\n",
      "   INFO log_versions:                                    pure_eval.version  |  0.2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO log_versions:                                               pydevd  |  2.9.5\n",
      "   INFO log_versions:                                             pygments  |  2.15.1\n",
      "   INFO log_versions:                                            pyparsing  |  3.0.9\n",
      "   INFO log_versions:                                                   re  |  2.2.1\n",
      "   INFO log_versions:                                             requests  |  2.31.0\n",
      "   INFO log_versions:                                 requests.__version__  |  2.31.0\n",
      "   INFO log_versions:                                                 idna  |  3.4\n",
      "   INFO log_versions:                                        idna.idnadata  |  15.0.0\n",
      "   INFO log_versions:                                    idna.package_data  |  3.4\n",
      "   INFO log_versions:                                              urllib3  |  1.26.15\n",
      "   INFO log_versions:                                     urllib3._version  |  1.26.15\n",
      "   INFO log_versions:                                   urllib3.connection  |  1.26.15\n",
      "   INFO log_versions:                                 urllib3.packages.six  |  1.16.0\n",
      "   INFO log_versions:                      urllib3.util.ssl_match_hostname  |  3.5.0.1\n",
      "   INFO log_versions:                                       requests.utils  |  2.31.0\n",
      "   INFO log_versions:                                           setuptools  |  67.7.2\n",
      "   INFO log_versions:                                            distutils  |  3.10.11\n",
      "   INFO log_versions:                    setuptools._vendor.more_itertools  |  8.8.0\n",
      "   INFO log_versions:                       setuptools._vendor.ordered_set  |  3.1\n",
      "   INFO log_versions:                         setuptools._vendor.packaging  |  23.0\n",
      "   INFO log_versions:                    setuptools._vendor.more_itertools  |  8.8.0\n",
      "   INFO log_versions:                       setuptools._vendor.ordered_set  |  3.1\n",
      "   INFO log_versions:                         setuptools._vendor.packaging  |  23.0\n",
      "   INFO log_versions:                                   setuptools.version  |  67.7.2\n",
      "   INFO log_versions:                                                  six  |  1.16.0\n",
      "   INFO log_versions:                                         socketserver  |  0.4\n",
      "   INFO log_versions:                                           stack_data  |  0.6.2\n",
      "   INFO log_versions:                                   stack_data.version  |  0.6.2\n",
      "   INFO log_versions:                                          tensorboard  |  2.13.0\n",
      "   INFO log_versions:                   tensorboard.compat.tensorflow_stub  |  stub\n",
      "   INFO log_versions: tensorboard.compat.tensorflow_stub.pywrap_tensorflow  |  0\n",
      "   INFO log_versions:                                           tensorflow  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                         tensorflow._api.v2.compat.v1  |  2.14.0-dev20230613\n",
      "   INFO log_versions:               tensorflow._api.v2.compat.v1.compat.v1  |  2.14.0-dev20230613\n",
      "   INFO log_versions:               tensorflow._api.v2.compat.v1.compat.v2  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                         tensorflow._api.v2.compat.v2  |  2.14.0-dev20230613\n",
      "   INFO log_versions:               tensorflow._api.v2.compat.v2.compat.v1  |  2.14.0-dev20230613\n",
      "   INFO log_versions:               tensorflow._api.v2.compat.v2.compat.v2  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                                 tensorflow.compat.v1  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                       tensorflow.compat.v1.compat.v1  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                       tensorflow.compat.v1.compat.v2  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                                 tensorflow.compat.v2  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                       tensorflow.compat.v2.compat.v1  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                       tensorflow.compat.v2.compat.v2  |  2.14.0-dev20230613\n",
      "   INFO log_versions:           tensorflow.python.client.pywrap_tf_session  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                 tensorflow.python.framework.versions  |  2.14.0-dev20230613\n",
      "   INFO log_versions:                              tensorflow.python.keras  |  2.6.0\n",
      "   INFO log_versions:                                            traitlets  |  5.9.0\n",
      "   INFO log_versions:                                   traitlets._version  |  5.9.0\n",
      "   INFO log_versions:                                       urllib.request  |  3.10\n",
      "   INFO log_versions:                                              urllib3  |  1.26.15\n",
      "   INFO log_versions:                                     urllib3._version  |  1.26.15\n",
      "   INFO log_versions:                                   urllib3.connection  |  1.26.15\n",
      "   INFO log_versions:                                 urllib3.packages.six  |  1.16.0\n",
      "   INFO log_versions:                      urllib3.util.ssl_match_hostname  |  3.5.0.1\n",
      "   INFO log_versions:                                              wcwidth  |  0.2.6\n",
      "   INFO log_versions:                                                wrapt  |  1.15.0\n",
      "   INFO log_versions:                                        xmlrpc.client  |  3.10\n",
      "   INFO log_versions:                                                 zlib  |  1.0\n",
      "   INFO log_versions:                                                  zmq  |  25.0.2\n",
      "   INFO log_versions:                                            zmq.sugar  |  25.0.2\n",
      "   INFO log_versions:                                    zmq.sugar.version  |  25.0.2\n",
      "   INFO log_versions: ------------------------------------------------------+-----------------------------------------------------------------------------------\n",
      "   INFO initialise_program: Registered config value global > base_seed: -1\n",
      "   INFO initialise_program: Registered config value global > working_dir: baseline_decoder_[problem_tag]_[model_tag]_[date]\n",
      "   INFO initialise_program: Registered config value global > problem_tag: int123467810\n",
      "   INFO initialise_program: Registered config value global > model_tag: baseline\n",
      "   INFO initialise_program: Registered config value global > log_lvl_iostream: 20\n",
      "   INFO initialise_program: Registered config value global > log_lvl_fstream: 10\n",
      "   INFO initialise_program: Registered config value data > train_data > int_lengths: [1, 2, 3, 4, 6]\n",
      "   INFO initialise_program: Registered config value data > train_data > num_ints: [1, 2, 4]\n",
      "   INFO initialise_program: Registered config value data > train_data > batch_size: 32\n",
      "   INFO initialise_program: Registered config value data > train_data > num_batches: 1000\n",
      "   INFO initialise_program: Registered config value data > train_data > gen_base_seed: 100\n",
      "   INFO initialise_program: Registered config value data > train_data > gen_reproducible: False\n",
      "   INFO initialise_program: Registered config value data > val_data > int_lengths: [5, 7]\n",
      "   INFO initialise_program: Registered config value data > val_data > num_ints: [3]\n",
      "   INFO initialise_program: Registered config value data > val_data > batch_size: 32\n",
      "   INFO initialise_program: Registered config value data > val_data > num_batches: 50\n",
      "   INFO initialise_program: Registered config value data > val_data > gen_base_seed: 101\n",
      "   INFO initialise_program: Registered config value data > val_data > gen_reproducible: True\n",
      "   INFO initialise_program: Registered config value data > test_data > int_lengths: [8, 9, 10]\n",
      "   INFO initialise_program: Registered config value data > test_data > num_ints: [3]\n",
      "   INFO initialise_program: Registered config value data > test_data > batch_size: 32\n",
      "   INFO initialise_program: Registered config value data > test_data > num_batches: 100\n",
      "   INFO initialise_program: Registered config value data > test_data > gen_base_seed: 102\n",
      "   INFO initialise_program: Registered config value data > test_data > gen_reproducible: True\n",
      "   INFO initialise_program: Registered config value data > characters: ['M', 'B', 'E', 'N', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-']\n",
      "   INFO initialise_program: Registered config value data > mask_char: M\n",
      "   INFO initialise_program: Registered config value data > seq_start_char: B\n",
      "   INFO initialise_program: Registered config value data > seq_end_char: E\n",
      "   INFO initialise_program: Registered config value data > negative_char: N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO initialise_program: Registered config value data > dtype: int32\n",
      "   INFO initialise_program: Registered config value data > dtype_int: int64\n",
      "   INFO initialise_program: Registered config value model > load_pretrained_model: None\n",
      "   INFO initialise_program: Registered config value model > name: decoder_model\n",
      "   INFO initialise_program: Registered config value model > dtype: float32\n",
      "   INFO initialise_program: Registered config value model > dropout: 0.1\n",
      "   INFO initialise_program: Registered config value model > learning_rate: 0.001\n",
      "   INFO initialise_program: Registered config value model > jit_compile: False\n",
      "   INFO initialise_program: Registered config value model > positional_encoding > num_freqs: 16\n",
      "   INFO initialise_program: Registered config value model > positional_encoding > min_period: 4\n",
      "   INFO initialise_program: Registered config value model > positional_encoding > max_period: 250\n",
      "   INFO initialise_program: Registered config value model > ndim_embedding: 32\n",
      "   INFO initialise_program: Registered config value model > comb_type: average\n",
      "   INFO initialise_program: Registered config value model > pre_encoder > num_layers: -1\n",
      "   INFO initialise_program: Registered config value model > pre_encoder > ndim: 128\n",
      "   INFO initialise_program: Registered config value model > pre_encoder > skip_connect: True\n",
      "   INFO initialise_program: Registered config value model > pre_decoder > num_layers: -1\n",
      "   INFO initialise_program: Registered config value model > pre_decoder > ndim: 256\n",
      "   INFO initialise_program: Registered config value model > pre_decoder > skip_connect: True\n",
      "   INFO initialise_program: Registered config value model > encoder > num_blocks: 5\n",
      "   INFO initialise_program: Registered config value model > encoder > num_heads: 8\n",
      "   INFO initialise_program: Registered config value model > encoder > ndim: 32\n",
      "   INFO initialise_program: Registered config value model > encoder > ndim_att_hidden: 32\n",
      "   INFO initialise_program: Registered config value model > encoder > ndim_ff_hidden: 128\n",
      "   INFO initialise_program: Registered config value model > encoder > skip_connect: True\n",
      "   INFO initialise_program: Registered config value model > decoder > num_blocks: 6\n",
      "   INFO initialise_program: Registered config value model > decoder > num_heads: 8\n",
      "   INFO initialise_program: Registered config value model > decoder > ndim: 32\n",
      "   INFO initialise_program: Registered config value model > decoder > ndim_att_hidden: 128\n",
      "   INFO initialise_program: Registered config value model > decoder > ndim_ff_hidden: 512\n",
      "   INFO initialise_program: Registered config value model > decoder > skip_connect: True\n",
      "   INFO initialise_program: Registered config value model > decoder > num_loops: 1\n",
      "   INFO initialise_program: Registered config value model > post_decoder > num_layers: 3\n",
      "   INFO initialise_program: Registered config value model > post_decoder > ndim: 512\n",
      "   INFO initialise_program: Registered config value model > use_old_loss: True\n",
      "   INFO initialise_program: Registered config value model > optimizer: <class 'keras.src.optimizers.adam.Adam'>\n",
      "   INFO initialise_program: Registered config value model > optimizer_args > learning_rate: 0.0001\n",
      "   INFO initialise_program: Registered config value model > int_positional_encoding > num_freqs: 128\n",
      "   INFO initialise_program: Registered config value model > int_positional_encoding > min_period: 3\n",
      "   INFO initialise_program: Registered config value model > int_positional_encoding > max_period: 10000000000000.0\n",
      "   INFO initialise_program: Registered config value model > int_positional_encoding > learnable: True\n",
      "   INFO initialise_program: Registered config value model > str_positional_encoding > num_freqs: 32\n",
      "   INFO initialise_program: Registered config value model > str_positional_encoding > min_period: 4\n",
      "   INFO initialise_program: Registered config value model > str_positional_encoding > max_period: 400\n",
      "   INFO initialise_program: Registered config value model > str_positional_encoding > learnable: True\n",
      "   INFO initialise_program: Registered config value training > train: True\n",
      "   INFO initialise_program: Registered config value training > max_epochs: 100000\n",
      "   INFO initialise_program: Registered config value training > log_after_epoch > do: True\n",
      "   INFO initialise_program: Registered config value training > log_after_epoch > log_lvl: 10\n",
      "   INFO initialise_program: Registered config value training > early_stopping > do: True\n",
      "   INFO initialise_program: Registered config value training > early_stopping > patience: 5\n",
      "   INFO initialise_program: Registered config value training > early_stopping > monitor: val_loss\n",
      "   INFO initialise_program: Registered config value training > early_stopping > mode: min\n",
      "   INFO initialise_program: Registered config value training > early_stopping > restore_best_weights: True\n",
      "   INFO initialise_program: Registered config value training > model_checkpoint > do: True\n",
      "   INFO initialise_program: Registered config value training > model_checkpoint > filename: model_checkpoint_epoch{epoch}_val_loss_{val_loss:.5}.h5\n",
      "   INFO initialise_program: Registered config value training > layer_weights_record > do: True\n",
      "   INFO initialise_program: Registered config value training > layer_weights_record > batch_frequency: 2000\n",
      "   INFO initialise_program: Registered config value training > layer_weights_record > recursive: True\n",
      "   INFO initialise_program: Registered config value training > adaptive_learning_rate > do: True\n",
      "   INFO initialise_program: Registered config value training > adaptive_learning_rate > decay_factor: 0.3\n",
      "   INFO initialise_program: Registered config value training > adaptive_learning_rate > monitor: loss\n",
      "   INFO initialise_program: Registered config value training > adaptive_learning_rate > mode: min\n",
      "   INFO initialise_program: Registered config value training > adaptive_learning_rate > patience: 1\n",
      "   INFO initialise_program: Registered config value training > adaptive_learning_rate > log_lvl: 10\n",
      "   INFO initialise_program: Registered config value evaluate > num_print: 20\n",
      "   INFO initialise_program: Registered config value evaluate > save_model: True\n",
      "   INFO initialise_program: Registered config value evaluate > plot_weights: False\n",
      "   INFO initialise_program: Registered config value evaluate > plot_training_curves: True\n",
      "   INFO initialise_program: Python random seed set: 1686756661\n",
      "   INFO initialise_program: Numpy random seed set: 1686756662\n",
      "   INFO initialise_program: TensorFlow random seed set: 1686756663\n"
     ]
    }
   ],
   "source": [
    "##==============================##\n",
    "##   Create working directory   ##\n",
    "##==============================##\n",
    "\n",
    "##  Report success\n",
    "working_dir, logger, base_seed, np_seed, tf_seed = utils.initialise_program(\n",
    "    \"unsupervised_learning_addition_model_generator (notebook)\", \n",
    "    working_dir       = cfg_global[\"working_dir\"], \n",
    "    cfg               = cfg,\n",
    "    base_seed         = cfg_global[\"base_seed\"],\n",
    "    log_lvl_iostream  = cfg_global[\"log_lvl_iostream\"],\n",
    "    log_lvl_fstream   = cfg_global[\"log_lvl_fstream\" ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492137a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO summary: TokenTransform of dtype int32 with 16 characters: ['M', 'B', 'E', 'N', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-']\n",
      "   INFO summary: Special characters are seq_start_char (B), seq_end_char (E), mask_char (M)\n",
      "   INFO summary: Tokeniser dictionary is {'M': 0, 'B': 1, 'E': 2, 'N': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '+': 14, '-': 15}\n",
      "   INFO summary: Detokeniser dictionary is {0: 'M', 1: 'B', 2: 'E', 3: 'N', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '+', 15: '-'}\n"
     ]
    }
   ],
   "source": [
    "##======================##\n",
    "##   Create tokeniser   ##\n",
    "##======================##\n",
    "\n",
    "token_transform = data.TokenTransform.from_dictionary(cfg_data)\n",
    "token_transform.summary(print_fn=logger.info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d156c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_text_to_text_model(vocab_length:int, \n",
    "                              name:str, \n",
    "                              do_compile:bool     = True,\n",
    "                              use_old_loss:bool   = False,\n",
    "                              dtype_encoded_in    = tf.int32, \n",
    "                              dtype_decoder_in    = tf.int32, \n",
    "                              dtype               = tf.float32, \n",
    "                              dropout:float       = 0.1, \n",
    "                              jit_compile:bool    = None,\n",
    "                              optimizer           = Adam,\n",
    "                              optimizer_args:dict = None,\n",
    "                              encoder_num_freqs:int = 96, encoder_min_period:float = 3, encoder_max_period:float = 1e13, encoder_learnable:bool = False,\n",
    "                              pos_enc_num_freqs:int = 32, pos_enc_min_period:float = 4, pos_enc_max_period:float = 500 , pos_enc_learnable:bool = False,\n",
    "                              ndim_embedding:int          = 64,\n",
    "                              num_decoder_blocks:int      = 6 , ndim_decoder:int            = 64 , skip_connect_decoder:bool  = True,\n",
    "                              num_heads_decoder:int       = 8 , ndim_att_hidden_decoder:int = 128, ndim_ff_hidden_decoder:int = 128, \n",
    "                              num_decoder_loops:int       = 1 ,\n",
    "                              num_post_layers_decoder:int = 3 , ndim_post_layers_decoder:int = 512, \n",
    "                             ) :\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ##  Resolve mutable default args\n",
    "    if optimizer_args is None :\n",
    "        optimizer_args = {'learning_rate': 1e-3}\n",
    "    \n",
    "    ##=============================================##\n",
    "    ##===   Input layer - Output shape [B, S]   ===##\n",
    "    ##=============================================##\n",
    "    x_in_enc = Input((None,), dtype=dtype_encoded_in, name=f\"{name}_encoded_input_layer\")\n",
    "    x_in_dec = Input((None,), dtype=dtype_decoder_in, name=f\"{name}_decoder_input_layer\")\n",
    "            \n",
    "    ##===========================================================================##\n",
    "    ##===  Token embedding, masking 0s - Output shape [B, S, ndim_embedding]  ===##\n",
    "    ##===========================================================================##\n",
    "    x_embed_dec = Embedding(vocab_length, \n",
    "                            ndim_embedding, \n",
    "                            mask_zero=True, \n",
    "                            dtype=dtype, \n",
    "                            name=f\"{name}_decoder_embedding\")(x_in_dec)\n",
    "    \n",
    "    ##=========================================================================##\n",
    "    ##===  Enumerate indices for positional encoding - Output shape [B, S]  ===##\n",
    "    ##=========================================================================##\n",
    "    x_pos_dec = Enumerate(name=f\"{name}_decoder_enumerate\", dtype=dtype)(x_in_dec, minimal_dims=False)\n",
    "    \n",
    "    ##========================================================================##\n",
    "    ##===  Positional encoding - Output shape [B, S, 2*pos_enc_num_freqs]  ===##\n",
    "    ##========================================================================##\n",
    "    x_pos_enc = PositionalEncoding(num_freqs  = encoder_num_freqs, \n",
    "                                   min_period = encoder_min_period, \n",
    "                                   max_period = encoder_max_period, \n",
    "                                   learnable  = encoder_learnable,\n",
    "                                   dtype      = dtype, \n",
    "                                   name       = f\"{name}_encoder_position_encoding\")(x_in_enc)\n",
    "    \n",
    "    x_pos_dec = PositionalEncoding(num_freqs  = pos_enc_num_freqs, \n",
    "                                   min_period = pos_enc_min_period, \n",
    "                                   max_period = pos_enc_max_period, \n",
    "                                   learnable  = pos_enc_learnable,\n",
    "                                   dtype      = dtype, \n",
    "                                   name       = f\"{name}_decoder_position_encoding\")(x_pos_dec)\n",
    "    \n",
    "    ##============================================================##\n",
    "    ##===  Decoder blocks - Output shape [B, S, ndim_decoder]  ===##\n",
    "    ##============================================================##\n",
    "    decoder_blocks = []\n",
    "    for layer_idx in range(num_decoder_blocks) :\n",
    "        decoder_blocks.append(DecoderBlock(\n",
    "                                 ndim_decoder, \n",
    "                                 num_heads_decoder, \n",
    "                                 ndim_att_hidden_decoder, \n",
    "                                 ndim_ff_hidden_decoder, \n",
    "                                 dropout_mha  = dropout, \n",
    "                                 dtype        = dtype, \n",
    "                                 layer_norm   = True, \n",
    "                                 skip_connect = False, \n",
    "                                 name         = f\"{name}_decoder_block_{layer_idx+1}\"))\n",
    "        \n",
    "    x_dec = x_embed_dec\n",
    "    for loop_idx in range(num_decoder_loops) :\n",
    "        x_dec = Concatenate(name=f\"{name}_decoder_emb_and_pos_loop{loop_idx+1}\", dtype=dtype)([x_dec, x_pos_dec])\n",
    "        for decoder_block in decoder_blocks :\n",
    "            x_dec = decoder_block([x_dec, x_pos_enc])\n",
    "        \n",
    "    ##==================================================================================================##\n",
    "    ##===  Predict logit probabilities using feed-forward block - Output shape [B, S, vocab_length]  ===##\n",
    "    ##==================================================================================================##\n",
    "    ##  - use layer_norm instead of batch_norm because elements in sequence are not independent\n",
    "    x = FeedForwardBlock(vocab_length, \n",
    "                         ndim_hidden       = ndim_post_layers_decoder, \n",
    "                         num_hidden_layers = num_post_layers_decoder, \n",
    "                         skip_connect      = False, \n",
    "                         layer_norm        = True, \n",
    "                         batch_norm        = False, \n",
    "                         dtype             = dtype, \n",
    "                         name              = f\"{name}_feedfwd_block_post_attention\")(x_dec)\n",
    "    \n",
    "    ##  Create model\n",
    "    model = Model([x_in_enc, x_in_dec], x, name=name)\n",
    "    \n",
    "    ##  Compile model with sparse categorical crossentropy loss and accuracy metric\n",
    "    if do_compile :\n",
    "        acc  = MaskedCategoricalAccuracy(scalar_output=True, equal_token_weight=True, use_keras_mask=False, mask_value=0)\n",
    "        loss = MaskedSparseCategoricalCrossentropy(scalar_output=True, equal_token_weight=True, use_keras_mask=False, mask_value=0, from_logits=True)\n",
    "        model.compile(loss        = loss, \n",
    "                      optimizer   = optimizer(**optimizer_args), \n",
    "                      metrics     = [acc],\n",
    "                      jit_compile = jit_compile)\n",
    "    \n",
    "    ##  Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b30c604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: Creating new text-to-text model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: Model created with summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:Model created with summary:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: Model: \"decoder_model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:Model: \"decoder_model\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  Layer (type)                Output Shape                 Param #   Connected to                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: Layer (type)                Output Shape                 Param #   Connected to                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: ==================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_inpu  [(None, None)]               0         []                            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_inpu  [(None, None)]               0         []                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  t_layer (InputLayer)                                                                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: t_layer (InputLayer)                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_enum  (None, None)                 0         ['decoder_model_decoder_input_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_enum  (None, None)                 0         ['decoder_model_decoder_input_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  erate (Enumerate)                                                  layer[0][0]']                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: erate (Enumerate)                                                  layer[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_embe  (None, None, 64)             1024      ['decoder_model_decoder_input_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_embe  (None, None, 64)             1024      ['decoder_model_decoder_input_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  dding (Embedding)                                                  layer[0][0]']                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: dding (Embedding)                                                  layer[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_posi  (None, None, 64)             32        ['decoder_model_decoder_enumer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_posi  (None, None, 64)             32        ['decoder_model_decoder_enumer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  tion_encoding (PositionalE                                         ate[0][0]']                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: tion_encoding (PositionalE                                         ate[0][0]']                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  ncoding)                                                                                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: ncoding)                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_encoded_inpu  [(None, None)]               0         []                            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_encoded_inpu  [(None, None)]               0         []                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  t_layer (InputLayer)                                                                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: t_layer (InputLayer)                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_emb_  (None, None, 128)            0         ['decoder_model_decoder_embedd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_emb_  (None, None, 128)            0         ['decoder_model_decoder_embedd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  and_pos_loop1 (Concatenate                                         ing[0][0]',                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: and_pos_loop1 (Concatenate                                         ing[0][0]',                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  )                                                                   'decoder_model_decoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: )                                                                   'decoder_model_decoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_encoder_posi  (None, None, 192)            96        ['decoder_model_encoded_input_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_encoder_posi  (None, None, 192)            96        ['decoder_model_encoded_input_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  tion_encoding (PositionalE                                         layer[0][0]']                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: tion_encoding (PositionalE                                         layer[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  ncoding)                                                                                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: ncoding)                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_bloc  (None, None, 64)             915648    ['decoder_model_decoder_emb_an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_bloc  (None, None, 64)             915648    ['decoder_model_decoder_emb_an\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_1 (DecoderBlock)                                                 d_pos_loop1[0][0]',           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_1 (DecoderBlock)                                                 d_pos_loop1[0][0]',           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                      'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                     'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_2 (DecoderBlock)                                                 1[0][0]',                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_2 (DecoderBlock)                                                 1[0][0]',                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                      'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                     'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_3 (DecoderBlock)                                                 2[0][0]',                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_3 (DecoderBlock)                                                 2[0][0]',                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                      'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                     'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_4 (DecoderBlock)                                                 3[0][0]',                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_4 (DecoderBlock)                                                 3[0][0]',                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                      'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                     'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_5 (DecoderBlock)                                                 4[0][0]',                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_5 (DecoderBlock)                                                 4[0][0]',                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                      'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                     'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_decoder_bloc  (None, None, 64)             612160    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_6 (DecoderBlock)                                                 5[0][0]',                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_6 (DecoderBlock)                                                 5[0][0]',                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                      'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                     'decoder_model_encoder_positi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                     on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                    on_encoding[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  decoder_model_feedfwd_bloc  (None, None, 16)             569872    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: decoder_model_feedfwd_bloc  (None, None, 16)             569872    ['decoder_model_decoder_block_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  k_post_attention (FeedForw                                         6[0][0]']                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: k_post_attention (FeedForw                                         6[0][0]']                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:  ardBlock)                                                                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer: ardBlock)                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>:                                                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: ==================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: Total params: 4547472 (17.35 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:Total params: 4547472 (17.35 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: Trainable params: 4547472 (17.35 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:Trainable params: 4547472 (17.35 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: Non-trainable params: 0 (0.00 Byte)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:Non-trainable params: 0 (0.00 Byte)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO <module>: __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathsformer:__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##===================================================##\n",
    "##   Load or create self-supervised learning model   ##\n",
    "##===================================================##\n",
    "\n",
    "##  Get filename for load model\n",
    "fname = cfg_model.get(\"load_pretrained_model\", None)\n",
    "\n",
    "##  Load model if fname is not None, otherwise create from scratch\n",
    "if fname is not None :\n",
    "    logger.info   (f\"Loading model from: {fname}\")\n",
    "    logger.warning(\"Loading a pretrained model will disregard model config!\")\n",
    "    model = backend.load_text_to_text_model(fname)\n",
    "    model.optimizer.learning_rate.assign(cfg_model[\"learning_rate\"])  ## Reset LR to config value\n",
    "else :\n",
    "    logger.info(f\"Creating new text-to-text model\")\n",
    "    model = create_text_to_text_model(name=cfg_model[\"name\"], vocab_length=token_transform.vocab_length)\n",
    "\n",
    "##  Create hack to catch model summary\n",
    "model_summary = []\n",
    "model.summary(print_fn = lambda s : model_summary.append(s))\n",
    "\n",
    "##  Print model summary\n",
    "logger.info(\"Model created with summary:\")\n",
    "for s in model_summary : logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b49f300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tornado\n",
      "<Logger tornado (WARNING)> [<StreamHandler <stderr> (NOTSET)>]\n",
      "\n",
      "IPKernelApp\n",
      "<Logger IPKernelApp (DEBUG)> [<StreamHandler 57 (WARNING)>]\n",
      "\n",
      "tensorflow\n",
      "<Logger tensorflow (WARNING)> [<StreamHandler stdout (NOTSET)>]\n",
      "\n",
      "urllib3\n",
      "<Logger urllib3 (WARNING)> [<NullHandler (NOTSET)>]\n",
      "\n",
      "charset_normalizer\n",
      "<Logger charset_normalizer (WARNING)> [<NullHandler (NOTSET)>]\n",
      "\n",
      "requests\n",
      "<Logger requests (WARNING)> [<NullHandler (NOTSET)>]\n",
      "\n",
      "mathsformer\n",
      "<Logger mathsformer (DEBUG)> [<StreamHandler stdout (INFO)>, <FileHandler /Users/Ste/PROJECTS/misc/ML-sandbox/ML-sandbox/Project_Maths_Transformer/3_pretrained_decoder/baseline_decoder_int123467810_baseline_2023_06_14/log.txt (DEBUG)>]\n",
      "\n",
      "root\n",
      "<Logger mathsformer (DEBUG)> [<StreamHandler stdout (INFO)>, <FileHandler /Users/Ste/PROJECTS/misc/ML-sandbox/ML-sandbox/Project_Maths_Transformer/3_pretrained_decoder/baseline_decoder_int123467810_baseline_2023_06_14/log.txt (DEBUG)>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loggers = dict(logging.root.manager.loggerDict.items())\n",
    "\n",
    "loggers[\"root\"] = logger\n",
    "\n",
    "for name, _logger in loggers.items() :\n",
    "    if not hasattr(_logger, \"level\") : continue\n",
    "    if len(_logger.handlers) == 0 : continue\n",
    "    print(name)\n",
    "    print(_logger, _logger.handlers)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=================================##\n",
    "##   RandomNumberGenerator class   ##\n",
    "##=================================##\n",
    "##\n",
    "class RandomNumberGenerator(tf.keras.utils.Sequence) :\n",
    "    \n",
    "    def __init__(self, token_transform:data.TokenTransform, int_lengths:list, batch_size:int, num_batches:int, \n",
    "                 base_seed:int=-1, reproducible:bool=False, negative_char:str='-', dtype=tf.int32) :\n",
    "        \"\"\"\n",
    "        class RandomNumberGenerator\n",
    "        \n",
    "        Data generator used to create individual batches of input/output data on-the-fly for a keras model.\n",
    "        If reproducible=True then self[i] will always generate the same result, otherwise it will sample new\n",
    "        data every time it is called.\n",
    "        WARNING: data are not guaranteed to be unique - different batches may contain the same datapoints!\n",
    "        Inputs  are integers\n",
    "        Outputs are tokens representing the string\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  token_transform, TokenTransform\n",
    "               Method for transforming strings to/from tokenised tensors\n",
    "               \n",
    "            >  int_lengths, list\n",
    "               List of allowed integer-lengths (N.B. will be sampled uniformly, so 1-digit numbers occur with the\n",
    "               same frequency as N-digit numbers!)\n",
    "               \n",
    "            >  batch_size, int\n",
    "               Number of sequences per batch\n",
    "               \n",
    "            >  num_batches, int\n",
    "               Number of batches to constitute a full epoch\n",
    "               \n",
    "            >  base_seed, int, default=-1\n",
    "               Random seed used to initialise random number generator, if -1 then fall back to system time\n",
    "        \n",
    "            >  reproducible, bool, default=False\n",
    "               If True then re-initialise the rng seed to base_seed + i when calling self[i] for reproducible results\n",
    "               Otherwise do not re-initialise rng, allowing it to continue generating potentially new datapoints\n",
    "\n",
    "            >  negative_char, str, default='N'\n",
    "               Character used to represent a negative number\n",
    "               \n",
    "            >  dtype, dtype, default=tf.int32\n",
    "               Dtype for the integer tensor\n",
    "        \"\"\"\n",
    "        if base_seed < 0 :\n",
    "            base_seed = int(time.time())\n",
    "        \n",
    "        self.token_transform = token_transform\n",
    "        self.int_lengths     = int_lengths\n",
    "        self.batch_size      = batch_size\n",
    "        self.num_batches     = num_batches\n",
    "        self.base_seed       = base_seed\n",
    "        self.reproducible    = reproducible\n",
    "        self.negative_char   = negative_char\n",
    "        self.dtype           = dtype\n",
    "        self.reset_rng()\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index:int) :\n",
    "        \"\"\"\n",
    "        Returns a new set of tensors (X, Y) with length self.batch_size\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  index, int\n",
    "               Index of the call, only meaningful if we have self.reproducible = True\n",
    "        \"\"\"\n",
    "        if self.reproducible :\n",
    "            self.reset_rng(self.base_seed + index)\n",
    "        Z = [self._generate_x_y() for _ in range(self.batch_size)]\n",
    "        X = tf.constant([z[0] for z in Z], dtype=self.dtype)[:, tf.newaxis]\n",
    "        Y = self.token_transform.strings_to_tensor([z[1] for z in Z])\n",
    "        return [X, Y[:,:-1]], Y[:,1:]\n",
    "    \n",
    "    \n",
    "    def __len__(self) :\n",
    "        \"\"\"\n",
    "        Following generator convention: returns number of batches\n",
    "        \"\"\"\n",
    "        return self.num_batches\n",
    "    \n",
    "    \n",
    "    def __str__(self) :\n",
    "        \"\"\"\n",
    "        Returns a string summarising the generator configuration\n",
    "        \"\"\"\n",
    "        return f\"Generator of integers of length {self.int_lengths} in {self.num_batches} batches of size {self.batch_size} (base_seed={self.base_seed}, reproducible={self.reproducible})\"\n",
    "    \n",
    "    \n",
    "    def _generate_x_y(self) :\n",
    "        \"\"\"\n",
    "        Returns a pair of (int, string) = (X, Y) where X is a number and Y is its string repr\n",
    "        The number of digits in X is uniformly sampled from self.int_lengths\n",
    "        \n",
    "        \"\"\"\n",
    "        length      = self.rng.choice(self.int_lengths)\n",
    "        sign        = self.rng.choice([\"\", self.negative_char])\n",
    "        lead_char   = str(self.rng.randint(1, 10))\n",
    "        other_chars = \"\".join([str(self.rng.randint(0, 10)) for i in range(length-1)])\n",
    "        int_string  = sign + lead_char + other_chars\n",
    "        int_int     = int(int_string.replace(self.negative_char, \"-\"))\n",
    "        return int_int, int_string\n",
    "    \n",
    "    \n",
    "    def get_as_tensors(self, num_batches:int=-1) :\n",
    "        \"\"\"\n",
    "        Create a number of batches and combine their outputs into a single set of tensors\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  num_batches, int, default=-1\n",
    "               Number of batches to generate, if < 1 then fall back to self.num_batches\n",
    "        \"\"\"\n",
    "        ##  If num batches not set then return all of them\n",
    "        if num_batches < 1 :\n",
    "            num_batches = self.num_batches\n",
    "        \n",
    "        ##  Containers to stores batches\n",
    "        X, Y_in, Y_out = [], [] ,[]\n",
    "\n",
    "        ##  Fill containers with batch results\n",
    "        for i in range(num_batches) :\n",
    "            [x, yi], yo = self[i]\n",
    "            X, Y_in, Y_out = X + [x], Y_in + [yi], Y_out + [yo]\n",
    "\n",
    "        ##  Find max widths of tensors, which currently have ragged shapes\n",
    "        len_x, len_yi, len_yo = max([xp.shape[1] for xp in X]), max([xp.shape[1] for xp in Y_in]), max([xp.shape[1] for xp in Y_out])\n",
    "\n",
    "        ##  Pad all tensors to the same width\n",
    "        for i in range(len(X)) :\n",
    "            X    [i] = tf.pad(X    [i], [[0, 0], (0, len_x -X    [i].shape[1])])\n",
    "            Y_in [i] = tf.pad(Y_in [i], [[0, 0], (0, len_yi-Y_in [i].shape[1])])\n",
    "            Y_out[i] = tf.pad(Y_out[i], [[0, 0], (0, len_yo-Y_out[i].shape[1])])\n",
    "\n",
    "        ##  Concatenate batch results into single tensor\n",
    "        X, Y_in, Y_out = tf.concat(X, axis=0), tf.concat(Y_in, axis=0), tf.concat(Y_out, axis=0)\n",
    "        \n",
    "        ##  Return\n",
    "        return X, Y_in, Y_out\n",
    "    \n",
    "    \n",
    "    def reset_rng(self, seed:int=-1) :\n",
    "        \"\"\"\n",
    "        Set the internal rng with the seed provided\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  seed, int, default=-1\n",
    "               Random seed, if < 0 then fall back to self.base_seed\n",
    "        \"\"\"\n",
    "        if seed < 0 :\n",
    "            seed = self.base_seed\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        \n",
    "        \n",
    "    def summary(self, print_fn:Callable[[str],None]=None) :\n",
    "        \"\"\"\n",
    "        Print a summary of the generator\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  print_fn, callable with signature print_fn(str), default=print\n",
    "               Function used to print strings\n",
    "        \"\"\"\n",
    "        if print_fn is None :\n",
    "            print_fn = print\n",
    "        print_fn(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_generators(cfg_data, token_transform) :\n",
    "    \"\"\"\n",
    "    Create train/val/test data generators\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "        >  cfg, Config\n",
    "           Data configuration\n",
    "\n",
    "        >  token_transform, TokenTransform\n",
    "           Tokeniser object\n",
    "    \"\"\"\n",
    "    ##  Create training data generator\n",
    "    train_gen = RandomNumberGenerator(\n",
    "                                    token_transform = token_transform, \n",
    "                                    int_lengths     = cfg_data[\"train_data\"][\"int_lengths\"],\n",
    "                                    batch_size      = cfg_data[\"train_data\"][\"batch_size\"],\n",
    "                                    num_batches     = cfg_data[\"train_data\"][\"num_batches\"],\n",
    "                                    base_seed       = cfg_data[\"train_data\"][\"gen_base_seed\"],\n",
    "                                    reproducible    = cfg_data[\"train_data\"][\"gen_reproducible\"],\n",
    "                                    negative_char   = cfg_data[\"negative_char\"],\n",
    "                                    dtype           = cfg_data.get(\"dtype_int\", tf.int32),)\n",
    "    \n",
    "    ##  Create training data generator that has forced reproducible=True\n",
    "    train_gen_reproducible = RandomNumberGenerator(\n",
    "                                    token_transform = token_transform, \n",
    "                                    int_lengths     = cfg_data[\"train_data\"][\"int_lengths\"],\n",
    "                                    batch_size      = cfg_data[\"train_data\"][\"batch_size\"],\n",
    "                                    num_batches     = cfg_data[\"train_data\"][\"num_batches\"],\n",
    "                                    base_seed       = cfg_data[\"train_data\"][\"gen_base_seed\"],\n",
    "                                    reproducible    = True,\n",
    "                                    negative_char   = cfg_data[\"negative_char\"],\n",
    "                                    dtype           = cfg_data.get(\"dtype_int\", tf.int32),)\n",
    "    \n",
    "    ##  Log a sample training batch\n",
    "    logger.info(f\"Training data generator created with the following config: {train_gen}\")\n",
    "    (X, Y_in), Y_out = train_gen[0]\n",
    "    logger.info(f\"Output shapes for a test batch are ({X.shape}, {Y_in.shape}), {Y_out.shape}\")\n",
    "\n",
    "    ##  Create validation data generator\n",
    "    val_gen = RandomNumberGenerator(\n",
    "                                    token_transform = token_transform, \n",
    "                                    int_lengths     = cfg_data[\"val_data\"][\"int_lengths\"],\n",
    "                                    batch_size      = cfg_data[\"val_data\"][\"batch_size\"],\n",
    "                                    num_batches     = cfg_data[\"val_data\"][\"num_batches\"],\n",
    "                                    base_seed       = cfg_data[\"val_data\"][\"gen_base_seed\"],\n",
    "                                    reproducible    = cfg_data[\"val_data\"][\"gen_reproducible\"],\n",
    "                                    negative_char   = cfg_data[\"negative_char\"],\n",
    "                                    dtype           = cfg_data.get(\"dtype_int\", tf.int32),)\n",
    "    \n",
    "    ##  Log a sample validation batch\n",
    "    logger.info(f\"Validation data generator created with the following config: {val_gen}\")\n",
    "    (X, Y_in), Y_out = val_gen[0]\n",
    "    logger.info(f\"Output shapes for a test batch are ({X.shape}, {Y_in.shape}), {Y_out.shape}\")\n",
    "\n",
    "    ##  Create test data generator\n",
    "    test_gen = RandomNumberGenerator(\n",
    "                                    token_transform = token_transform, \n",
    "                                    int_lengths     = cfg_data[\"test_data\"][\"int_lengths\"],\n",
    "                                    batch_size      = cfg_data[\"test_data\"][\"batch_size\"],\n",
    "                                    num_batches     = cfg_data[\"test_data\"][\"num_batches\"],\n",
    "                                    base_seed       = cfg_data[\"test_data\"][\"gen_base_seed\"],\n",
    "                                    reproducible    = cfg_data[\"test_data\"][\"gen_reproducible\"],\n",
    "                                    negative_char   = cfg_data[\"negative_char\"],\n",
    "                                    dtype           = cfg_data.get(\"dtype_int\", tf.int32),)\n",
    "    \n",
    "    ##  Log a sample test batch\n",
    "    logger.info(f\"Test data generator created with the following config: {test_gen}\")\n",
    "    (X, Y_in), Y_out = test_gen[0]\n",
    "    logger.info(f\"Output shapes for a test batch are ({X.shape}, {Y_in.shape}), {Y_out.shape}\")\n",
    "    \n",
    "    ##  Return all three generators\n",
    "    return train_gen, train_gen_reproducible, val_gen, test_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==============================================================##\n",
    "##   Create transformer wrapper for model and token_transform   ##\n",
    "##==============================================================##\n",
    "\n",
    "transformer = transformers.Transformer_Text_to_Text(model, token_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e961ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##============================##\n",
    "##   Create data generators   ##\n",
    "##============================##\n",
    "\n",
    "train_gen, train_gen_reproducible, val_gen, test_gen = get_data_generators(cfg_data, token_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09034fa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##=========================================##\n",
    "##   Test transformer on data generators   ##\n",
    "##=========================================##\n",
    "\n",
    "negative_char = cfg_data.get(\"negative_char\")\n",
    "backend.test_transformer(transformer, train_gen, val_gen, test_gen, negative_char=negative_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652930b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================================##\n",
    "##   Create callbacks for training   ##\n",
    "##===================================##\n",
    "\n",
    "callbacks = backend.get_callbacks(cfg_training, working_dir, transformer=transformer, train_gen=train_gen_reproducible, \n",
    "                                  val_gen=val_gen, negative_char=negative_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=================##\n",
    "##   Train model   ##\n",
    "##=================##\n",
    "\n",
    "do_train = cfg_training.get(\"train\", True)\n",
    "\n",
    "if do_train :\n",
    "    max_epochs = cfg_training[\"max_epochs\"]\n",
    "    logger.info(f\"Begin model training with max_epochs={max_epochs}\")\n",
    "    model.fit(train_gen, \n",
    "              epochs          = max_epochs,\n",
    "              validation_data = val_gen,\n",
    "              callbacks       = callbacks\n",
    "             )\n",
    "else :\n",
    "    logger.warning(\"Skipping model training following global config instructions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6014ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X, Y_in], Y_out = train_gen_reproducible[0]\n",
    "\n",
    "model.predict([X, Y_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe01eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
