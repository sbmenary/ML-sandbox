{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884dc59d",
   "metadata": {},
   "source": [
    "#  Unsupervised learning addition model\n",
    "\n",
    "Author: S. Menary [sbmenary@gmail.com]\n",
    "\n",
    "Date: 5/4/2023  (last update: 5/4/2023)\n",
    "\n",
    "Overview: Train a `sequence -> sequence` model where the input sequence is a text representation of a simple sum $A \\pm B$ for integers $A,B\\in\\mathbb{Z}\\left([-N,~N]\\right)$ for some maximum-amplitude $N$, and the output is a text representation of the answer. Since the output is a numerical value, the loss function is a sparse categorical entropy\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Set up program\n",
    "\n",
    "###  Import\n",
    "\n",
    "All imports go here at the top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48f1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=========================##\n",
    "##   All imports go here   ##\n",
    "##=========================##\n",
    "\n",
    "##  Import entire python stdlib packages\n",
    "import datetime, json, logging, math, os, pickle, random, sys, time\n",
    "\n",
    "##  Import entire pypi packages\n",
    "import matplotlib as mpl\n",
    "import numpy      as np\n",
    "import tensorflow as tf\n",
    "\n",
    "##  Import individual modules/objects from python stdlib packages\n",
    "from pathlib import Path\n",
    "\n",
    "##  Import individual modules/objects from pypi packages\n",
    "from tensorflow.keras.callbacks  import Callback, CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers     import Average, Concatenate, Dense, Embedding, Input, Layer, Masking\n",
    "from tensorflow.keras.losses     import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models     import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib                  import pyplot as plt\n",
    "\n",
    "##  Add directory above this to system path to expose mathsformer package location\n",
    "sys.path.append(\"/\".join(os.getcwd().split(\"/\")[:-1]))\n",
    "\n",
    "##  Import individual modules/objects from local packages\n",
    "from mathsformer.data       import add_positions_to_sequences, detokenise_string, detokenise_strings, strings_to_tensor, tokenise_strings\n",
    "from mathsformer.tf_objects import (create_custom_objects_dict, get_nested_sublayers, masked_accuracy, \n",
    "                                    masked_sparse_categorical_crossentropy, AttentionBlock, DecoderBlock, \n",
    "                                    EncoderBlock, Enumerate, FeedForwardBlock, LayerActivationRecord, LayerWeightsRecord, \n",
    "                                    LoggerCallback, LossRecord, PositionalEncoding, ReduceSequence, RightSlice)\n",
    "from mathsformer.utils      import create_working_directory, fancy_message, initialise_program, log_versions, summarise_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320886f9",
   "metadata": {},
   "source": [
    "### Configuation\n",
    "\n",
    "Set global configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042ea7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "===   Created global_config   ===\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "##===================##\n",
    "##   Global config   ##\n",
    "##===================##\n",
    "\n",
    "\n",
    "##  Set log levels for logging streams to notebook and to log file in working directory\n",
    "log_lvl_iostream = logging.INFO\n",
    "log_lvl_fstream  = logging.DEBUG\n",
    "\n",
    "\n",
    "##  Create dictionary of config values\n",
    "##  -  config values to be set here and never changed!\n",
    "##  -  use nested dictionary as a proxy for namespacing\n",
    "##  -  e.g. can access data config value like config[\"data\"][\"label_prescale\"]\n",
    "global_config = {\n",
    "    \"base_seed\"         : -1,\n",
    "    \"working_directory\" : \"unsupervised_learning_addition_model_[tag]_[date]_[time]\",\n",
    "    \"tag\"               : \"LARGE\",\n",
    "    \"data\" : {\n",
    "        \"max_int\"        : 999,\n",
    "        \"char_tokens\"    : ['M', 'B', 'E', 'N', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-'],\n",
    "        \"mask_char\"      : 'M',\n",
    "        \"seq_start_char\" : 'B',\n",
    "        \"seq_end_char\"   : 'E',\n",
    "        \"negative_char\"  : 'N',\n",
    "        \"test_split_idx\" : 0.05,\n",
    "        \"val_split_idx\"  : 0.15,\n",
    "        \"dtype\"          : \"int32\",\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"load_pretrained_model\" : \"\",\n",
    "        \"ndim_model\"            : 64,\n",
    "        \"ndim_embedding\"        : 32,\n",
    "        \"num_freqs\"             : 16,\n",
    "        \"min_period\"            : 4,\n",
    "        \"max_period\"            : 200,\n",
    "        \"num_pre_layers_encoder\": 3,\n",
    "        \"ndim_pre_layers\"       : 512,\n",
    "        \"num_encoder_layers\"    : 8,\n",
    "        \"num_heads\"             : 12,\n",
    "        \"ndim_att_hidden\"       : 128,\n",
    "        \"ndim_ff_hidden\"        : 512,\n",
    "        \"num_pre_layers_decoder\": 3,\n",
    "        \"num_decoder_layers\"    : 8,\n",
    "        \"num_post_layers\"       : 4,\n",
    "        \"ndim_post_layers\"      : 1024,\n",
    "        \"dropout\"               : 0.1,\n",
    "        \"learning_rate\"         : 1e-4,\n",
    "        \"dtype\"                 : \"float32\"\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"train\"                   : True,\n",
    "        \"max_epochs\"              : 100,\n",
    "        \"batch_size\"              : 32,\n",
    "        \"early_stopping\"          : True,\n",
    "        \"early_stopping_patience\" : 3,\n",
    "        \"layer_activation_record_batch_frequency\" : 2000, \n",
    "        \"layer_activation_record_max_datapoints\"  : 32, \n",
    "        \"layer_weights_record_batch_frequency\" : 2000, \n",
    "        \"layer_weights_record_recursive\"       : True, \n",
    "        \"loss_record_batch_frequency\" : 2000, \n",
    "        \"loss_record_max_datapoints\"  : 256, \n",
    "        \"loss_record_num_bootstrap\"   : 20, \n",
    "    },\n",
    "    \"evaluate\" : {\n",
    "        \"num_bootstrap\" : 100,\n",
    "        \"num_print\"     : 40,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "##  Report success\n",
    "print(fancy_message(f\"Created global_config\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0be50",
   "metadata": {},
   "source": [
    "###  Validate config\n",
    "\n",
    "Look for some obvious confguration errors. WARNING: This is not an exhaustive search and can't be replied upon to catch all misconfigurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0dad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "===   Config successfully validated   ===\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "##===============================##\n",
    "##   Global config - continued   ##\n",
    "##===============================##\n",
    "\n",
    "def validate_config(config) :\n",
    "    \"\"\"Raise exceptions in the case of program misconfigurations\"\"\"\n",
    "    mask_char      = global_config[\"data\"][\"mask_char\"]\n",
    "    seq_start_char = global_config[\"data\"][\"seq_start_char\"]\n",
    "    seq_end_char   = global_config[\"data\"][\"seq_end_char\"]\n",
    "    negative_char  = global_config[\"data\"][\"negative_char\"]\n",
    "    char_tokens    = global_config[\"data\"][\"char_tokens\"]\n",
    "    \n",
    "    ##  Check that only single character tokens are provided\n",
    "    for char_token in char_tokens :\n",
    "        if len(char_token) == 1 : continue\n",
    "        raise ValueError(f\"All character tokens must be single characters but '{char_tokens}' found\")\n",
    "        \n",
    "    ##  Check mask character is provided\n",
    "    if len(mask_char) != 1 :\n",
    "        raise ValueError(f\"Mask character must be a single character but '{mask_char}' provided\")\n",
    "        \n",
    "    ##  Check mask character in character list\n",
    "    if mask_char not in char_tokens :\n",
    "        raise ValueError(f\"Mask character '{mask_char}' not found in character list: {char_tokens}\")\n",
    "    \n",
    "    ##  Check that mask character is first in char_tokens list (ensures it's assigned a token of 0)\n",
    "    if char_tokens[0] != mask_char :\n",
    "        raise ValueError(f\"Mask character '{mask_char}' must be the first in the char_tokens list provided, \"\n",
    "                        +f\"instead found list: {char_tokens}\")\n",
    "        \n",
    "    ##  Check seq_start_char character is provided\n",
    "    if len(seq_start_char) != 1 :\n",
    "        raise ValueError(f\"Sequence start character must be a single character but '{seq_start_char}' provided\")\n",
    "        \n",
    "    ##  Check seq_start_char character in character list\n",
    "    if seq_start_char not in char_tokens :\n",
    "        raise ValueError(f\"Sequence start character '{seq_start_char}' not found in character list: {char_tokens}\")\n",
    "        \n",
    "    ##  Check seq_end_char character is provided\n",
    "    if len(seq_end_char) != 1 :\n",
    "        raise ValueError(f\"Sequence end character must be a single character but '{seq_end_char}' provided\")\n",
    "        \n",
    "    ##  Check seq_start_char character in character list\n",
    "    if seq_end_char not in char_tokens :\n",
    "        raise ValueError(f\"Sequence end character '{seq_end_char}' not found in character list: {char_tokens}\")\n",
    "        \n",
    "    ##  Check negative_char character is provided\n",
    "    if len(negative_char) != 1 :\n",
    "        raise ValueError(f\"Negative symbol character must be a single character but '{negative_char}' provided\")\n",
    "        \n",
    "    ##  Check negative_char character in character list\n",
    "    if negative_char not in char_tokens :\n",
    "        raise ValueError(f\"Negative symbol character '{negative_char}' not found in character list: {char_tokens}\")\n",
    "        \n",
    "    ##  If here then config validated correctly\n",
    "    print(fancy_message(\"Config successfully validated\"))\n",
    "    \n",
    "validate_config(global_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af05439",
   "metadata": {},
   "source": [
    "##  2. Set up environment\n",
    "\n",
    "- Create working directory\n",
    "- Create logger\n",
    "- Log package versions for reproducibility\n",
    "- Log config values for reproducibility\n",
    "- Set random seeds for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7cc532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================\n",
      "===   Working directory created at unsupervised_learning_addition_model_LARGE_2023_04_06_211603   ===\n",
      "=====================================================================================================\n",
      "   INFO  Begin logging on 2023-04-06 at 21:16:03\n",
      "   INFO  Program description: unsupervised_learning_model notebook\n",
      "   INFO  Working directory: unsupervised_learning_addition_model_LARGE_2023_04_06_211603\n",
      "   INFO  ------------------------------------------------------+-----------------------------------------------------------------------------------\n",
      "   INFO                                               PACKAGE  |  VERSION\n",
      "   INFO  ------------------------------------------------------+-----------------------------------------------------------------------------------\n",
      "   INFO                                                Python  |  3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:12:31) [Clang 14.0.6 ]\n",
      "   INFO                                               IPython  |  8.12.0\n",
      "   INFO                                  IPython.core.release  |  8.12.0\n",
      "   INFO                                                   PIL  |  9.5.0\n",
      "   INFO                                             PIL.Image  |  9.5.0\n",
      "   INFO                                        PIL._deprecate  |  9.5.0\n",
      "   INFO                                          PIL._version  |  9.5.0\n",
      "   INFO                                         _cffi_backend  |  1.15.1\n",
      "   INFO                                                  _csv  |  1.0\n",
      "   INFO                                               _ctypes  |  1.1.0\n",
      "   INFO                                               _curses  |  b'2.2'\n",
      "   INFO                                               decimal  |  1.70\n",
      "   INFO                                _pydev_bundle.fsnotify  |  0.1.5\n",
      "   INFO                  _pydevd_frame_eval.vendored.bytecode  |  0.13.0.dev\n",
      "   INFO                                               appnope  |  0.1.3\n",
      "   INFO                                              argparse  |  1.1\n",
      "   INFO                                            astunparse  |  1.6.3\n",
      "   INFO                                              backcall  |  0.2.0\n",
      "   INFO                                               certifi  |  2022.12.07\n",
      "   INFO                                                  cffi  |  1.15.1\n",
      "   INFO                                    charset_normalizer  |  2.0.4\n",
      "   INFO                            charset_normalizer.version  |  2.0.4\n",
      "   INFO                                                  comm  |  0.1.3\n",
      "   INFO                                                   csv  |  1.0\n",
      "   INFO                                                ctypes  |  1.1.0\n",
      "   INFO                                       ctypes.macholib  |  1.0\n",
      "   INFO                                                cycler  |  0.10.0\n",
      "   INFO                                              dateutil  |  2.8.2\n",
      "   INFO                                               debugpy  |  1.6.7\n",
      "   INFO                                    debugpy.public_api  |  1.6.7\n",
      "   INFO                                               decimal  |  1.70\n",
      "   INFO                                             decorator  |  5.1.1\n",
      "   INFO                                            defusedxml  |  0.7.1\n",
      "   INFO                                             distutils  |  3.10.10\n",
      "   INFO                                             executing  |  1.2.0\n",
      "   INFO                                     executing.version  |  1.2.0\n",
      "   INFO                                           flatbuffers  |  23.3.3\n",
      "   INFO                                  flatbuffers._version  |  23.3.3\n",
      "   INFO                                       google.protobuf  |  4.22.1\n",
      "   INFO                                                  h5py  |  3.6.0\n",
      "   INFO                                           http.server  |  0.6\n",
      "   INFO                                                  idna  |  3.4\n",
      "   INFO                                         idna.idnadata  |  15.0.0\n",
      "   INFO                                     idna.package_data  |  3.4\n",
      "   INFO                                             ipaddress  |  1.0\n",
      "   INFO                                             ipykernel  |  6.22.0\n",
      "   INFO                                    ipykernel._version  |  6.22.0\n",
      "   INFO                                                  jedi  |  0.18.2\n",
      "   INFO                                                  json  |  2.0.9\n",
      "   INFO                                        jupyter_client  |  8.1.0\n",
      "   INFO                               jupyter_client._version  |  8.1.0\n",
      "   INFO                                          jupyter_core  |  5.3.0\n",
      "   INFO                                  jupyter_core.version  |  5.3.0\n",
      "   INFO                                                 keras  |  2.12.0\n",
      "   INFO                                   keras.api._v2.keras  |  2.12.0\n",
      "   INFO                                       keras.api.keras  |  2.12.0\n",
      "   INFO                                            kiwisolver  |  1.4.4\n",
      "   INFO                                      kiwisolver._cext  |  1.4.4\n",
      "   INFO                                               logging  |  0.5.1.2\n",
      "   INFO                                            matplotlib  |  3.7.1\n",
      "   INFO                                   matplotlib._version  |  3.7.1\n",
      "   INFO                                                 numpy  |  1.23.2\n",
      "   INFO                                            numpy.core  |  1.23.2\n",
      "   INFO                          numpy.core._multiarray_umath  |  3.1\n",
      "   INFO                                             numpy.lib  |  1.23.2\n",
      "   INFO                            numpy.linalg._umath_linalg  |  0.1.5\n",
      "   INFO                                         numpy.version  |  1.23.2\n",
      "   INFO                                            opt_einsum  |  v3.3.0\n",
      "   INFO                                             packaging  |  23.0\n",
      "   INFO                                                 parso  |  0.8.3\n",
      "   INFO                                               pexpect  |  4.8.0\n",
      "   INFO                                           pickleshare  |  0.7.5\n",
      "   INFO                         pkg_resources._vendor.appdirs  |  1.4.3\n",
      "   INFO                  pkg_resources._vendor.more_itertools  |  8.12.0\n",
      "   INFO                       pkg_resources._vendor.packaging  |  21.3\n",
      "   INFO             pkg_resources._vendor.packaging.__about__  |  21.3\n",
      "   INFO                       pkg_resources._vendor.pyparsing  |  3.0.9\n",
      "   INFO                         pkg_resources._vendor.appdirs  |  1.4.3\n",
      "   INFO                  pkg_resources._vendor.more_itertools  |  8.12.0\n",
      "   INFO                       pkg_resources._vendor.packaging  |  21.3\n",
      "   INFO                       pkg_resources._vendor.pyparsing  |  3.0.9\n",
      "   INFO                                              platform  |  1.0.8\n",
      "   INFO                                          platformdirs  |  3.2.0\n",
      "   INFO                                  platformdirs.version  |  3.2.0\n",
      "   INFO                                        prompt_toolkit  |  3.0.38\n",
      "   INFO                                                psutil  |  5.9.4\n",
      "   INFO                                            ptyprocess  |  0.7.0\n",
      "   INFO                                             pure_eval  |  0.2.2\n",
      "   INFO                                     pure_eval.version  |  0.2.2\n",
      "   INFO                                                pydevd  |  2.9.5\n",
      "   INFO                                              pygments  |  2.14.0\n",
      "   INFO                                             pyparsing  |  3.0.9\n",
      "   INFO                                                    re  |  2.2.1\n",
      "   INFO                                              requests  |  2.28.1\n",
      "   INFO                                  requests.__version__  |  2.28.1\n",
      "   INFO                                                  idna  |  3.4\n",
      "   INFO                                         idna.idnadata  |  15.0.0\n",
      "   INFO                                     idna.package_data  |  3.4\n",
      "   INFO                                               urllib3  |  1.26.14\n",
      "   INFO                                      urllib3._version  |  1.26.14\n",
      "   INFO                                    urllib3.connection  |  1.26.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO                                  urllib3.packages.six  |  1.16.0\n",
      "   INFO                       urllib3.util.ssl_match_hostname  |  3.5.0.1\n",
      "   INFO                                        requests.utils  |  2.28.1\n",
      "   INFO                                                 scipy  |  1.10.1\n",
      "   INFO                                  scipy._lib.decorator  |  4.0.5\n",
      "   INFO                                   scipy.linalg._fblas  |  1.21.6\n",
      "   INFO                                 scipy.linalg._flapack  |  1.21.6\n",
      "   INFO                                 scipy.linalg._flinalg  |  1.21.6\n",
      "   INFO             scipy.sparse.linalg._eigen.arpack._arpack  |  1.21.6\n",
      "   INFO                scipy.sparse.linalg._isolve._iterative  |  1.21.6\n",
      "   INFO                                scipy.special._specfun  |  1.21.6\n",
      "   INFO                                            setuptools  |  65.6.3\n",
      "   INFO                                             distutils  |  3.10.10\n",
      "   INFO                     setuptools._vendor.more_itertools  |  8.8.0\n",
      "   INFO                        setuptools._vendor.ordered_set  |  3.1\n",
      "   INFO                          setuptools._vendor.packaging  |  21.3\n",
      "   INFO                setuptools._vendor.packaging.__about__  |  21.3\n",
      "   INFO                          setuptools._vendor.pyparsing  |  3.0.9\n",
      "   INFO                     setuptools._vendor.more_itertools  |  8.8.0\n",
      "   INFO                        setuptools._vendor.ordered_set  |  3.1\n",
      "   INFO                          setuptools._vendor.packaging  |  21.3\n",
      "   INFO                          setuptools._vendor.pyparsing  |  3.0.9\n",
      "   INFO                                    setuptools.version  |  65.6.3\n",
      "   INFO                                                   six  |  1.16.0\n",
      "   INFO                                          socketserver  |  0.4\n",
      "   INFO                                                 socks  |  1.7.1\n",
      "   INFO                                            stack_data  |  0.6.2\n",
      "   INFO                                    stack_data.version  |  0.6.2\n",
      "   INFO                                           tensorboard  |  2.12.1\n",
      "   INFO                    tensorboard.compat.tensorflow_stub  |  stub\n",
      "   INFO  tensorboard.compat.tensorflow_stub.pywrap_tensorflow  |  0\n",
      "   INFO                                            tensorflow  |  2.12.0\n",
      "   INFO                          tensorflow._api.v2.compat.v1  |  2.12.0\n",
      "   INFO                tensorflow._api.v2.compat.v1.compat.v1  |  2.12.0\n",
      "   INFO                tensorflow._api.v2.compat.v1.compat.v2  |  2.12.0\n",
      "   INFO                          tensorflow._api.v2.compat.v2  |  2.12.0\n",
      "   INFO                tensorflow._api.v2.compat.v2.compat.v1  |  2.12.0\n",
      "   INFO                tensorflow._api.v2.compat.v2.compat.v2  |  2.12.0\n",
      "   INFO                                  tensorflow.compat.v1  |  2.12.0\n",
      "   INFO                        tensorflow.compat.v1.compat.v1  |  2.12.0\n",
      "   INFO                        tensorflow.compat.v1.compat.v2  |  2.12.0\n",
      "   INFO                                  tensorflow.compat.v2  |  2.12.0\n",
      "   INFO                        tensorflow.compat.v2.compat.v1  |  2.12.0\n",
      "   INFO                        tensorflow.compat.v2.compat.v2  |  2.12.0\n",
      "   INFO                                      tensorflow.keras  |  2.12.0\n",
      "   INFO            tensorflow.python.client.pywrap_tf_session  |  2.12.0\n",
      "   INFO                  tensorflow.python.framework.versions  |  2.12.0\n",
      "   INFO                               tensorflow.python.keras  |  2.6.0\n",
      "   INFO                                             traitlets  |  5.9.0\n",
      "   INFO                                    traitlets._version  |  5.9.0\n",
      "   INFO                                        urllib.request  |  3.10\n",
      "   INFO                                               urllib3  |  1.26.14\n",
      "   INFO                                      urllib3._version  |  1.26.14\n",
      "   INFO                                    urllib3.connection  |  1.26.14\n",
      "   INFO                                  urllib3.packages.six  |  1.16.0\n",
      "   INFO                       urllib3.util.ssl_match_hostname  |  3.5.0.1\n",
      "   INFO                                               wcwidth  |  0.2.6\n",
      "   INFO                                                 wrapt  |  1.14.1\n",
      "   INFO                                         xmlrpc.client  |  3.10\n",
      "   INFO                                                  zlib  |  1.0\n",
      "   INFO                                                   zmq  |  25.0.2\n",
      "   INFO                                             zmq.sugar  |  25.0.2\n",
      "   INFO                                     zmq.sugar.version  |  25.0.2\n",
      "   INFO  ------------------------------------------------------+-----------------------------------------------------------------------------------\n",
      "   INFO  Registered global config value base_seed: -1\n",
      "   INFO  Registered global config value working_directory: unsupervised_learning_addition_model_[tag]_[date]_[time]\n",
      "   INFO  Registered global config value tag: LARGE\n",
      "   INFO  Registered global config value data > max_int: 999\n",
      "   INFO  Registered global config value data > char_tokens: ['M', 'B', 'E', 'N', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-']\n",
      "   INFO  Registered global config value data > mask_char: M\n",
      "   INFO  Registered global config value data > seq_start_char: B\n",
      "   INFO  Registered global config value data > seq_end_char: E\n",
      "   INFO  Registered global config value data > negative_char: N\n",
      "   INFO  Registered global config value data > test_split_idx: 0.05\n",
      "   INFO  Registered global config value data > val_split_idx: 0.15\n",
      "   INFO  Registered global config value data > dtype: int32\n",
      "   INFO  Registered global config value model > load_pretrained_model: \n",
      "   INFO  Registered global config value model > ndim_model: 64\n",
      "   INFO  Registered global config value model > ndim_embedding: 32\n",
      "   INFO  Registered global config value model > num_freqs: 16\n",
      "   INFO  Registered global config value model > min_period: 4\n",
      "   INFO  Registered global config value model > max_period: 200\n",
      "   INFO  Registered global config value model > num_pre_layers_encoder: 3\n",
      "   INFO  Registered global config value model > ndim_pre_layers: 512\n",
      "   INFO  Registered global config value model > num_encoder_layers: 8\n",
      "   INFO  Registered global config value model > num_heads: 12\n",
      "   INFO  Registered global config value model > ndim_att_hidden: 128\n",
      "   INFO  Registered global config value model > ndim_ff_hidden: 512\n",
      "   INFO  Registered global config value model > num_pre_layers_decoder: 3\n",
      "   INFO  Registered global config value model > num_decoder_layers: 8\n",
      "   INFO  Registered global config value model > num_post_layers: 4\n",
      "   INFO  Registered global config value model > ndim_post_layers: 1024\n",
      "   INFO  Registered global config value model > dropout: 0.1\n",
      "   INFO  Registered global config value model > learning_rate: 0.0001\n",
      "   INFO  Registered global config value model > dtype: float32\n",
      "   INFO  Registered global config value training > train: True\n",
      "   INFO  Registered global config value training > max_epochs: 100\n",
      "   INFO  Registered global config value training > batch_size: 32\n",
      "   INFO  Registered global config value training > early_stopping: True\n",
      "   INFO  Registered global config value training > early_stopping_patience: 3\n",
      "   INFO  Registered global config value training > layer_activation_record_batch_frequency: 2000\n",
      "   INFO  Registered global config value training > layer_activation_record_max_datapoints: 32\n",
      "   INFO  Registered global config value training > layer_weights_record_batch_frequency: 2000\n",
      "   INFO  Registered global config value training > layer_weights_record_recursive: True\n",
      "   INFO  Registered global config value training > loss_record_batch_frequency: 2000\n",
      "   INFO  Registered global config value training > loss_record_max_datapoints: 256\n",
      "   INFO  Registered global config value training > loss_record_num_bootstrap: 20\n",
      "   INFO  Registered global config value evaluate > num_bootstrap: 100\n",
      "   INFO  Registered global config value evaluate > num_print: 40\n",
      "   INFO  Python random seed set: 1680812163\n",
      "   INFO  Numpy random seed set: 1680812164\n",
      "   INFO  TensorFlow random seed set: 1680812165\n"
     ]
    }
   ],
   "source": [
    "##==============================##\n",
    "##   Create working directory   ##\n",
    "##==============================##\n",
    "\n",
    "##  Report success\n",
    "working_dir = global_config[\"working_directory\"]\n",
    "working_dir, logger, base_seed, np_seed, tf_seed = initialise_program(\n",
    "    \"unsupervised_learning_model notebook\", \n",
    "    working_dir, \n",
    "    global_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960860d",
   "metadata": {},
   "source": [
    "##  3. Create training data\n",
    "\n",
    "###  Generate string-string pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c310bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=====================##\n",
    "##   Create raw data   ##\n",
    "##=====================##\n",
    "\n",
    "def generate_raw_dataset(max_int:int=999, include_neg:bool=True, shuffle:bool=True, negative_char:str='N') :\n",
    "    '''\n",
    "    Create dataset where input is str of \"A+B\" or \"A-B\", and output is str containing the answer\n",
    "    A and B are positive or negative integers with maximum amplitude given\n",
    "    '''\n",
    "    logger.info(f\"generate_raw_dataset(): called with max_int={max_int:,}, include_neg={include_neg}, shuffle={shuffle}\")\n",
    "    if include_neg : singles = np.arange(-max_int, max_int+1, dtype=np.int32) \n",
    "    else           : singles = np.arange(0       , max_int+1, dtype=np.int32)\n",
    "    pairs   = np.array([[(x,y) for x in singles] for y in singles])\n",
    "    pairs   = np.concatenate(pairs)\n",
    "    summed  = pairs[:,0] + pairs[:,1]\n",
    "    minus   = pairs[:,0] - pairs[:,1]\n",
    "    dataset = []\n",
    "    for (i1, i2), s, m in zip(pairs, summed, minus) :\n",
    "        i1, i2 = f\"{i1}\".replace(\"-\",negative_char), f\"{i2}\".replace(\"-\",negative_char)\n",
    "        s , m  = f\"{s }\".replace(\"-\",negative_char), f\"{m }\".replace(\"-\",negative_char)\n",
    "        dataset.append((f\"{i1}+{i2}\", f\"{s}\"))\n",
    "        dataset.append((f\"{i1}-{i2}\", f\"{m}\"))\n",
    "    np.random.shuffle(dataset)\n",
    "    logger.info(f\"generate_raw_dataset(): created dataset with length {len(dataset):,}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06740a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO  generate_raw_dataset(): called with max_int=999, include_neg=True, shuffle=True\n",
      "   INFO  generate_raw_dataset(): created dataset with length 7,992,002\n",
      "   INFO  Created 7,992,002 datapoints in 9.3s\n",
      "   INFO  Maximum input sequence length is 9\n",
      "   INFO  Maximum output sequence length is 5\n"
     ]
    }
   ],
   "source": [
    "##=================================##\n",
    "##   Create raw data - continued   ##\n",
    "##=================================##\n",
    "\n",
    "start_time   = time.time()\n",
    "max_int      = global_config[\"data\"][\"max_int\"]\n",
    "raw_dataset  = generate_raw_dataset(max_int)\n",
    "max_X_length = max([len(s[0]) for s in raw_dataset])\n",
    "max_Y_length = max([len(s[1]) for s in raw_dataset])\n",
    "\n",
    "logger.info(f\"Created {len(raw_dataset):,} datapoints in {time.time()-start_time:.1f}s\")\n",
    "logger.info(f\"Maximum input sequence length is {max_X_length}\")\n",
    "logger.info(f\"Maximum output sequence length is {max_Y_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af8304",
   "metadata": {},
   "source": [
    "###   Process strings into fixed-length tokenised dataset with position indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c6be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO  Created tokens dictionaries with vocab_length=16\n"
     ]
    }
   ],
   "source": [
    "##=============================================##\n",
    "##   Convert raw data to tokenised sequences   ##\n",
    "##=============================================##\n",
    "\n",
    "mask_char        = global_config[\"data\"][\"mask_char\"]\n",
    "char_tokens      = global_config[\"data\"][\"char_tokens\"]\n",
    "vocab_length     = len(char_tokens)\n",
    "tokeniser_dict   = dict([(t,i) for i,t in enumerate(char_tokens)])\n",
    "detokeniser_dict = dict([(i,t) for i,t in enumerate(char_tokens)])\n",
    "mask_token       = tokeniser_dict[mask_char]\n",
    "\n",
    "logger.info (f\"Created tokens dictionaries with vocab_length={vocab_length}\")\n",
    "logger.debug(f\"Tokeniser dictionary is {tokeniser_dict}\")\n",
    "logger.debug(f\"Detokeniser dictionary is {detokeniser_dict}\")\n",
    "\n",
    "if mask_token != 0 :\n",
    "    raise RuntimeError(f\"Mask character {mask_char} with a token value {mask_token}, expected 0\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ef5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INFO  Using mask character 'M'\n",
      "   INFO  Using sequence start character 'B'\n",
      "   INFO  Using sequence end character 'E'\n",
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "##=========================================================##\n",
    "##   Convert raw data to tokenised sequences - continued   ##\n",
    "##=========================================================##\n",
    "\n",
    "dtype_data     = global_config[\"data\"][\"dtype\"]\n",
    "seq_start_char = global_config[\"data\"][\"seq_start_char\"]\n",
    "seq_end_char   = global_config[\"data\"][\"seq_end_char\"]\n",
    "\n",
    "logger.info(f\"Using mask character '{mask_char}'\")\n",
    "logger.info(f\"Using sequence start character '{seq_start_char}'\")\n",
    "logger.info(f\"Using sequence end character '{seq_end_char}'\")\n",
    "\n",
    "start_time = time.time()\n",
    "data_X     = strings_to_tensor([x[0] for x in raw_dataset], \n",
    "                               tokeniser_dict, fix_output_length=max_X_length+len(seq_start_char)+len(seq_end_char), \n",
    "                               mask_char=mask_char, seq_start_char=seq_start_char, seq_end_char=seq_end_char, \n",
    "                               logger=logger, add_position_indices=False, dtype=dtype_data)\n",
    "data_Y     = strings_to_tensor([x[1] for x in raw_dataset], \n",
    "                               tokeniser_dict, fix_output_length=max_Y_length+len(seq_start_char)+len(seq_end_char), \n",
    "                               mask_char=mask_char, seq_start_char=seq_start_char, seq_end_char=seq_end_char, \n",
    "                               logger=logger, add_position_indices=False, dtype=dtype_data)\n",
    "data_Y_in, data_Y_out = data_Y[:,:-1], data_Y[:,1:]\n",
    "del data_Y\n",
    "logger.info(f\"Data tensors created in {time.time()-start_time:.0f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e55423",
   "metadata": {},
   "source": [
    "###  Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80066d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==================================##\n",
    "##   Perform train/val/test split   ##\n",
    "##==================================##\n",
    "\n",
    "split_idx1 = global_config.get(\"data\", {}).get(\"test_split_idx\", 0.1)\n",
    "split_idx2 = global_config.get(\"data\", {}).get(\"val_split_idx\" , 0.3)\n",
    "\n",
    "if split_idx1 < 1 : split_idx1 = int(split_idx1*len(data_X))\n",
    "if split_idx2 < 1 : split_idx2 = int(split_idx2*len(data_X))\n",
    "    \n",
    "test_X , test_Y_in , test_Y_out  = data_X[          :split_idx1], data_Y_in[          :split_idx1], data_Y_out[          :split_idx1]\n",
    "val_X  , val_Y_in  , val_Y_out   = data_X[split_idx1:split_idx2], data_Y_in[split_idx1:split_idx2], data_Y_out[split_idx1:split_idx2]\n",
    "train_X, train_Y_in, train_Y_out = data_X[split_idx2:          ], data_Y_in[split_idx2:          ], data_Y_out[split_idx2:          ]\n",
    "\n",
    "del data_X, data_Y_in, data_Y_out\n",
    "logger.info(f\"  Training data with shape {train_X.shape}, {train_Y_in.shape} and labels {train_Y_out.shape}\")\n",
    "logger.info(f\"Validation data with shape {  val_X.shape}, {  val_Y_in.shape} and labels {  val_Y_out.shape}\")\n",
    "logger.info(f\"      Test data with shape { test_X.shape}, { test_Y_in.shape} and labels { test_Y_out.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39db1bb",
   "metadata": {},
   "source": [
    "##  4.  Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1889c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##======================================##\n",
    "##   Create supervised learning model   ##\n",
    "##======================================##\n",
    "\n",
    "def create_model(vocab_length:int, \n",
    "                 name=None, dtype=tf.float32, \n",
    "                 pos_enc_slice_index:int=None,\n",
    "                 ndim_model:int=32, ndim_embedding:int=16, num_freqs:int=8, \n",
    "                 min_period:float=5, max_period:float=200, num_pre_layers_encoder:int=1, ndim_pre_layers:int=64, \n",
    "                 num_encoder_layers:int=3, num_heads:int=6, ndim_att_hidden:int=32, ndim_ff_hidden:int=64, \n",
    "                 num_pre_layers_decoder:int=1, num_decoder_layers:int=3, num_post_layers:int=1, \n",
    "                 ndim_post_layers:int=64, dropout=0.1, learning_rate:float=0.001, dtype_in=tf.int32, \n",
    "                 dtype_pos=tf.int32) :\n",
    "    \n",
    "    ##  Input layer - Shape [B, S, 2]\n",
    "    x_in_enc = Input((None,), dtype=dtype_in, name=f\"{name}_encoder_input_layer\")\n",
    "    x_in_dec = Input((None,), dtype=dtype_in, name=f\"{name}_decoder_input_layer\")\n",
    "            \n",
    "    ##  Token embedding - assuming input feature 0 is the token ID\n",
    "    ##  Output shape [B, S, ndim_embedding]\n",
    "    x_embed_enc = Embedding(vocab_length, \n",
    "                            ndim_embedding, \n",
    "                            mask_zero=True, \n",
    "                            dtype=dtype, \n",
    "                            name=f\"{name}_encoder_embedding\")(x_in_enc)\n",
    "    x_embed_dec = Embedding(vocab_length, \n",
    "                            ndim_embedding, \n",
    "                            mask_zero=True, \n",
    "                            dtype=dtype, \n",
    "                            name=f\"{name}_decoder_embedding\")(x_in_dec)\n",
    "    \n",
    "    ##  Enumerate indices for positional encoding\n",
    "    x_pos_enc = Enumerate(name=f\"{name}_encoder_enumerate\", dtype=dtype_pos)(x_in_enc, minimal_dims=True)\n",
    "    x_pos_dec = Enumerate(name=f\"{name}_decoder_enumerate\", dtype=dtype_pos)(x_in_dec, minimal_dims=True)\n",
    "    \n",
    "    ##  Position encoding - assuming feature index pos_enc_slice_axis is the token index\n",
    "    ##  -  Output shape [B, S, 2*num_freqs]\n",
    "    x_pos_enc = PositionalEncoding(slice_index=pos_enc_slice_index,\n",
    "                                   num_freqs=num_freqs, \n",
    "                                   min_period=min_period, \n",
    "                                   max_period=max_period, \n",
    "                                   dtype=dtype, \n",
    "                                   name=f\"{name}_encoder_position_encoding\")(x_in_enc)\n",
    "    x_pos_dec = PositionalEncoding(slice_index=pos_enc_slice_index,\n",
    "                                   num_freqs=num_freqs, \n",
    "                                   min_period=min_period, \n",
    "                                   max_period=max_period, \n",
    "                                   dtype=dtype, \n",
    "                                   name=f\"{name}_decoder_position_encoding\")(x_in_dec)\n",
    "    \n",
    "    ##  Combine embedding and position encoding by concatenation\n",
    "    ##  - Output shape [B, S, ndim_embedding + 2*num_freqs]\n",
    "    x_enc = Concatenate(name=f\"{name}_encoder_concat\", dtype=dtype)([x_embed_enc, x_pos_enc])\n",
    "    x_dec = Concatenate(name=f\"{name}_decoder_concat\", dtype=dtype)([x_embed_dec, x_pos_dec])\n",
    "    \n",
    "    ##  If input to feed forward block is the same size as ndim_model then turn on skip_connect\n",
    "    skip_connect_enc = (ndim_model==x_enc.shape[-1])\n",
    "    skip_connect_dec = (ndim_model==x_dec.shape[-1])\n",
    "    logger.info(f\"Pre-attention skip connection set to encoder={skip_connect_enc}, decoder={skip_connect_dec}\")\n",
    "    \n",
    "    ##  Do initial pre-processing and collapse to model size\n",
    "    ##  - use layer_norm instead of batch_norm because tokens in sequence are not independent\n",
    "    x_enc = FeedForwardBlock(ndim_model, \n",
    "                             ndim_hidden=ndim_pre_layers, \n",
    "                             num_hidden_layers=num_pre_layers_encoder, \n",
    "                             dropout=dropout, \n",
    "                             layer_norm=True, \n",
    "                             batch_norm=False,  \n",
    "                             skip_connect=skip_connect_enc, \n",
    "                             dtype=dtype, \n",
    "                             name =f\"{name}_encoder_feedfwd_block_pre_attention\")(x_enc)\n",
    "    x_dec = FeedForwardBlock(ndim_model, \n",
    "                             ndim_hidden=ndim_pre_layers, \n",
    "                             num_hidden_layers=num_pre_layers_decoder, \n",
    "                             dropout=dropout, \n",
    "                             layer_norm=True, \n",
    "                             batch_norm=False,  \n",
    "                             skip_connect=skip_connect_dec, \n",
    "                             dtype=dtype, \n",
    "                             name =f\"{name}_decoder_feedfwd_block_pre_attention\")(x_dec)\n",
    "    \n",
    "    ##  Run encoder blocks\n",
    "    for layer_idx in range(num_encoder_layers) :\n",
    "        x_enc = EncoderBlock(ndim_model, \n",
    "                             num_heads, \n",
    "                             ndim_att_hidden, \n",
    "                             ndim_ff_hidden, \n",
    "                             dropout_mha=dropout, \n",
    "                             dtype=dtype, \n",
    "                             layer_norm=True, \n",
    "                             name=f\"{name}_encoder_block_{layer_idx+1}\")(x_enc)\n",
    "    \n",
    "    ##  Run decoder blocks\n",
    "    for layer_idx in range(num_decoder_layers) :\n",
    "        x_dec = DecoderBlock(ndim_model, \n",
    "                             num_heads, \n",
    "                             ndim_att_hidden, \n",
    "                             ndim_ff_hidden, \n",
    "                             dropout_mha=dropout, \n",
    "                             dtype=dtype, \n",
    "                             layer_norm=True, \n",
    "                             name=f\"{name}_decoder_block_{layer_idx+1}\")([x_dec, x_enc])\n",
    "        \n",
    "    ##  Collapse to model size\n",
    "    x = FeedForwardBlock(vocab_length, \n",
    "                         ndim_hidden=ndim_post_layers, \n",
    "                         num_hidden_layers=num_post_layers, \n",
    "                         skip_connect=False, \n",
    "                         layer_norm=True, \n",
    "                         dtype=dtype, \n",
    "                         name=f\"{name}_feedfwd_block_post_attention\")(x_dec)\n",
    "    \n",
    "    ##  Create model\n",
    "    model = Model([x_in_enc, x_in_dec], x, name=name)\n",
    "    \n",
    "    ##  Compile model with MSE loss for supervised learning to numerical output\n",
    "    model.compile(loss      = masked_sparse_categorical_crossentropy, \n",
    "                  optimizer = Adam(learning_rate=learning_rate), \n",
    "                  metrics   = [masked_accuracy])\n",
    "    \n",
    "    ##  Return model\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6a53e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##==================================================##\n",
    "##   Create supervised learning model - continued   ##\n",
    "##==================================================##\n",
    "\n",
    "model = create_model(vocab_length           = vocab_length, \n",
    "                     name                   = \"mathsformer_LLM\",\n",
    "                     ndim_model             = global_config[\"model\"][\"ndim_model\"],\n",
    "                     ndim_embedding         = global_config[\"model\"][\"ndim_embedding\"],\n",
    "                     num_freqs              = global_config[\"model\"][\"num_freqs\"],\n",
    "                     min_period             = global_config[\"model\"][\"min_period\"],\n",
    "                     max_period             = global_config[\"model\"][\"max_period\"],\n",
    "                     num_pre_layers_encoder = global_config[\"model\"][\"num_pre_layers_encoder\"],\n",
    "                     ndim_pre_layers        = global_config[\"model\"][\"ndim_pre_layers\"],\n",
    "                     num_encoder_layers     = global_config[\"model\"][\"num_encoder_layers\"],\n",
    "                     num_heads              = global_config[\"model\"][\"num_heads\"],\n",
    "                     ndim_att_hidden        = global_config[\"model\"][\"ndim_att_hidden\"],\n",
    "                     ndim_ff_hidden         = global_config[\"model\"][\"ndim_ff_hidden\"],\n",
    "                     num_pre_layers_decoder = global_config[\"model\"][\"num_pre_layers_decoder\"],\n",
    "                     num_decoder_layers     = global_config[\"model\"][\"num_decoder_layers\"],\n",
    "                     num_post_layers        = global_config[\"model\"][\"num_post_layers\"],\n",
    "                     ndim_post_layers       = global_config[\"model\"][\"ndim_post_layers\"],\n",
    "                     dropout                = global_config[\"model\"][\"dropout\"],\n",
    "                     learning_rate          = global_config[\"model\"][\"learning_rate\"],\n",
    "                     dtype_in               = global_config[\"data\" ][\"dtype\"],\n",
    "                     dtype                  = global_config[\"model\"][\"dtype\"],)\n",
    "\n",
    "logging.info(\"Model created with summary:\")\n",
    "model.summary(print_fn=logging.info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f7a52",
   "metadata": {},
   "source": [
    "##  5.  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea411fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================================##\n",
    "##   Create callbacks for training   ##\n",
    "##===================================##\n",
    "\n",
    "\n",
    "##  Create list of training callbacks\n",
    "callbacks = []\n",
    "\n",
    "\n",
    "##  Add logger callback\n",
    "logger.info(\"Registering training callback: LoggerCallback with DEBUG log-level\")\n",
    "callbacks.append(LoggerCallback(logger, loglvl=logging.DEBUG))\n",
    "\n",
    "\n",
    "##  Add callback for early stopping\n",
    "if global_config[\"training\"].get(\"early_stopping\", True) :\n",
    "    monitor, restore_best_weights = \"val_loss\", True\n",
    "    patience = global_config[\"training\"].get(\"early_stopping_patience\", 1)\n",
    "    logger.info(f\"Registering training callback: EarlyStopping with monitor={monitor}, patience={patience}, restore_best_weights={restore_best_weights}\")\n",
    "    callbacks.append(EarlyStopping(monitor=monitor, patience=patience, restore_best_weights=restore_best_weights))\n",
    "    \n",
    "    \n",
    "## Add callback for model checkpointing\n",
    "if global_config[\"training\"].get(\"model_checkpoint\", True) :\n",
    "    filepath = working_dir+\"/model_checkpoint_epoch{epoch}_val_loss_{val_loss:.5}.h5\"\n",
    "    logger.info(f\"Registering training callback: ModelCheckpoint with filepath={filepath}\")\n",
    "    callbacks.append(ModelCheckpoint(filepath=filepath))\n",
    "\n",
    "    \n",
    "##  Add callback to intermittently record loss over small subset of validation data\n",
    "batch_frequency = global_config.get(\"training\").get(\"loss_record_batch_frequency\", 1000)\n",
    "max_datapoints  = global_config.get(\"training\").get(\"loss_record_max_datapoints\" , 2048)\n",
    "num_bootstrap   = global_config.get(\"training\").get(\"loss_record_num_bootstrap\"  , 10)\n",
    "loss_record = LossRecord(\n",
    "    batch_frequency   = batch_frequency, \n",
    "    val_input         = [val_X[:max_datapoints], val_Y_in[:max_datapoints]], \n",
    "    val_output        = val_Y_out[:max_datapoints],\n",
    "    num_bootstrap     = num_bootstrap,\n",
    "    plot_on_train_end = True,\n",
    ")\n",
    "logger.info(f\"Registering training callback: LossRecord with batch_frequency={batch_frequency}, max_datapoints={max_datapoints}, num_bootstrap={num_bootstrap}\")\n",
    "callbacks.append(loss_record)\n",
    "\n",
    "    \n",
    "##  Add callback to intermittently record layer activations\n",
    "##  -  Since sublayers are not tracked by the computational graph, we cannot access sublayer.output within \n",
    "##     a keras function, and therefore we cannot track the activations of sublayers as we may wish\n",
    "batch_frequency = global_config.get(\"training\").get(\"layer_activation_record_batch_frequency\", 1000)\n",
    "max_datapoints  = global_config.get(\"training\").get(\"layer_activation_record_max_datapoints\" , 128)\n",
    "layer_activation_record = LayerActivationRecord(\n",
    "    batch_frequency = batch_frequency, \n",
    "    val_input       = [val_X[:max_datapoints], val_Y_in[:max_datapoints]], \n",
    ")\n",
    "logger.info(f\"Registering training callback: LayerActivationRecord with batch_frequency={batch_frequency}, max_datapoints={max_datapoints}\")\n",
    "callbacks.append(layer_activation_record)\n",
    "\n",
    "\n",
    "##  Add callback to intermittently record layer weights - use recursive=True to monitor all sublayers\n",
    "batch_frequency = global_config.get(\"training\").get(\"layer_weights_record_batch_frequency\", 1000)\n",
    "recursive       = global_config.get(\"training\").get(\"layer_weights_record_recursive\"      , True)\n",
    "layer_weights_record = LayerWeightsRecord(\n",
    "    batch_frequency = batch_frequency, \n",
    "    recursive       = recursive\n",
    ")\n",
    "logger.info(f\"Registering training callback: LayerWeightsRecord with batch_frequency={batch_frequency}, recursive={recursive}\")\n",
    "callbacks.append(layer_weights_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1736d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##=================##\n",
    "##   Train model   ##\n",
    "##=================##\n",
    "\n",
    "\n",
    "##  Fit the model if configured\n",
    "if global_config.get(\"training\",{}).get(\"train\",True) :\n",
    "    max_epochs = global_config[\"training\"][\"max_epochs\"]\n",
    "    batch_size = global_config[\"training\"][\"batch_size\"]\n",
    "    \n",
    "    logger.info(f\"Begin model training with max_epochs={max_epochs}, batch_size={batch_size}\")\n",
    "    model.fit([train_X, train_Y_in], \n",
    "              train_Y_out,\n",
    "              epochs          = max_epochs,\n",
    "              batch_size      = batch_size,\n",
    "              validation_data = ([val_X, val_Y_in], val_Y_out),\n",
    "              callbacks       = callbacks\n",
    "             )\n",
    "else :\n",
    "    logger.warning(\"Skipping model training following global config instructions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb38943",
   "metadata": {},
   "outputs": [],
   "source": [
    "##================##\n",
    "##   Save model   ##\n",
    "##================##\n",
    "\n",
    "save_fname = f\"{working_dir}/final_model.h5\"\n",
    "model.save(save_fname)\n",
    "logger.info(f\"Model save to file {save_fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc69448",
   "metadata": {},
   "source": [
    "## 6.  Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer_Seq_to_Seq :\n",
    "    \n",
    "    def __init__(self, model:Model, tokeniser_dict:dict, detokeniser_dict:dict, mask_char:str, seq_start_char:str, \n",
    "                 seq_end_char:str, input_dtype=None) :\n",
    "        \"\"\"\n",
    "        class Transformer_Seq_to_Seq\n",
    "        \n",
    "        Wrapper for sequence-to-sequence model that allows easy transform operations\n",
    "        \n",
    "        Input:\n",
    "        \n",
    "            >  model, Model\n",
    "               Sequence-to-sequence keras model\n",
    "            \n",
    "            >  tokeniser_dict, dict\n",
    "               Dictionary of character:token pairs\n",
    "               \n",
    "            >  detokeniser_dict, dict\n",
    "               Dictionary of token_character pairs\n",
    "               \n",
    "            >  mask_char, str of length 1\n",
    "               Masking character\n",
    "               \n",
    "            >  seq_start_char, str of length 1\n",
    "               Special character denoting the start of a sequence\n",
    "               \n",
    "            >  seq_end_char, str of length 1\n",
    "               Special character denoting the end of a sequence\n",
    "               \n",
    "            >  input_dtype, tf dtype compatible, default=None\n",
    "               dtype of input data tensors\n",
    "        \"\"\"\n",
    "        self.model            = model\n",
    "        self.tokeniser_dict   = tokeniser_dict\n",
    "        self.detokeniser_dict = detokeniser_dict\n",
    "        self.mask_char        = mask_char\n",
    "        self.seq_start_char   = seq_start_char\n",
    "        self.seq_end_char     = seq_end_char\n",
    "        self.input_dtype      = input_dtype\n",
    "        \n",
    "\n",
    "    def transform_from_data_tensor(self, X, max_tokens:int=-1, device:str=\"CPU\") :\n",
    "        \"\"\"\n",
    "        Transform a tensor of input data into its predicted output string using argmax to select tokens\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  X, Tensor with final dimensions [S, 2]\n",
    "               Tensor of (token, index) pairs for the input sequence of length S\n",
    "               \n",
    "            >  max_tokens, int, default=-1\n",
    "               Maximum tokens in sequence\n",
    "               \n",
    "            >  device, str, default=\"CPU\"\n",
    "               Device to run tensorflow on\n",
    "        \"\"\"\n",
    "        ##  Recurse over tensor of inputs\n",
    "        if len(X.shape) > 1 and X.shape[0] > 1 :\n",
    "            return [self.transform_from_data_tensor(Xp, max_tokens, device) for Xp in X]\n",
    "        \n",
    "        ##  Check arguments\n",
    "        if max_tokens > 0 and max_tokens < 3 :\n",
    "            raise ArgumentError(f\"max_tokens must have a minimum length of 3, {max_tokens} provided\")\n",
    "        \n",
    "        ##  If no internal dtype chosen then fall back to X.dtype\n",
    "        dtype = self.input_dtype\n",
    "        if not dtype : dtype = X.dtype\n",
    "        \n",
    "        ##  If X is shape [S, F] then reshape it to [B, S, F] with B=1\n",
    "        one_sequence_provided = len(X.shape) == 1\n",
    "        if one_sequence_provided :\n",
    "            X = X[tf.newaxis, :]\n",
    "            \n",
    "        #  Find tokens for start and end characters\n",
    "        start_token = self.tokeniser_dict[self.seq_start_char]\n",
    "        end_token   = self.tokeniser_dict[self.seq_end_char  ]\n",
    "\n",
    "        ##  Select tf device\n",
    "        with tf.device(device) :\n",
    "\n",
    "            ##  Create initial sequence with shape [1, 1] and features [[seq_start_char]]\n",
    "            Y = tf.cast([[start_token]], dtype=dtype)\n",
    "\n",
    "            ##  Keep generating tokens until we reach an end character, or the token limit\n",
    "            best_token, num_tokens = start_token, 1\n",
    "            while best_token != end_token and (max_tokens <= 0 or num_tokens < max_tokens) :\n",
    "                \n",
    "                ##  Generate logit predictions for all indices; slice logits for first sequence & final index\n",
    "                token_logits = model([X, Y])[0,-1]\n",
    "                \n",
    "                ##  Use argmax operation to interpret the predicted token\n",
    "                best_token   = np.argmax(token_logits.numpy(), axis=-1)\n",
    "                \n",
    "                ##  Append new token to the list\n",
    "                Y = tf.concat([Y, tf.constant([[best_token]], dtype=dtype)], axis=1)\n",
    "                \n",
    "                ##  Iterate forwards num_tokens\n",
    "                num_tokens += 1\n",
    "\n",
    "        ##  Convert tensor-of-tokens-and-indices Y into a string of detokenised characters\n",
    "        out_str = \"\".join([self.detokeniser_dict[t] for t in Y.numpy()[0]])\n",
    "        out_str = out_str[1:]\n",
    "        if out_str[-1] == self.seq_end_char : out_str = out_str[:-1]\n",
    "            \n",
    "        ##  Return string with same format as input\n",
    "        return out_str if one_sequence_provided else [out_str]\n",
    "    \n",
    "    \n",
    "    def transform_from_string(self, X, max_tokens:int=-1, device:str=\"CPU\") :\n",
    "        \"\"\"\n",
    "        Transform a list of input strings into their predicted output string using argmax to select tokens\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            >  X, string or list of strings\n",
    "               String(s) to be transformed\n",
    "               \n",
    "            >  max_tokens, int, default=-1\n",
    "               Maximum tokens in sequence\n",
    "               \n",
    "            >  device, str, default=\"CPU\"\n",
    "               Device to run tensorflow on\n",
    "        \"\"\"\n",
    "        \n",
    "        ##  Cast data to list format\n",
    "        one_string_provided = type(X) == str\n",
    "        if one_string_provided : X = [X]\n",
    "                     \n",
    "        ##  If no dtype set internally then fall back to using tf.int32   \n",
    "        dtype = self.input_dtype\n",
    "        if not dtype : dtype = tf.int32\n",
    "        \n",
    "        ##  Figure out the maximum sequence length of the inputs to determine tensor dims\n",
    "        fix_output_length = max([len(x) for x in X]) + len(self.seq_start_char) + len(self.seq_end_char)\n",
    "                \n",
    "        ##  Create tensor of input data\n",
    "        X = strings_to_tensor(X, self.tokeniser_dict, fix_output_length=fix_output_length, \n",
    "                              mask_char=self.mask_char, seq_start_char=self.seq_start_char, \n",
    "                              seq_end_char=self.seq_end_char, add_position_indices=False, dtype=dtype)\n",
    "        \n",
    "        ##  Predict outputs from tensor input\n",
    "        Y = self.transform_from_data_tensor(X, max_tokens=max_tokens, device=device)\n",
    "        \n",
    "        ##  Return strings with same format as input\n",
    "        return Y[0] if one_string_provided else Y\n",
    "    \n",
    "    \n",
    "transformer = Transformer_Seq_to_Seq(model, tokeniser_dict, detokeniser_dict, mask_char, seq_start_char, seq_end_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##====================================##\n",
    "##   Evaluate model using test data   ##\n",
    "##====================================##\n",
    "\n",
    "##  Find out how many datapoints to print predictions for \n",
    "num_print = int(global_config.get(\"evaluate\",{}).get(\"num_print\", 20))\n",
    "\n",
    "##  Log table header\n",
    "logger.info(\"Evaluating text --> text mathsformer on the test set:\")\n",
    "logger.info(\"-\"*50)\n",
    "logger.info(\"INPUT\".rjust(12) + \"TRUE\".rjust(8) + \"PRED\".rjust(8) + \"CORRECT\".rjust(10) + \"RESIDUAL\".rjust(10))\n",
    "logger.info(\"-\"*50)\n",
    "\n",
    "##  Get model predictions and log alongside true labels \n",
    "for x, x_str, true_y_str in zip(test_X[:num_print], \n",
    "                     detokenise_strings(test_X[:num_print,:].numpy(), detokeniser_dict, mask_char=mask_char, \n",
    "                                        seq_start_char=seq_start_char, seq_end_char=seq_end_char),\n",
    "                     detokenise_strings(test_Y_out[:num_print].numpy(), detokeniser_dict, mask_char=mask_char, \n",
    "                                        seq_start_char=seq_start_char, seq_end_char=seq_end_char)\n",
    "                    ) :\n",
    "    pred_y_str = transformer.transform_from_data_tensor(x, max_tokens=20)\n",
    "    result     = \"  X\" if pred_y_str == true_y_str else \"\"\n",
    "    try    : residual = str(int(pred_y_str.replace(\"N\",\"-\")) - int(true_y_str.replace(\"N\",\"-\")))\n",
    "    except : residual = \"N/A\"\n",
    "    logger.info(x_str.rjust(12) + true_y_str.rjust(8) + pred_y_str.rjust(8) + result.rjust(8) + residual.rjust(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c02db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##======================================================================================##\n",
    "##   Evaluate model using random numbers with lengths not experienced during training   ##\n",
    "##======================================================================================##\n",
    "\n",
    "##  Find out how many datapoints to print predictions for \n",
    "num_print = int(global_config.get(\"evaluate\",{}).get(\"num_print\", 20))\n",
    "\n",
    "##  Find out the maximum number of digits to allow per input number (= 2 higher than maximum input length)\n",
    "max_char = math.floor(math.log10(max_int)) + 3\n",
    "\n",
    "##  Log table header\n",
    "logger.info(f\"Evaluating text --> text mathsformer on randomly generated strings with max number length of {max_char}:\")\n",
    "logger.info(\"-\"*70)\n",
    "logger.info(\"INPUT\".rjust(15) + \"TRUE\".rjust(15) + \"PRED\".rjust(15) + \"RESIDUAL\".rjust(15))\n",
    "logger.info(\"-\"*70)\n",
    "\n",
    "for i in range(num_print) :\n",
    "    m1, m2 = np.random.randint(low=1, high=max_char+1, size=(2,))\n",
    "    n1, n2 = np.random.randint(low=10**(m1-1), high=10**m1), np.random.randint(low=10**(m2-1), high=10**m2)\n",
    "    s1, s2 = np.random.choice([-1, 1], size=(2,))\n",
    "    n1, n2 = s1*n1, s2*n2\n",
    "    s1, s2 = str(n1).replace(\"-\",\"N\"), str(n2).replace(\"-\",\"N\")\n",
    "    op = np.random.choice([\"+\", \"-\"])\n",
    "    if op == \"+\" : answer = n1 + n2\n",
    "    if op == \"-\" : answer = n1 - n2\n",
    "    query = f\"{s1}{op}{s2}\"\n",
    "    guess = transformer.transform_from_string(query).replace(\"N\",\"-\")\n",
    "    residual = str(int(guess) - answer)\n",
    "    logger.info(query.rjust(15) + str(answer).rjust(15) + guess.rjust(15) + residual.rjust(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfb55b",
   "metadata": {},
   "source": [
    "##  7. Additional visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e385e68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##=============================================##\n",
    "##   Visualise layer weights during training   ##\n",
    "##=============================================##\n",
    "\n",
    "if len(layer_weights_record.batch_indices) == 0 :\n",
    "    logger.warning(\"Not plotting layer weights because no data found\")\n",
    "else :\n",
    "    logger.info(\"Plotting layer weights\")\n",
    "    layer_weights_record.plot(num_col=6)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc708793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
