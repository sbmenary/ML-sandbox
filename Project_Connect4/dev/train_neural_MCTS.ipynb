{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69d1a5f",
   "metadata": {},
   "source": [
    "# Train neural MCTS\n",
    "\n",
    "---\n",
    "\n",
    "Author: S. Menary [sbmenary@gmail.com]\n",
    "\n",
    "Date  : 2023-01-15, last edit 2023-01-19\n",
    "\n",
    "Brief : Train a bot neural network by playing games using a previous bot generation (vanilla MCTS if this is the first iteration)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54ef68",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff93cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=====================================##\n",
    "##  All imports should be placed here  ##\n",
    "##=====================================##\n",
    "\n",
    "##  Python core libs\n",
    "import pickle, sys, time\n",
    "\n",
    "##  PyPI libs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "##  Local packages\n",
    "from connect4.utils    import DebugLevel\n",
    "from connect4.game     import BinaryPlayer, GameBoard, GameResult\n",
    "from connect4.MCTS     import Node_NeuralMCTS, PolicyStrategy\n",
    "from connect4.bot      import Bot_NeuralMCTS, Bot_VanillaMCTS\n",
    "from connect4.parallel import generate_from_processes\n",
    "from connect4.methods  import get_training_data_from_bot_game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e7c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Python version is 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "       Numpy version is 1.23.2\n"
     ]
    }
   ],
   "source": [
    "##=====================================##\n",
    "##  Print version for reproducibility  ##\n",
    "##=====================================##\n",
    "\n",
    "print(f\"{'Python'    .rjust(12)} version is {sys.version}\")\n",
    "print(f\"{'Numpy'     .rjust(12)} version is {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c9a25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Configure run\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094a5692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old model: ../models/.neural_model_v5.h5\n",
      "Using new model: ../models/.neural_model_v6.h5\n",
      "Using data file: ../data/.training_data_v6.pickle\n"
     ]
    }
   ],
   "source": [
    "##============================##\n",
    "##  Set global config values  ##\n",
    "##============================##\n",
    "\n",
    "model_idx      = 6\n",
    "old_model_name = f\"../models/.neural_model_v{model_idx-1}.h5\"\n",
    "new_model_name = f\"../models/.neural_model_v{model_idx}.h5\"\n",
    "\n",
    "load_data  = False\n",
    "save_data  = not load_data\n",
    "data_fname = f\"../data/.training_data_v{model_idx}.pickle\"\n",
    "\n",
    "tune_previous_model = True       ## whether to load+tune previous model or create+train new one\n",
    "\n",
    "num_processes      = 7\n",
    "num_games_per_proc = 300\n",
    "base_seed          = int(time.time())\n",
    "duration           = 3\n",
    "discount           = 0.99\n",
    "monitor_frequency  = 3\n",
    "num_random_moves   = 2\n",
    "policy_strategy    = PolicyStrategy.NOISY_POSTERIOR_POLICY\n",
    "\n",
    "print(f\"Using old model: {old_model_name}\")\n",
    "print(f\"Using new model: {new_model_name}\")\n",
    "print(f\"Using data file: {data_fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad4ef1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data generation and preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50c3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "##========================================================================##\n",
    "##  Define method allowing data to be generated in parallel subprocesses  ##\n",
    "##========================================================================##\n",
    "\n",
    "##  N.B. connect4.neural import must be performed inside the method executed inside each child process\n",
    "##  to avoid a deadlock caused when a tf session has already been created in __main__\n",
    "\n",
    "def generate_datapoints_process(proc_idx, num_games, out_queue, func_args) :\n",
    "    from connect4.neural import load_model\n",
    "    model_name, duration, discount, num_random_moves, policy_strategy, base_seed = func_args\n",
    "    np.random.seed(base_seed+proc_idx)\n",
    "    model = load_model(model_name) if len(model_name) > 0 else None\n",
    "    for game_idx in range(num_games) :\n",
    "        _ = get_training_data_from_bot_game(model, duration, discount, num_random_moves)\n",
    "        out_queue.put(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2100 datapoints with base seed 1674161700\n",
      "Using duration = 3.000\n",
      "Using discount = 0.990\n",
      "Generated 0 / 2100 results [t=6.01s]"
     ]
    }
   ],
   "source": [
    "##======================##\n",
    "##  Generate/load data  ##\n",
    "##======================##\n",
    "\n",
    "if load_data :\n",
    "    ##  Load data from pickle file\n",
    "    print(f\"Loading data from file: {data_fname}\")\n",
    "    loaded   = pickle.load(open(data_fname, \"rb\"))\n",
    "    model_in = loaded[\"model_in\"]\n",
    "    model_p  = loaded[\"model_p\" ]\n",
    "    model_v  = loaded[\"model_v\" ]\n",
    "else :\n",
    "    ##  Run subprocesses to generate new data\n",
    "    print(f\"Generating {num_processes*num_games_per_proc} datapoints with base seed {base_seed}\")\n",
    "    print(f\"Using duration = {duration:.3f}\\nUsing discount = {discount:.3f}\")\n",
    "    results = generate_from_processes(\n",
    "        func                 = generate_datapoints_process, \n",
    "        func_args            = [old_model_name, duration, discount, num_random_moves, policy_strategy, base_seed],\n",
    "        num_proc             = num_processes, \n",
    "        num_results_per_proc = num_games_per_proc, \n",
    "        mon_freq             = monitor_frequency)\n",
    "                              \n",
    "    ##  Retrieve training data from worker thread\n",
    "    model_in = np.concatenate([r[0] for r in results])\n",
    "    model_p  = np.concatenate([r[1] for r in results])\n",
    "    model_v  = np.concatenate([r[2] for r in results])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb071aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=============##\n",
    "##  Save data  ##\n",
    "##=============##\n",
    "\n",
    "if save_data :\n",
    "    print(f\"Saving data to file: {data_fname}\")\n",
    "    to_save = {\"model_in\":model_in, \"model_p\":model_p, \"model_v\":model_v, \n",
    "               \"base_seed\":base_seed, \"duration\":duration, \"discount\":discount}\n",
    "    pickle.dump(to_save, open(data_fname, \"wb\"))\n",
    "else :\n",
    "    print(\"Not saving data because it was loaded from file\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc238c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##================================##\n",
    "##  Report on the data generated  ##\n",
    "##================================##\n",
    "\n",
    "print(f\"model_in with shape: {model_in.shape}\")\n",
    "print(f\"model_p  with shape: {model_p .shape}\")\n",
    "print(f\"model_v  with shape: {model_v .shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================================##\n",
    "##  Data Augmentation and splitting  ##\n",
    "##===================================##\n",
    "\n",
    "##  Randomly flip board and posterior in x-direction to created augmented dataset reflecting game symmetry\n",
    "for idx in range(len(model_in)) :\n",
    "    if np.random.choice([True, False]) : continue\n",
    "    model_in[idx] = np.flip(model_in[idx], axis=0)\n",
    "    model_p [idx] = np.flip(model_p [idx], axis=0)\n",
    "\n",
    "##  Shuffle data\n",
    "indices = np.arange(len(model_in))\n",
    "np.random.shuffle(indices)\n",
    "model_in, model_p, model_v = model_in[indices], model_p [indices], model_v [indices]\n",
    "\n",
    "##  Split data into train and val sets\n",
    "num_datapoints = len(model_in)\n",
    "split_idx = int(0.7*num_datapoints)\n",
    "\n",
    "train_model_in = model_in[:split_idx]\n",
    "train_model_p  = model_p [:split_idx]\n",
    "train_model_v  = model_v [:split_idx]\n",
    "\n",
    "val_model_in = model_in[split_idx:]\n",
    "val_model_p  = model_p [split_idx:]\n",
    "val_model_v  = model_v [split_idx:]\n",
    "\n",
    "print(f\"Created training set of size {len(train_model_v)}\")\n",
    "print(f\"Created validation set of size {len(val_model_v)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c9e74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c950d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##=========================##\n",
    "##  Create/load the model  ##\n",
    "##=========================##\n",
    "\n",
    "##  Must import neural module here because it uses tensorflow, which breaks the subprocessing if done above\n",
    "from connect4.neural import create_model, load_model\n",
    "\n",
    "##  Load or create model\n",
    "if tune_previous_model :\n",
    "    print(\"Loading model from previous iteration\")\n",
    "    new_model = load_model(old_model_name)\n",
    "else :\n",
    "    print(\"Creating new model\")\n",
    "    new_model = create_model(name=new_model_name, num_conv_blocks=4, num_filters=40, num_dense=5, \n",
    "                         dense_width=200, batch_norm=True)\n",
    "\n",
    "##  Print model summary\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e95745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##==================##\n",
    "##  Model training  ##\n",
    "##==================##\n",
    "\n",
    "##  Must import tensorflow modules here, because it breaks the subprocessing if done above\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "##  Train the model\n",
    "history = new_model.fit(\n",
    "            model_in, [model_p, model_v], epochs=1000, batch_size=100,\n",
    "            validation_data=(val_model_in, [val_model_p, val_model_v]),\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)])\n",
    "\n",
    "##  Save the new model\n",
    "new_model.save(new_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=================================##\n",
    "##  Visualise the training curves  ##\n",
    "##=================================##\n",
    "\n",
    "##  Define metrics to track\n",
    "metrics     = [\"loss\", \"policy_loss\", \"value_loss\"]\n",
    "num_metrics = len(metrics)\n",
    "\n",
    "##  Plot metrics on both linear and log scales\n",
    "for do_log in [False, True] :\n",
    "    \n",
    "    ##  Create, show and close figure showing training curves (one axis per metric)\n",
    "    fig      = plt.figure(figsize=(4*num_metrics, 3))\n",
    "    for ax_idx, metric in enumerate(metrics) :\n",
    "        val_metric = f\"val_{metric}\"\n",
    "        ax  = fig.add_subplot(1, num_metrics, 1+ax_idx)\n",
    "        ax.plot(history.history[metric], \"-\", lw=3, c=\"r\", alpha=0.5, label=metric)\n",
    "        if val_metric in history.history :\n",
    "            ax.plot(history.history[val_metric], \"-\", lw=3, c=\"b\", alpha=0.5, label=val_metric)\n",
    "        ax.legend(loc=\"upper right\", frameon=False, fontsize=10)\n",
    "        ax.set_xlabel(\"Epoch\", labelpad=15, fontsize=11, ha=\"center\", va=\"top\")\n",
    "        if do_log : ax.set_yscale(\"log\")\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2c57a",
   "metadata": {},
   "source": [
    "- Note that we consistently observe val_loss < loss due to either (i) the fact that we are using dropout which affect train but not val metrics, or (ii) train metrics being summed over an epoch but val metrics being calculated only at the end. Suspect (i) because the lag appears to be longer than one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6390d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
