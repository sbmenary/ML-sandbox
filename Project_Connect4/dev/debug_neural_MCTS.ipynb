{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69d1a5f",
   "metadata": {},
   "source": [
    "#  Debug neural MCTS\n",
    "\n",
    "---\n",
    "\n",
    "Author: S. Menary [sbmenary@gmail.com]\n",
    "\n",
    "Date  : 2023-01-15, last edit 2023-01-19\n",
    "\n",
    "Brief : Debug behaviour of bot using a neural network bot with Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54ef68",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff93cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=====================================##\n",
    "##  All imports should be placed here  ##\n",
    "##=====================================##\n",
    "\n",
    "##  Python core libs\n",
    "import pickle, sys, time\n",
    "\n",
    "##  PyPI libs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "##  Local packages\n",
    "from connect4.utils    import DebugLevel\n",
    "from connect4.game     import BinaryPlayer, GameBoard, GameResult\n",
    "from connect4.MCTS     import Node_NeuralMCTS, PolicyStrategy\n",
    "from connect4.bot      import Bot_NeuralMCTS, Bot_VanillaMCTS\n",
    "from connect4.parallel import generate_from_processes\n",
    "from connect4.neural   import load_model\n",
    "from connect4.methods  import get_training_data_from_bot_game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e7c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Python version is 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "       Numpy version is 1.23.2\n"
     ]
    }
   ],
   "source": [
    "##=====================================##\n",
    "##  Print version for reproducibility  ##\n",
    "##=====================================##\n",
    "\n",
    "print(f\"{'Python'    .rjust(12)} version is {sys.version}\")\n",
    "print(f\"{'Numpy'     .rjust(12)} version is {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094a5692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ../models/.neural_model_v4.h5\n"
     ]
    }
   ],
   "source": [
    "##============================##\n",
    "##  Set global config values  ##\n",
    "##============================##\n",
    "\n",
    "model_idx = 4\n",
    "model_name = f\"../models/.neural_model_v{model_idx}.h5\"\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929b157",
   "metadata": {},
   "source": [
    "##  Test neural model MCTS\n",
    "\n",
    "- Test that we can propagate values and make decisions correctly with neural MCTS\n",
    "- Find a good value for the duration parameter, (smallest value that allows us to make stable posteriors)\n",
    "- Cannot run these cells when doing regular run, since tf cannot be used in main process before spawning children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83b333f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial game board:\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Initial tree:\n",
      "> [0: ROOT] N=0, T=0.000, E=nan, Q=-inf\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "\n",
      "Running MCTS step 0\n",
      "Select unvisited action X:0\n",
      "Simulation using prior value 0.0227\n",
      "Node X:0 with parent=X, N=0, T=0.00 receiving score 0.02\n",
      "Node ROOT with parent=NONE, N=0, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 1\n",
      "Select unvisited action X:6\n",
      "Simulation using prior value 0.0227\n",
      "Node X:6 with parent=X, N=0, T=0.00 receiving score 0.02\n",
      "Node ROOT with parent=NONE, N=1, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 2\n",
      "Select unvisited action X:2\n",
      "Simulation using prior value 0.0008\n",
      "Node X:2 with parent=X, N=0, T=0.00 receiving score 0.00\n",
      "Node ROOT with parent=NONE, N=2, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 3\n",
      "Select unvisited action X:1\n",
      "Simulation using prior value 0.7073\n",
      "Node X:1 with parent=X, N=0, T=0.00 receiving score 0.71\n",
      "Node ROOT with parent=NONE, N=3, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 4\n",
      "Select unvisited action X:5\n",
      "Simulation using prior value -0.0251\n",
      "Node X:5 with parent=X, N=0, T=0.00 receiving score -0.03\n",
      "Node ROOT with parent=NONE, N=4, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 5\n",
      "Select unvisited action X:3\n",
      "Simulation using prior value -0.0763\n",
      "Node X:3 with parent=X, N=0, T=0.00 receiving score -0.08\n",
      "Node ROOT with parent=NONE, N=5, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 6\n",
      "Select unvisited action X:4\n",
      "Simulation using prior value 0.4075\n",
      "Node X:4 with parent=X, N=0, T=0.00 receiving score 0.41\n",
      "Node ROOT with parent=NONE, N=6, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 7\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:5\n",
      "Simulation using prior value 0.9453\n",
      "Node O:5 with parent=O, N=0, T=0.00 receiving score -0.95\n",
      "Node X:3 with parent=X, N=1, T=-0.08 receiving score 0.94\n",
      "Node ROOT with parent=NONE, N=7, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 8\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:0\n",
      "Simulation using prior value 0.4945\n",
      "Node O:0 with parent=O, N=0, T=0.00 receiving score -0.49\n",
      "Node X:3 with parent=X, N=2, T=0.86 receiving score 0.49\n",
      "Node ROOT with parent=NONE, N=8, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 9\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:6\n",
      "Simulation using prior value 0.8661\n",
      "Node O:6 with parent=O, N=0, T=0.00 receiving score -0.87\n",
      "Node X:3 with parent=X, N=3, T=1.35 receiving score 0.86\n",
      "Node ROOT with parent=NONE, N=9, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 10\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:2\n",
      "Simulation using prior value 0.1320\n",
      "Node O:2 with parent=O, N=0, T=0.00 receiving score -0.13\n",
      "Node X:3 with parent=X, N=4, T=2.21 receiving score 0.13\n",
      "Node ROOT with parent=NONE, N=10, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 11\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:3\n",
      "Simulation using prior value -0.0185\n",
      "Node O:3 with parent=O, N=0, T=0.00 receiving score 0.02\n",
      "Node X:3 with parent=X, N=5, T=2.34 receiving score -0.02\n",
      "Node ROOT with parent=NONE, N=11, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 12\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:4\n",
      "Simulation using prior value 0.4507\n",
      "Node O:4 with parent=O, N=0, T=0.00 receiving score -0.45\n",
      "Node X:3 with parent=X, N=6, T=2.32 receiving score 0.45\n",
      "Node ROOT with parent=NONE, N=12, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 13\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:1\n",
      "Simulation using prior value -0.8791\n",
      "Node O:1 with parent=O, N=0, T=0.00 receiving score 0.88\n",
      "Node X:3 with parent=X, N=7, T=2.77 receiving score -0.87\n",
      "Node ROOT with parent=NONE, N=13, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 14\n",
      "Select known action X:1\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:0\n",
      "Simulation using prior value 0.0490\n",
      "Node O:0 with parent=O, N=0, T=0.00 receiving score -0.05\n",
      "Node X:1 with parent=X, N=1, T=0.71 receiving score 0.05\n",
      "Node ROOT with parent=NONE, N=14, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 15\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:1\n",
      "Simulation using prior value -0.0905\n",
      "Node X:1 with parent=X, N=0, T=0.00 receiving score -0.09\n",
      "Node O:3 with parent=O, N=1, T=0.02 receiving score 0.09\n",
      "Node X:3 with parent=X, N=8, T=1.89 receiving score -0.09\n",
      "Node ROOT with parent=NONE, N=15, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 16\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:5\n",
      "Simulation using prior value 0.0185\n",
      "Node X:5 with parent=X, N=0, T=0.00 receiving score 0.02\n",
      "Node O:3 with parent=O, N=2, T=0.11 receiving score -0.02\n",
      "Node X:3 with parent=X, N=9, T=1.81 receiving score 0.02\n",
      "Node ROOT with parent=NONE, N=16, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 17\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:1\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:3\n",
      "Simulation using prior value 0.0544\n",
      "Node X:3 with parent=X, N=0, T=0.00 receiving score 0.05\n",
      "Node O:1 with parent=O, N=1, T=0.88 receiving score -0.05\n",
      "Node X:3 with parent=X, N=10, T=1.82 receiving score 0.05\n",
      "Node ROOT with parent=NONE, N=17, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 18\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:2\n",
      "Simulation using prior value -0.0047\n",
      "Node X:2 with parent=X, N=0, T=0.00 receiving score -0.00\n",
      "Node O:3 with parent=O, N=3, T=0.09 receiving score 0.00\n",
      "Node X:3 with parent=X, N=11, T=1.88 receiving score -0.00\n",
      "Node ROOT with parent=NONE, N=18, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 19\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:3\n",
      "Simulation using prior value 0.0301\n",
      "Node X:3 with parent=X, N=0, T=0.00 receiving score 0.03\n",
      "Node O:3 with parent=O, N=4, T=0.09 receiving score -0.03\n",
      "Node X:3 with parent=X, N=12, T=1.87 receiving score 0.03\n",
      "Node ROOT with parent=NONE, N=19, T=0.00 receiving score 0.00\n",
      "\n",
      "Updated tree:\n",
      "> [0: ROOT] N=20, T=0.000, E=nan, Q=0.000\n",
      "     > [1: X:0] N=1, T=0.023, E=0.026, Q=0.023\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:1] N=2, T=0.756, E=0.380, Q=0.378\n",
      "          > [2: O:0] N=1, T=-0.049, E=-0.046, Q=-0.049\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:2] N=1, T=0.001, E=0.017, Q=0.001\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:3] N=13, T=1.902, E=0.460, Q=0.146\n",
      "          > [2: O:0] N=1, T=-0.494, E=-0.492, Q=-0.494\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:1] N=2, T=0.825, E=0.416, Q=0.413\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > [3: X:3] N=1, T=0.054, E=0.667, Q=0.054\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:2] N=1, T=-0.132, E=-0.110, Q=-0.132\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:3] N=5, T=0.065, E=0.595, Q=0.013\n",
      "               > None\n",
      "               > [3: X:1] N=1, T=-0.090, E=0.023, Q=-0.090\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > [3: X:2] N=1, T=-0.005, E=0.363, Q=-0.005\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > [3: X:3] N=1, T=0.030, E=0.104, Q=0.030\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > None\n",
      "               > [3: X:5] N=1, T=0.018, E=0.129, Q=0.018\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > None\n",
      "          > [2: O:4] N=1, T=-0.451, E=-0.431, Q=-0.451\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:5] N=1, T=-0.945, E=-0.941, Q=-0.945\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:6] N=1, T=-0.866, E=-0.864, Q=-0.866\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "     > [1: X:4] N=1, T=0.408, E=0.421, Q=0.408\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:5] N=1, T=-0.025, E=-0.023, Q=-0.025\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:6] N=1, T=0.023, E=0.026, Q=0.023\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##============================##\n",
    "##  Perform a few MCTS steps  ##\n",
    "##============================##\n",
    "\n",
    "##  Create game board\n",
    "game_board = GameBoard()\n",
    "print(f\"\\nInitial game board:\\n{game_board}\")\n",
    "\n",
    "##  Create a root node at the current game state\n",
    "model      = load_model(model_name)\n",
    "root_node  = Node_NeuralMCTS(game_board, params=[model, 1.], label=\"ROOT\")\n",
    "\n",
    "##  Print the initial value tree (should be a ROOT node with no children)\n",
    "print(\"Initial tree:\")\n",
    "print(root_node.tree_summary())\n",
    "print()\n",
    "\n",
    "##  Perform several MCTS steps with a HIGH debug level\n",
    "root_node.multi_step_MCTS(num_steps=20, max_sim_steps=-1, discount=0.99, debug_lvl=DebugLevel.MEDIUM)\n",
    "\n",
    "##  Print the updated value tree \n",
    "print(\"Updated tree:\")\n",
    "print(root_node.tree_summary())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af26ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bot <connect4.bot.Bot_NeuralMCTS object at 0x12708e7d0>\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting uniformly random action\n",
      "Action values are:  0.023   0.076   0.001   0.046   0.020   -0.025  0.023 \n",
      "Visit counts are:   1       10      1       226     3       1       1     \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.00 0.98 0.00 0.00 0.00\n",
      "Action values are:  -0.024  0.128   0.018   0.371   0.009   0.020   0.006 \n",
      "Visit counts are:   1       1       1       242     1       1       1     \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.10 0.05 0.18 0.54 0.09 0.04 0.00\n",
      "Action values are:  -0.361  -0.352  -0.375  -0.186  -0.396  -0.347  -0.845\n",
      "Visit counts are:   25      12      45      134     22      11      1     \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.82 0.16 0.00 0.00 0.00\n",
      "Action values are:  -0.102  0.268   0.391   0.407   0.256   -0.144  -0.310\n",
      "Visit counts are:   1       1       202     39      1       1       1     \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | . | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.05 0.66 0.25 0.02 0.01 0.00\n",
      "Action values are:  -0.486  -0.336  -0.302  -0.440  -0.465  -0.463  -0.572\n",
      "Visit counts are:   1       13      162     62      5       2       1     \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.01 0.97 0.00 0.00 0.00\n",
      "Action values are:  0.032   -0.326  0.249   0.441   0.014   -0.030  -0.548\n",
      "Visit counts are:   1       1       3       235     1       1       1     \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.01 0.00 0.00 0.83 0.00 0.14 0.00\n",
      "Action values are:  -0.612  -0.793  -0.858  -0.562  -0.828  -0.517  -0.749\n",
      "Visit counts are:   2       1       1       201     1       35      1     \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.91 0.00 0.00 0.04 0.03\n",
      "Action values are:  0.273   0.276   0.572   -0.054  0.249   0.520   0.463 \n",
      "Visit counts are:   1       1       223     1       1       10      7     \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.97 0.00 0.01 0.00 0.00\n",
      "Action values are:  -0.989  -0.887  -0.595  -0.994  -0.873  -0.972  -0.998\n",
      "Visit counts are:   1       1       240     1       2       1       1     \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.06 0.38 0.09 0.02 0.43 0.01\n",
      "Action values are:  0.457   0.230   0.592   0.561   -0.108  0.611   -0.056\n",
      "Visit counts are:   1       15      94      23      5       105     2     \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.02 0.26 0.07 0.27 0.08 0.27 0.04\n",
      "Action values are:  -0.892  -0.627  -0.704  -0.646  -0.701  -0.616  -0.714\n",
      "Visit counts are:   5       64      16      66      20      66      9     \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.01 0.07 0.08 0.26 0.55 0.01 0.02\n",
      "Action values are:  -0.118  0.423   0.713   0.754   0.759   0.404   0.434 \n",
      "Visit counts are:   2       20      23      70      150     2       5     \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.01 0.05 0.03 0.07 0.03 0.11 0.70\n",
      "Action values are:  -0.826  -0.781  -0.888  -0.906  -0.942  -0.758  -0.768\n",
      "Visit counts are:   3       13      7       19      9       31      191   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling action from posterior policy 0.01 0.01 0.15 0.37 0.06 0.39 0.02\n",
      "Action values are:  0.193   0.489   0.619   0.827   0.673   0.796   0.090 \n",
      "Visit counts are:   2       2       38      91      16      96      4     \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.03 0.08 0.02 0.13 0.65 0.09\n",
      "Action values are:  -0.943  -0.855  -0.882  -0.989  -0.821  -0.733  -0.792\n",
      "Visit counts are:   1       7       20      6       33      170     23    \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.21 0.04 0.04 0.00 0.05 0.34 0.33\n",
      "Action values are:  0.902   0.819   0.747   0.201   0.746   0.832   0.840 \n",
      "Visit counts are:   52      10      9       1       12      85      82    \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.04 0.75 0.03 0.02 0.04 0.06 0.06\n",
      "Action values are:  -0.751  -0.585  -0.830  -0.986  -0.784  -0.735  -0.694\n",
      "Visit counts are:   11      186     7       6       9       15      15    \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.08 0.52 0.08 0.00 0.04 0.27 0.01\n",
      "Action values are:  0.556   0.640   0.540   0.003   0.543   0.495   0.327 \n",
      "Visit counts are:   21      128     19      1       10      67      2     \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.50 0.24 0.04 0.00 0.03 0.06 0.13\n",
      "Action values are:  -0.449  -0.653  -0.721  -0.996  -0.908  -0.774  -0.657\n",
      "Visit counts are:   126     59      10      1       7       15      33    \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.33 0.40 0.05 0.00 0.00 0.20 0.00\n",
      "Action values are:  0.637   0.557   0.560   -0.494  0.225   0.562   0.402 \n",
      "Visit counts are:   81      96      13      1       1       49      1     \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.04 0.03 0.81 0.03 0.08\n",
      "Action values are:  -0.875  -0.607  -0.745  -0.450  -0.661  -0.582\n",
      "Visit counts are:   1       11      7       199     7       20    \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.35 0.00 0.04 0.04 0.24 0.22 0.10\n",
      "Action values are:  0.503   0.355   0.401   0.325   0.510   0.482 \n",
      "Visit counts are:   85      11      11      58      55      25    \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.31 0.00 0.04 0.02 0.59 0.00 0.05\n",
      "Action values are:  -0.484  -0.553  -0.871  -0.214  -1.000  -0.585\n",
      "Visit counts are:   79      10      4       153     1       12    \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.04 0.00 0.00 0.03 0.06 0.84 0.03\n",
      "Action values are:  0.168   -0.174  0.129   -0.567  0.273   0.109 \n",
      "Visit counts are:   10      1       8       17      231     9     \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.93 0.00 0.02 0.01 0.03 0.01 0.00\n",
      "Action values are:  -0.160  -0.629  -0.971  -0.882  -0.723  -0.990\n",
      "Visit counts are:   241     5       3       7       2       1     \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling action from posterior policy 0.35 0.00 0.01 0.00 0.59 0.04 0.01\n",
      "Action values are:  0.159   0.070   -0.194  0.124   0.064   -0.002\n",
      "Visit counts are:   90      2       1       150     11      2     \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.89 0.00 0.01 0.02 0.03 0.05 0.01\n",
      "Action values are:  -0.236  -0.838  -0.720  -0.650  -0.412  -0.927\n",
      "Visit counts are:   225     2       5       7       12      2     \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.01 0.00 0.94 0.04 0.01\n",
      "Action values are:  0.005   0.018   0.195   -0.139  0.089 \n",
      "Visit counts are:   2       1       248     10      2     \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.03 0.02 0.15 0.07 0.73\n",
      "Action values are:  -0.314  -0.249  -0.264  -0.264  -0.001\n",
      "Visit counts are:   8       4       37      17      180   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.00 0.66 0.21 0.03 0.11\n",
      "Action values are:  -0.677  0.371   -0.036  -0.295  -0.031\n",
      "Visit counts are:   1       205     64      8       34    \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.81 0.00 0.06 0.07 0.07\n",
      "Action values are:  -0.649  -0.874  -0.898  -0.799\n",
      "Visit counts are:   435     30      39      36    \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.01 0.00 0.00 0.01 0.98\n",
      "Action values are:  0.005   -0.439  -0.244  0.750 \n",
      "Visit counts are:   4       1       8       572   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.01 0.00 0.09 0.22 0.68\n",
      "Action values are:  -0.251  -0.791  -0.840  -0.740\n",
      "Visit counts are:   4       53      134     413   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.01 0.00 0.03 0.01 0.95\n",
      "Action values are:  0.053   0.110   -0.159  0.891 \n",
      "Visit counts are:   9       30      8       868   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.15 0.00 0.07 0.50 0.27\n",
      "Action values are:  -0.879  -0.889  -0.930  -0.887\n",
      "Visit counts are:   183     87      592     322   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.00 0.00 0.00 0.99 0.00\n",
      "Action values are:  -0.654  -0.271  0.974   0.001 \n",
      "Visit counts are:   5       9       2009    3     \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling action from posterior policy 0.00 0.00 0.03 0.00 0.09 0.80 0.08\n",
      "Action values are:  -0.961  -0.966  -0.979  -0.976\n",
      "Visit counts are:   65      222     1914    191   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.00 0.00 0.52 0.37 0.11\n",
      "Action values are:  0.479   1.000   0.984   0.975 \n",
      "Visit counts are:   17      1854    1335    383   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.31 0.00 0.29 0.00 0.40\n",
      "Action values are:  -0.989  -0.986  -0.989\n",
      "Visit counts are:   1714    1647    2231  \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.00 0.00 1.00 0.00 0.00\n",
      "Action values are:  0.018   0.990   0.117 \n",
      "Visit counts are:   1       9243    8     \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 0.16 0.00 0.00 0.00 0.84\n",
      "Action values are:  -0.990  -0.990\n",
      "Visit counts are:   1914    10433 \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Sampling action from posterior policy 0.00 0.00 1.00 0.00 0.00 0.00 0.00\n",
      "Action values are:  1.000 \n",
      "Visit counts are:   15913 \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: O\n"
     ]
    }
   ],
   "source": [
    "##==========================================##\n",
    "##  Play a game and generate training data  ##\n",
    "##==========================================##\n",
    "\n",
    "model_inputs, posteriors, values = get_training_data_from_bot_game(model, duration=1, discount=0.99,\n",
    "                                                                  debug_lvl = DebugLevel.LOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca240c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]] ,  posterior=0.00  0.04  0.00  0.93  0.01  0.00  0.00 ,  value = -0.662\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.00  0.00  0.98  0.00  0.00  0.00 ,  value = 0.669\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.10  0.05  0.18  0.54  0.09  0.04  0.00 ,  value = -0.676\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.00  0.82  0.16  0.00  0.00  0.00 ,  value = 0.683\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.05  0.66  0.25  0.02  0.01  0.00 ,  value = -0.689\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.00  0.01  0.97  0.00  0.00  0.00 ,  value = 0.696\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1  1 -1  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.01  0.00  0.00  0.83  0.00  0.14  0.00 ,  value = -0.703\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1 -1  1 -1  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.00  0.91  0.00  0.00  0.04  0.03 ,  value = 0.711\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [-1  1 -1  1  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.00  0.97  0.00  0.01  0.00  0.00 ,  value = -0.718\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.00  0.06  0.38  0.09  0.02  0.43  0.01 ,  value = 0.725\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.02  0.26  0.07  0.27  0.08  0.27  0.04 ,  value = -0.732\n",
      "[[-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.01  0.07  0.08  0.26  0.55  0.01  0.02 ,  value = 0.740\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.01  0.05  0.03  0.07  0.03  0.11  0.70 ,  value = -0.747\n",
      "[[-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.01  0.01  0.15  0.37  0.06  0.39  0.02 ,  value = 0.755\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.00  0.03  0.08  0.02  0.13  0.65  0.09 ,  value = -0.762\n",
      "[[-1 -1  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.21  0.04  0.04  0.00  0.05  0.34  0.33 ,  value = 0.770\n",
      "[[ 1  1  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.04  0.75  0.03  0.02  0.04  0.06  0.06 ,  value = -0.778\n",
      "[[-1 -1  0  0  0  0]\n",
      " [-1  1 -1  0  0  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.08  0.52  0.08  0.00  0.04  0.27  0.01 ,  value = 0.786\n",
      "[[ 1  1  0  0  0  0]\n",
      " [ 1 -1  1 -1  0  0]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.50  0.24  0.04  0.00  0.03  0.06  0.13 ,  value = -0.794\n",
      "[[-1 -1  0  0  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.33  0.40  0.05  0.00  0.00  0.20  0.00 ,  value = 0.802\n",
      "[[ 1  1  0  0  0  0]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.00  0.00  0.04  0.03  0.81  0.03  0.08 ,  value = -0.810\n",
      "[[-1 -1  0  0  0  0]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.35  0.00  0.04  0.04  0.24  0.22  0.10 ,  value = 0.818\n",
      "[[ 1  1 -1  0  0  0]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.31  0.00  0.04  0.02  0.59  0.00  0.05 ,  value = -0.826\n",
      "[[-1 -1  1  0  0  0]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.04  0.00  0.00  0.03  0.06  0.84  0.03 ,  value = 0.835\n",
      "[[ 1  1 -1  0  0  0]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  1  1  0  0  0]\n",
      " [-1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.93  0.00  0.02  0.01  0.03  0.01  0.00 ,  value = -0.843\n",
      "[[-1 -1  1 -1  0  0]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [ 1  1  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.35  0.00  0.01  0.00  0.59  0.04  0.01 ,  value = 0.851\n",
      "[[ 1  1 -1  1 -1  0]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  1  1  0  0  0]\n",
      " [-1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.89  0.00  0.01  0.02  0.03  0.05  0.01 ,  value = -0.860\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [ 1  1  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.00  0.00  0.01  0.00  0.94  0.04  0.01 ,  value = 0.869\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1  0]\n",
      " [-1  1  1 -1  0  0]\n",
      " [-1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.00  0.00  0.03  0.02  0.15  0.07  0.73 ,  value = -0.878\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [ 1 -1  1 -1  1  0]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [ 1  1  0  0  0  0]\n",
      " [-1 -1  0  0  0  0]] ,  posterior=0.00  0.00  0.00  0.66  0.21  0.03  0.11 ,  value = 0.886\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [-1  1 -1  1 -1 -1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [-1 -1  0  0  0  0]\n",
      " [ 1  1  0  0  0  0]] ,  posterior=0.00  0.00  0.81  0.00  0.06  0.07  0.07 ,  value = -0.895\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1 -1  0]\n",
      " [ 1 -1  1 -1  1  1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [ 1  1  0  0  0  0]\n",
      " [-1 -1  0  0  0  0]] ,  posterior=0.00  0.00  0.01  0.00  0.00  0.01  0.98 ,  value = 0.904\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  1  0]\n",
      " [-1  1 -1  1 -1 -1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [-1 -1  0  0  0  0]\n",
      " [ 1  1 -1  0  0  0]] ,  posterior=0.00  0.00  0.01  0.00  0.09  0.22  0.68 ,  value = -0.914\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1 -1  0]\n",
      " [ 1 -1  1 -1  1  1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [ 1  1  0  0  0  0]\n",
      " [-1 -1  1 -1  0  0]] ,  posterior=0.00  0.00  0.01  0.00  0.03  0.01  0.95 ,  value = 0.923\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  1  0]\n",
      " [-1  1 -1  1 -1 -1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [-1 -1  0  0  0  0]\n",
      " [ 1  1 -1  1 -1  0]] ,  posterior=0.00  0.00  0.15  0.00  0.07  0.50  0.27 ,  value = -0.932\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1 -1  0]\n",
      " [ 1 -1  1 -1  1  1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [ 1  1 -1  0  0  0]\n",
      " [-1 -1  1 -1  1  0]] ,  posterior=0.00  0.00  0.00  0.00  0.00  0.99  0.00 ,  value = 0.941\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  1  0]\n",
      " [-1  1 -1  1 -1 -1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [-1 -1  1 -1  0  0]\n",
      " [ 1  1 -1  1 -1  0]] ,  posterior=0.00  0.00  0.03  0.00  0.09  0.80  0.08 ,  value = -0.951\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1 -1  0]\n",
      " [ 1 -1  1 -1  1  1]\n",
      " [ 1 -1 -1  1  0  0]\n",
      " [ 1  1 -1  1 -1  0]\n",
      " [-1 -1  1 -1  1  0]] ,  posterior=0.00  0.00  0.00  0.00  0.52  0.37  0.11 ,  value = 0.961\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  1  0]\n",
      " [-1  1 -1  1 -1 -1]\n",
      " [-1  1  1 -1  0  0]\n",
      " [-1 -1  1 -1  1 -1]\n",
      " [ 1  1 -1  1 -1  0]] ,  posterior=0.00  0.00  0.31  0.00  0.29  0.00  0.40 ,  value = -0.970\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1 -1  0]\n",
      " [ 1 -1  1 -1  1  1]\n",
      " [ 1 -1 -1  1 -1  0]\n",
      " [ 1  1 -1  1 -1  1]\n",
      " [-1 -1  1 -1  1  0]] ,  posterior=0.00  0.00  0.00  0.00  1.00  0.00  0.00 ,  value = 0.980\n",
      "[[ 1  1 -1  1 -1  1]\n",
      " [ 1 -1  1 -1  1 -1]\n",
      " [ 1 -1 -1  1  1  0]\n",
      " [-1  1 -1  1 -1 -1]\n",
      " [-1  1  1 -1  1 -1]\n",
      " [-1 -1  1 -1  1 -1]\n",
      " [ 1  1 -1  1 -1  0]] ,  posterior=0.00  0.00  0.16  0.00  0.00  0.00  0.84 ,  value = -0.990\n",
      "[[-1 -1  1 -1  1 -1]\n",
      " [-1  1 -1  1 -1  1]\n",
      " [-1  1  1 -1 -1  0]\n",
      " [ 1 -1  1 -1  1  1]\n",
      " [ 1 -1 -1  1 -1  1]\n",
      " [ 1  1 -1  1 -1  1]\n",
      " [-1 -1  1 -1  1 -1]] ,  posterior=0.00  0.00  1.00  0.00  0.00  0.00  0.00 ,  value = 1.000\n"
     ]
    }
   ],
   "source": [
    "##====================================================##\n",
    "##  Check the data generated by the game is sensible  ##\n",
    "##====================================================##\n",
    "\n",
    "for inp, pos, val in zip(model_inputs, posteriors, values) :\n",
    "    print(inp[:,:,0], \",  posterior=\"+\"  \".join([f\"{x:.2f}\" for x in pos]), f\",  value = {val[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3dcbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting greedy action from posterior policy 0.00 0.03 0.01 0.94 0.01 0.00 0.00\n",
      "Action values are:  0.014   0.005   -0.007  0.017   -0.025  -0.025  -0.087\n",
      "Visit counts are:   3       24      13      825     9       1       2     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.00  0.00  0.01  0.98  0.01  0.00  0.00\n",
      "Prior values were:  0.02  0.71  0.00  -0.08  0.41  -0.03  0.02\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.03 0.00 0.96 0.00 0.00 0.00\n",
      "Action values are:  -0.494  -0.045  -0.414  0.007   -0.451  -0.945  -0.866\n",
      "Visit counts are:   1       25      2       852     1       1       1     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.00  0.00  0.01  0.97  0.01  0.00  0.00\n",
      "Prior values were:  -0.49  0.88  -0.13  0.02  -0.45  -0.95  -0.87\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.01 0.40 0.13 0.29 0.13 0.04 0.01\n",
      "Action values are:  -0.140  0.028   -0.052  0.015   -0.046  -0.050  -0.135\n",
      "Visit counts are:   7       356     115     254     113     37      8     \n",
      "Selecting action 1\n",
      "Prior policy was :  0.05  0.10  0.33  0.07  0.32  0.10  0.04\n",
      "Prior values were:  -0.52  -0.09  -0.00  0.03  0.02  0.02  -0.11\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.01 0.00 0.07 0.01 0.89 0.00 0.03\n",
      "Action values are:  -0.090  -0.111  -0.114  -0.048  0.087   -0.631  -0.047\n",
      "Visit counts are:   10      1       61      6       794     1       24    \n",
      "Selecting action 4\n",
      "Prior policy was :  0.02  0.00  0.33  0.00  0.63  0.00  0.01\n",
      "Prior values were:  0.16  -0.11  0.00  0.33  0.02  -0.63  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.30 0.01 0.01 0.66 0.01 0.00 0.00\n",
      "Action values are:  -0.176  -0.111  -0.154  -0.066  -0.145  -0.212  -0.376\n",
      "Visit counts are:   270     10      13      590     13      1       3     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.93  0.00  0.01  0.03  0.02  0.00  0.01\n",
      "Prior values were:  -0.06  0.22  -0.14  0.15  -0.12  -0.21  -0.14\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.00 0.00 0.99 0.00 0.00 0.00\n",
      "Action values are:  -0.129  -0.193  -0.796  0.118   -0.674  -0.172  -0.399\n",
      "Visit counts are:   2       3       1       891     1       1       1     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.00  0.01  0.02  0.96  0.01  0.00  0.00\n",
      "Prior values were:  0.12  0.08  -0.80  -0.46  -0.67  -0.17  -0.40\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.06 0.14 0.31 0.00 0.48 0.00 0.00\n",
      "Action values are:  -0.058  -0.055  -0.055  -0.144  -0.075  -0.333  -0.257\n",
      "Visit counts are:   58      133     284     3       439     3       2     \n",
      "Selecting action 4\n",
      "Prior policy was :  0.05  0.09  0.19  0.00  0.63  0.02  0.02\n",
      "Prior values were:  -0.51  0.53  -0.09  0.06  0.33  -0.27  0.02\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.01 0.01 0.00 0.96 0.01 0.00\n",
      "Action values are:  -0.998  -0.173  -0.639  -0.993  0.098   -0.397  -0.955\n",
      "Visit counts are:   1       10      13      1       885     7       1     \n",
      "Selecting action 4\n",
      "Prior policy was :  0.03  0.10  0.34  0.03  0.37  0.12  0.02\n",
      "Prior values were:  -1.00  0.07  -0.95  -0.99  0.41  -0.93  -0.95\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.01 0.01 0.37 0.00 0.59 0.00 0.00\n",
      "Action values are:  -0.234  -0.168  -0.099  -0.326  0.213   -0.718  -0.407\n",
      "Visit counts are:   10      15      378     3       607     3       5     \n",
      "Selecting action 4\n",
      "Prior policy was :  0.06  0.06  0.27  0.03  0.42  0.10  0.05\n",
      "Prior values were:  0.05  0.12  0.86  0.01  -0.62  -0.95  -0.05\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.23 0.09 0.03 0.43 0.06 0.10 0.06\n",
      "Action values are:  -0.189  -0.320  -0.627  -0.196  -0.244  -0.217  -0.207\n",
      "Visit counts are:   262     102     29      492     68      114     66    \n",
      "Selecting action 3\n",
      "Prior policy was :  0.03  0.38  0.37  0.04  0.10  0.07  0.01\n",
      "Prior values were:  -0.34  -0.79  0.28  0.10  0.05  0.23  0.38\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.91 0.07 0.01 0.00 0.01 0.00 0.00\n",
      "Action values are:  0.947   -0.220  -0.298  -0.976  -0.584  -0.891  -0.730\n",
      "Visit counts are:   2651    210     27      1       17      1       2     \n",
      "Selecting action 0\n",
      "Prior policy was :  0.07  0.25  0.15  0.02  0.42  0.02  0.07\n",
      "Prior values were:  -0.02  -0.17  -0.65  -0.98  -1.00  -0.89  -0.66\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting greedy action from posterior policy 0.10 0.30 0.15 0.16 0.08 0.14 0.08\n",
      "Action values are:  -0.964  -0.977  -0.969  -0.966  -0.964  -0.964  -0.962\n",
      "Visit counts are:   886     2687    1321    1403    752     1279    676   \n",
      "Selecting action 1\n",
      "Prior policy was :  0.05  0.52  0.15  0.11  0.05  0.10  0.03\n",
      "Prior values were:  -0.47  -0.92  0.53  0.28  0.46  0.53  0.11\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.00 1.00 0.00 0.00 0.00 0.00\n",
      "Action values are:  -0.291  -0.064  1.000   -0.845  0.046   0.188   -0.331\n",
      "Visit counts are:   16      25      17802   1       30      14      2     \n",
      "Selecting action 2\n",
      "Prior policy was :  0.15  0.20  0.31  0.01  0.21  0.08  0.03\n",
      "Prior values were:  -0.59  -0.35  -0.37  -0.84  -0.21  -0.72  -0.65\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: X\n"
     ]
    }
   ],
   "source": [
    "##===================================================================================================##\n",
    "##  Use MCTS to search for an optimal action, and compare the prior policy/value with the posterior  ##\n",
    "##===================================================================================================##\n",
    "\n",
    "game_board = GameBoard()\n",
    "bot = Bot_NeuralMCTS(model, policy_strategy=PolicyStrategy.GREEDY_POSTERIOR_POLICY)\n",
    "\n",
    "while not game_board.get_result() :\n",
    "    player = game_board.to_play\n",
    "    action = bot.choose_action(game_board, duration=5, discount=0.99, debug_lvl=DebugLevel.LOW)\n",
    "    print(\"Prior policy was :  \" + \"  \".join([f\"{c:.2f}\" for c in bot.root_node.child_priors]))\n",
    "    print(\"Prior values were:  \" + \"  \".join([f\"{player.value*c.prior_value:.2f}\" for c in bot.root_node.children]))\n",
    "    game_board.apply_action(action)\n",
    "    print(game_board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b677b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
