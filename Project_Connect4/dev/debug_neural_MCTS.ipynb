{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69d1a5f",
   "metadata": {},
   "source": [
    "#  Debug neural MCTS\n",
    "\n",
    "---\n",
    "\n",
    "Author: S. Menary [sbmenary@gmail.com]\n",
    "\n",
    "Date  : 2023-01-15, last edit 2023-01-19\n",
    "\n",
    "Brief : Debug behaviour of bot using a neural network bot with Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54ef68",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff93cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=====================================##\n",
    "##  All imports should be placed here  ##\n",
    "##=====================================##\n",
    "\n",
    "##  Python core libs\n",
    "import pickle, sys, time\n",
    "\n",
    "##  PyPI libs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "##  Local packages\n",
    "from connect4.utils    import DebugLevel\n",
    "from connect4.game     import BinaryPlayer, GameBoard, GameResult\n",
    "from connect4.MCTS     import Node_NeuralMCTS, PolicyStrategy\n",
    "from connect4.bot      import Bot_NeuralMCTS, Bot_VanillaMCTS\n",
    "from connect4.parallel import generate_from_processes\n",
    "from connect4.neural   import load_model\n",
    "from connect4.methods  import get_training_data_from_bot_game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e7c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Python version is 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "       Numpy version is 1.23.2\n"
     ]
    }
   ],
   "source": [
    "##=====================================##\n",
    "##  Print version for reproducibility  ##\n",
    "##=====================================##\n",
    "\n",
    "print(f\"{'Python'    .rjust(12)} version is {sys.version}\")\n",
    "print(f\"{'Numpy'     .rjust(12)} version is {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094a5692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ../models/.neural_model_v6.h5\n"
     ]
    }
   ],
   "source": [
    "##============================##\n",
    "##  Set global config values  ##\n",
    "##============================##\n",
    "\n",
    "model_idx = 6\n",
    "model_name = f\"../models/.neural_model_v{model_idx}.h5\"\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929b157",
   "metadata": {},
   "source": [
    "##  Test neural model MCTS\n",
    "\n",
    "- Test that we can propagate values and make decisions correctly with neural MCTS\n",
    "- Find a good value for the duration parameter, (smallest value that allows us to make stable posteriors)\n",
    "- Cannot run these cells when doing regular run, since tf cannot be used in main process before spawning children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83b333f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial game board:\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Initial tree:\n",
      "> [0: ROOT] N=0, T=0.000, E=nan, Q=-inf\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "     > None\n",
      "\n",
      "Running MCTS step 0\n",
      "Select unvisited action X:6\n",
      "Simulation using prior value 0.1297\n",
      "Node X:6 with parent=X, N=0, T=0.00 receiving score 0.13\n",
      "Node ROOT with parent=NONE, N=0, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 1\n",
      "Select unvisited action X:5\n",
      "Simulation using prior value 0.2291\n",
      "Node X:5 with parent=X, N=0, T=0.00 receiving score 0.23\n",
      "Node ROOT with parent=NONE, N=1, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 2\n",
      "Select unvisited action X:4\n",
      "Simulation using prior value 0.0829\n",
      "Node X:4 with parent=X, N=0, T=0.00 receiving score 0.08\n",
      "Node ROOT with parent=NONE, N=2, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 3\n",
      "Select unvisited action X:0\n",
      "Simulation using prior value 0.0377\n",
      "Node X:0 with parent=X, N=0, T=0.00 receiving score 0.04\n",
      "Node ROOT with parent=NONE, N=3, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 4\n",
      "Select unvisited action X:3\n",
      "Simulation using prior value 0.1717\n",
      "Node X:3 with parent=X, N=0, T=0.00 receiving score 0.17\n",
      "Node ROOT with parent=NONE, N=4, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 5\n",
      "Select unvisited action X:2\n",
      "Simulation using prior value 0.0781\n",
      "Node X:2 with parent=X, N=0, T=0.00 receiving score 0.08\n",
      "Node ROOT with parent=NONE, N=5, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 6\n",
      "Select unvisited action X:1\n",
      "Simulation using prior value -0.0544\n",
      "Node X:1 with parent=X, N=0, T=0.00 receiving score -0.05\n",
      "Node ROOT with parent=NONE, N=6, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 7\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:0\n",
      "Simulation using prior value 0.4082\n",
      "Node O:0 with parent=O, N=0, T=0.00 receiving score -0.41\n",
      "Node X:3 with parent=X, N=1, T=0.17 receiving score 0.40\n",
      "Node ROOT with parent=NONE, N=7, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 8\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:2\n",
      "Simulation using prior value 0.0948\n",
      "Node O:2 with parent=O, N=0, T=0.00 receiving score -0.09\n",
      "Node X:3 with parent=X, N=2, T=0.58 receiving score 0.09\n",
      "Node ROOT with parent=NONE, N=8, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 9\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:5\n",
      "Simulation using prior value 0.3361\n",
      "Node O:5 with parent=O, N=0, T=0.00 receiving score -0.34\n",
      "Node X:3 with parent=X, N=3, T=0.67 receiving score 0.33\n",
      "Node ROOT with parent=NONE, N=9, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 10\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:3\n",
      "Simulation using prior value 0.0081\n",
      "Node O:3 with parent=O, N=0, T=0.00 receiving score -0.01\n",
      "Node X:3 with parent=X, N=4, T=1.00 receiving score 0.01\n",
      "Node ROOT with parent=NONE, N=10, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 11\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:1\n",
      "Simulation using prior value 0.2998\n",
      "Node O:1 with parent=O, N=0, T=0.00 receiving score -0.30\n",
      "Node X:3 with parent=X, N=5, T=1.01 receiving score 0.30\n",
      "Node ROOT with parent=NONE, N=11, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 12\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:6\n",
      "Simulation using prior value 0.4049\n",
      "Node O:6 with parent=O, N=0, T=0.00 receiving score -0.40\n",
      "Node X:3 with parent=X, N=6, T=1.31 receiving score 0.40\n",
      "Node ROOT with parent=NONE, N=12, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 13\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:4\n",
      "Simulation using prior value -0.2433\n",
      "Node O:4 with parent=O, N=0, T=0.00 receiving score 0.24\n",
      "Node X:3 with parent=X, N=7, T=1.71 receiving score -0.24\n",
      "Node ROOT with parent=NONE, N=13, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 14\n",
      "Select known action X:5\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:0\n",
      "Simulation using prior value -0.0546\n",
      "Node O:0 with parent=O, N=0, T=0.00 receiving score 0.05\n",
      "Node X:5 with parent=X, N=1, T=0.23 receiving score -0.05\n",
      "Node ROOT with parent=NONE, N=14, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 15\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:2\n",
      "Simulation using prior value 0.3407\n",
      "Node X:2 with parent=X, N=0, T=0.00 receiving score 0.34\n",
      "Node O:3 with parent=O, N=1, T=-0.01 receiving score -0.34\n",
      "Node X:3 with parent=X, N=8, T=1.47 receiving score 0.34\n",
      "Node ROOT with parent=NONE, N=15, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 16\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:6\n",
      "Simulation using prior value 0.2553\n",
      "Node X:6 with parent=X, N=0, T=0.00 receiving score 0.26\n",
      "Node O:3 with parent=O, N=2, T=-0.35 receiving score -0.25\n",
      "Node X:3 with parent=X, N=9, T=1.80 receiving score 0.25\n",
      "Node ROOT with parent=NONE, N=16, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 17\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:4\n",
      "Simulation using prior value 0.6948\n",
      "Node X:4 with parent=X, N=0, T=0.00 receiving score 0.69\n",
      "Node O:3 with parent=O, N=3, T=-0.60 receiving score -0.69\n",
      "Node X:3 with parent=X, N=10, T=2.06 receiving score 0.69\n",
      "Node ROOT with parent=NONE, N=17, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 18\n",
      "Select known action X:3\n",
      "... iterating to next level ...\n",
      "Select known action O:3\n",
      "... iterating to next level ...\n",
      "Select unvisited action X:1\n",
      "Simulation using prior value -0.5096\n",
      "Node X:1 with parent=X, N=0, T=0.00 receiving score -0.51\n",
      "Node O:3 with parent=O, N=4, T=-1.29 receiving score 0.50\n",
      "Node X:3 with parent=X, N=11, T=2.75 receiving score -0.50\n",
      "Node ROOT with parent=NONE, N=18, T=0.00 receiving score 0.00\n",
      "\n",
      "Running MCTS step 19\n",
      "Select known action X:2\n",
      "... iterating to next level ...\n",
      "Select unvisited action O:5\n",
      "Simulation using prior value -0.2972\n",
      "Node O:5 with parent=O, N=0, T=0.00 receiving score 0.30\n",
      "Node X:2 with parent=X, N=1, T=0.08 receiving score -0.29\n",
      "Node ROOT with parent=NONE, N=19, T=0.00 receiving score 0.00\n",
      "\n",
      "Updated tree:\n",
      "> [0: ROOT] N=20, T=0.000, E=nan, Q=0.000\n",
      "     > [1: X:0] N=1, T=0.038, E=0.075, Q=0.038\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:1] N=1, T=-0.054, E=0.200, Q=-0.054\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:2] N=2, T=-0.216, E=0.083, Q=-0.108\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > [2: O:5] N=1, T=0.297, E=0.299, Q=0.297\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > None\n",
      "     > [1: X:3] N=12, T=2.241, E=0.357, Q=0.187\n",
      "          > [2: O:0] N=1, T=-0.408, E=-0.404, Q=-0.408\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:1] N=1, T=-0.300, E=-0.294, Q=-0.300\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:2] N=1, T=-0.095, E=-0.080, Q=-0.095\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:3] N=5, T=-0.781, E=0.405, Q=-0.156\n",
      "               > None\n",
      "               > [3: X:1] N=1, T=-0.510, E=-0.467, Q=-0.510\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > [3: X:2] N=1, T=0.341, E=0.429, Q=0.341\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > None\n",
      "               > [3: X:4] N=1, T=0.695, E=0.761, Q=0.695\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "               > None\n",
      "               > [3: X:6] N=1, T=0.255, E=0.419, Q=0.255\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "                    > None\n",
      "          > [2: O:4] N=1, T=0.243, E=0.256, Q=0.243\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:5] N=1, T=-0.336, E=-0.331, Q=-0.336\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > [2: O:6] N=1, T=-0.405, E=-0.400, Q=-0.405\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "     > [1: X:4] N=1, T=0.083, E=0.350, Q=0.083\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:5] N=2, T=0.175, E=0.252, Q=0.088\n",
      "          > [2: O:0] N=1, T=0.055, E=0.060, Q=0.055\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "               > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "     > [1: X:6] N=1, T=0.130, E=0.167, Q=0.130\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "          > None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##============================##\n",
    "##  Perform a few MCTS steps  ##\n",
    "##============================##\n",
    "\n",
    "##  Create game board\n",
    "game_board = GameBoard()\n",
    "print(f\"\\nInitial game board:\\n{game_board}\")\n",
    "\n",
    "##  Create a root node at the current game state\n",
    "model      = load_model(model_name)\n",
    "root_node  = Node_NeuralMCTS(game_board, params=[model, 1.], label=\"ROOT\")\n",
    "\n",
    "##  Print the initial value tree (should be a ROOT node with no children)\n",
    "print(\"Initial tree:\")\n",
    "print(root_node.tree_summary())\n",
    "print()\n",
    "\n",
    "##  Perform several MCTS steps with a HIGH debug level\n",
    "root_node.multi_step_MCTS(num_steps=20, max_sim_steps=-1, discount=0.99, debug_lvl=DebugLevel.MEDIUM)\n",
    "\n",
    "##  Print the updated value tree \n",
    "print(\"Updated tree:\")\n",
    "print(root_node.tree_summary())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af26ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bot <connect4.bot.Bot_NeuralMCTS object at 0x1440a9570>\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting uniformly random action\n",
      "Action values are:  0.008   -0.070  -0.090  0.075   0.014   0.078   0.019 \n",
      "Visit counts are:   2       12      12      168     18      55      3     \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting uniformly random action\n",
      "Action values are:  -0.019  -0.143  -0.160  -0.060  -0.184  -0.235  0.025 \n",
      "Visit counts are:   3       48      11      68      5       1       133   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | . | . | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.00 0.04 0.06 0.88 0.00 0.00 0.00\n",
      "Sampling action from noisy policy 0.04 0.07 0.08 0.70 0.04 0.04 0.04\n",
      "Action values are:  -0.396  -0.021  -0.097  0.052   -0.470  -0.743  -0.077\n",
      "Visit counts are:   1       11      16      235     1       1       1     \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.01 0.00 0.52 0.23 0.14 0.04 0.05\n",
      "Sampling action from noisy policy 0.05 0.04 0.43 0.21 0.14 0.06 0.08\n",
      "Action values are:  -0.091  -0.996  -0.122  -0.043  0.046   -0.108  -0.053\n",
      "Visit counts are:   4       1       143     63      37      10      15    \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.02 0.14 0.41 0.32 0.10 0.00 0.01\n",
      "Sampling action from noisy policy 0.05 0.14 0.34 0.27 0.11 0.04 0.04\n",
      "Action values are:  0.156   0.280   0.302   0.291   0.209   -0.621  0.157 \n",
      "Visit counts are:   5       39      114     88      29      1       3     \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.26 0.08 0.04 0.50 0.04 0.03 0.05\n",
      "Sampling action from noisy policy 0.23 0.10 0.06 0.41 0.07 0.06 0.07\n",
      "Action values are:  -0.045  -0.224  -0.197  0.043   -0.178  -0.289  -0.088\n",
      "Visit counts are:   71      22      10      137     12      7       13    \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | . | . | . | . | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.03 0.07 0.04 0.13 0.73 0.00 0.00\n",
      "Sampling action from noisy policy 0.06 0.09 0.07 0.13 0.58 0.04 0.04\n",
      "Action values are:  0.005   0.160   0.023   -0.047  0.193   -0.342  -0.364\n",
      "Visit counts are:   7       19      11      34      197     1       1     \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | . | . | . | . | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.01 0.02 0.89 0.00 0.05 0.02 0.01\n",
      "Sampling action from noisy policy 0.04 0.05 0.70 0.04 0.07 0.05 0.04\n",
      "Action values are:  -0.443  -0.180  -0.162  -0.925  -0.306  -0.399  -0.244\n",
      "Visit counts are:   2       6       236     1       13      5       3     \n",
      "Selecting action 1\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Adding noise to posterior policy 0.00 0.00 0.99 0.00 0.00 0.00 0.00\n",
      "Sampling action from noisy policy 0.04 0.04 0.78 0.04 0.04 0.04 0.04\n",
      "Action values are:  0.168   0.333   1.000   0.139   -0.229  -0.490  -0.901\n",
      "Visit counts are:   14      18      4256    15      4       3       1     \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . | . | \u001b[31mX\u001b[0m |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: X\n"
     ]
    }
   ],
   "source": [
    "##==========================================##\n",
    "##  Play a game and generate training data  ##\n",
    "##==========================================##\n",
    "\n",
    "model_inputs, posteriors, values = get_training_data_from_bot_game(\n",
    "    model, duration=1, discount=0.99, num_random_moves=2, base_policy=PolicyStrategy.NOISY_POSTERIOR_POLICY, \n",
    "    debug_lvl=DebugLevel.LOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca240c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]] ,  posterior=0.01  0.04  0.04  0.62  0.07  0.20  0.01 ,  value = 0.923\n",
      "[[ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]] ,  posterior=0.01  0.18  0.04  0.25  0.02  0.00  0.49 ,  value = -0.932\n",
      "[[ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.00  0.04  0.06  0.88  0.00  0.00  0.00 ,  value = 0.941\n",
      "[[ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]] ,  posterior=0.01  0.00  0.52  0.23  0.14  0.04  0.05 ,  value = -0.951\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]] ,  posterior=0.02  0.14  0.41  0.32  0.10  0.00  0.01 ,  value = 0.961\n",
      "[[ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]] ,  posterior=0.26  0.08  0.04  0.50  0.04  0.03  0.05 ,  value = -0.970\n",
      "[[-1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]] ,  posterior=0.03  0.07  0.04  0.13  0.73  0.00  0.00 ,  value = 0.980\n",
      "[[ 1  1  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]] ,  posterior=0.01  0.02  0.89  0.00  0.05  0.02  0.01 ,  value = -0.990\n",
      "[[-1 -1  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]] ,  posterior=0.00  0.00  0.99  0.00  0.00  0.00  0.00 ,  value = 1.000\n"
     ]
    }
   ],
   "source": [
    "##====================================================##\n",
    "##  Check the data generated by the game is sensible  ##\n",
    "##====================================================##\n",
    "\n",
    "for inp, pos, val in zip(model_inputs, posteriors, values) :\n",
    "    print(inp[:,:,0], \",  posterior=\"+\"  \".join([f\"{x:.2f}\" for x in pos]), f\",  value = {val[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dcbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting greedy action from posterior policy 0.01 0.03 0.02 0.57 0.04 0.30 0.01\n",
      "Action values are:  -0.018  -0.082  -0.188  0.036   -0.072  0.025   -0.033\n",
      "Visit counts are:   11      30      19      504     36      267     11    \n",
      "Selecting action 3\n",
      "Prior policy was :  0.02  0.11  0.13  0.50  0.12  0.11  0.02\n",
      "Prior values were:  0.04  -0.05  0.08  0.17  0.08  0.23  0.13\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.00 0.00 0.99 0.00 0.00 0.00\n",
      "Action values are:  -0.408  -0.300  -0.113  0.030   -0.132  -0.336  -0.405\n",
      "Visit counts are:   1       1       3       860     2       1       1     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.00  0.00  0.01  0.97  0.01  0.00  0.00\n",
      "Prior values were:  -0.41  -0.30  -0.09  -0.01  0.24  -0.34  -0.40\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.01 0.02 0.27 0.54 0.10 0.00 0.06\n",
      "Action values are:  -0.135  -0.055  0.018   -0.008  0.008   -0.489  -0.011\n",
      "Visit counts are:   11      14      239     478     84      2       51    \n",
      "Selecting action 3\n",
      "Prior policy was :  0.06  0.04  0.08  0.59  0.06  0.03  0.15\n",
      "Prior values were:  -0.52  -0.51  0.34  0.12  0.69  -0.27  0.26\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.00 0.00 0.99 0.00 0.00 0.00\n",
      "Action values are:  -0.968  -0.939  -0.953  0.023   -0.175  -0.697  -0.702\n",
      "Visit counts are:   1       1       1       865     1       1       1     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.00  0.00  0.00  0.98  0.01  0.00  0.00\n",
      "Prior values were:  -0.97  -0.94  -0.95  0.09  -0.18  -0.70  -0.70\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.00 0.02 0.96 0.00 0.02 0.00\n",
      "Action values are:  -0.621  -0.725  -0.005  -0.015  -0.625  -0.057  -0.404\n",
      "Visit counts are:   1       1       20      844     1       15      1     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.00  0.03  0.01  0.90  0.03  0.03  0.00\n",
      "Prior values were:  -0.62  -0.73  0.22  0.16  -0.62  -0.03  -0.40\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.01 0.91 0.01 0.01 0.02 0.01 0.02\n",
      "Action values are:  -0.256  0.001   -0.188  -0.499  -0.251  -0.516  -0.122\n",
      "Visit counts are:   11      812     12      7       16      13      17    \n",
      "Selecting action 1\n",
      "Prior policy was :  0.10  0.25  0.08  0.14  0.15  0.23  0.05\n",
      "Prior values were:  0.04  -0.27  -0.31  -0.67  -0.29  -0.26  0.71\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.47 0.35 0.00 0.06 0.03 0.05 0.03\n",
      "Action values are:  0.005   -0.069  -0.443  -0.069  -0.085  -0.170  -0.053\n",
      "Visit counts are:   422     312     2       53      26      44      30    \n",
      "Selecting action 0\n",
      "Prior policy was :  0.03  0.55  0.04  0.08  0.03  0.24  0.02\n",
      "Prior values were:  0.01  0.73  -0.53  -0.17  -0.39  -0.69  0.17\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.88 0.03 0.00 0.04 0.05 0.00\n",
      "Action values are:  -0.204  0.076   -0.092  -0.655  -0.166  -0.104  -0.612\n",
      "Visit counts are:   4       781     23      1       36      40      1     \n",
      "Selecting action 1\n",
      "Prior policy was :  0.04  0.21  0.13  0.03  0.31  0.25  0.03\n",
      "Prior values were:  -0.06  0.10  -0.08  -0.65  -0.46  -0.85  -0.61\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.90 0.02 0.02 0.02 0.01 0.01 0.02\n",
      "Action values are:  -0.008  -0.451  -0.364  -0.132  -0.130  -0.239  -0.075\n",
      "Visit counts are:   789     18      15      16      13      13      16    \n",
      "Selecting action 0\n",
      "Prior policy was :  0.31  0.27  0.17  0.06  0.06  0.09  0.02\n",
      "Prior values were:  0.12  -0.73  -0.74  -0.35  -0.45  -0.14  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.14 0.74 0.05 0.00 0.05 0.02 0.01\n",
      "Action values are:  0.031   0.071   -0.016  -0.283  -0.070  -0.274  -0.148\n",
      "Visit counts are:   120     653     46      3       44      15      7     \n",
      "Selecting action 1\n",
      "Prior policy was :  0.12  0.28  0.15  0.03  0.20  0.17  0.04\n",
      "Prior values were:  0.80  0.01  -0.19  -0.39  -0.15  -0.43  -0.42\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.01 0.98 0.00 0.00 0.00 0.00 0.00\n",
      "Action values are:  -0.154  0.074   -0.697  -0.515  -0.274  -0.348  -0.226\n",
      "Visit counts are:   9       883     1       1       3       2       1     \n",
      "Selecting action 1\n",
      "Prior policy was :  0.02  0.89  0.04  0.01  0.02  0.01  0.01\n",
      "Prior values were:  0.56  0.01  -0.70  -0.52  -0.03  -0.02  -0.23\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting greedy action from posterior policy 0.01 0.97 0.00 0.00 0.01 0.01 0.00\n",
      "Action values are:  -0.147  0.023   -0.561  -0.384  -0.277  -0.261  -0.182\n",
      "Visit counts are:   5       854     1       1       12      5       1     \n",
      "Selecting action 1\n",
      "Prior policy was :  0.03  0.75  0.02  0.02  0.13  0.05  0.01\n",
      "Prior values were:  0.29  0.11  -0.56  -0.38  -0.25  -0.41  -0.18\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.05 0.86 0.01 0.04 0.01 0.01 0.01\n",
      "Action values are:  -0.082  0.028   -0.715  -0.110  -0.387  -0.335  -0.300\n",
      "Visit counts are:   45      771     8       37      13      10      9     \n",
      "Selecting action 1\n",
      "Prior policy was :  0.12  0.13  0.22  0.11  0.18  0.12  0.11\n",
      "Prior values were:  0.01  0.01  -0.91  0.29  -0.82  0.68  -0.74\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | . | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.09 0.00 0.10 0.72 0.06 0.02 0.00\n",
      "Action values are:  -0.024  -0.018  0.025   -0.170  -0.117  -0.376\n",
      "Visit counts are:   83      88      642     55      21      3     \n",
      "Selecting action 3\n",
      "Prior policy was :  0.18  0.00  0.15  0.15  0.38  0.11  0.04\n",
      "Prior values were:  0.13  -0.42  -0.04  0.01  0.34  -0.55\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.02 0.00 0.00 0.00 0.84 0.03 0.11\n",
      "Action values are:  -0.153  -0.319  -0.007  -0.175  -0.092\n",
      "Visit counts are:   14      2       750     28      103   \n",
      "Selecting action 4\n",
      "Prior policy was :  0.06  0.01  0.03  0.01  0.39  0.16  0.34\n",
      "Prior values were:  0.86  0.01  0.01  0.01  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.13 0.00 0.12 0.00 0.13 0.62 0.00\n",
      "Action values are:  -0.007  -0.016  -0.055  0.041   -0.617\n",
      "Visit counts are:   115     108     116     555     4     \n",
      "Selecting action 5\n",
      "Prior policy was :  0.13  0.01  0.16  0.00  0.38  0.23  0.09\n",
      "Prior values were:  0.67  0.01  0.70  0.70  0.24\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.08 0.00 0.00 0.00 0.53 0.38 0.01\n",
      "Action values are:  -0.135  -0.661  -0.082  -0.111  -0.601\n",
      "Visit counts are:   73      3       475     337     7     \n",
      "Selecting action 4\n",
      "Prior policy was :  0.15  0.00  0.05  0.00  0.21  0.46  0.13\n",
      "Prior values were:  0.33  -0.11  0.50  0.01  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.02 0.00 0.02 0.00 0.93 0.03 0.00\n",
      "Action values are:  -0.173  -0.208  0.142   -0.046  -0.367\n",
      "Visit counts are:   17      17      823     30      2     \n",
      "Selecting action 4\n",
      "Prior policy was :  0.18  0.01  0.22  0.00  0.40  0.15  0.05\n",
      "Prior values were:  0.01  0.01  0.01  0.15  0.16\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.23 0.00 0.01 0.00 0.02 0.71 0.03\n",
      "Action values are:  -0.132  -0.392  -0.498  -0.136  -0.222\n",
      "Visit counts are:   205     10      15      636     30    \n",
      "Selecting action 5\n",
      "Prior policy was :  0.11  0.01  0.10  0.01  0.20  0.47  0.11\n",
      "Prior values were:  0.72  0.13  0.01  0.30  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.03 0.00 0.00 0.00 0.02 0.95 0.00\n",
      "Action values are:  -0.003  -0.386  -0.267  0.146   -0.448\n",
      "Visit counts are:   25      3       19      877     3     \n",
      "Selecting action 5\n",
      "Prior policy was :  0.12  0.02  0.07  0.00  0.26  0.47  0.06\n",
      "Prior values were:  0.16  0.01  0.01  0.01  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.00 0.00 0.00 0.00 0.02 0.95 0.03\n",
      "Action values are:  -0.323  -0.641  -0.251  -0.153  -0.189\n",
      "Visit counts are:   3       1       20      885     24    \n",
      "Selecting action 5\n",
      "Prior policy was :  0.02  0.00  0.01  0.00  0.08  0.85  0.03\n",
      "Prior values were:  0.01  -0.64  0.01  0.01  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.03 0.00 0.01 0.00 0.04 0.92 0.00\n",
      "Action values are:  0.061   -0.248  -0.054  0.165   -0.371\n",
      "Visit counts are:   30      6       37      860     4     \n",
      "Selecting action 5\n",
      "Prior policy was :  0.11  0.01  0.10  0.00  0.29  0.42  0.08\n",
      "Prior values were:  0.01  0.01  0.01  0.01  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting greedy action from posterior policy 0.10 0.00 0.02 0.00 0.79 0.04 0.04\n",
      "Action values are:  -0.215  -0.325  -0.193  -0.251  -0.240\n",
      "Visit counts are:   95      23      752     38      40    \n",
      "Selecting action 4\n",
      "Prior policy was :  0.13  0.00  0.12  0.02  0.54  0.10  0.09\n",
      "Prior values were:  0.53  -0.68  0.01  0.01  -0.18\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.13 0.00 0.03 0.00 0.57 0.25 0.00\n",
      "Action values are:  0.147   -0.015  0.164   0.341   -0.338\n",
      "Visit counts are:   130     32      552     245     4     \n",
      "Selecting action 4\n",
      "Prior policy was :  0.17  0.01  0.23  0.01  0.39  0.10  0.09\n",
      "Prior values were:  0.01  0.01  0.30  0.01  0.18\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.05 0.00 0.01 0.00 0.92 0.01 0.01\n",
      "Action values are:  -0.301  -0.503  -0.160  -0.613  -0.556\n",
      "Visit counts are:   47      14      917     5       13    \n",
      "Selecting action 4\n",
      "Prior policy was :  0.23  0.00  0.15  0.02  0.35  0.07  0.17\n",
      "Prior values were:  0.66  0.23  0.01  0.01  -0.10\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.18 0.00 0.34 0.00 0.00 0.26 0.22\n",
      "Action values are:  0.121   0.183   0.171   0.155 \n",
      "Visit counts are:   184     351     269     225   \n",
      "Selecting action 2\n",
      "Prior policy was :  0.41  0.00  0.21  0.00  0.01  0.11  0.25\n",
      "Prior values were:  0.01  0.01  0.46  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 0.89 0.00 0.02 0.00 0.00 0.02 0.08\n",
      "Action values are:  -0.072  -0.819  -0.183  -0.159\n",
      "Visit counts are:   1293    26      25      113   \n",
      "Selecting action 0\n",
      "Prior policy was :  0.32  0.00  0.51  0.00  0.01  0.03  0.12\n",
      "Prior values were:  0.01  0.01  0.01  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from posterior policy 1.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "Action values are:  0.064   -0.488  -0.168  -0.330\n",
      "Visit counts are:   1749    2       3       3     \n",
      "Selecting action 0\n",
      "Prior policy was :  0.95  0.00  0.02  0.00  0.00  0.01  0.02\n",
      "Prior values were:  0.02  0.01  0.50  0.01\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    }
   ],
   "source": [
    "##===================================================================================================##\n",
    "##  Use MCTS to search for an optimal action, and compare the prior policy/value with the posterior  ##\n",
    "##===================================================================================================##\n",
    "\n",
    "game_board = GameBoard()\n",
    "bot = Bot_NeuralMCTS(model, policy_strategy=PolicyStrategy.GREEDY_POSTERIOR_POLICY)\n",
    "\n",
    "while not game_board.get_result() :\n",
    "    player = game_board.to_play\n",
    "    action = bot.choose_action(game_board, duration=5, discount=0.99, debug_lvl=DebugLevel.LOW)\n",
    "    print(\"Prior policy was :  \" + \"  \".join([f\"{c:.2f}\" for c in bot.root_node.child_priors]))\n",
    "    print(\"Prior values were:  \" + \"  \".join([f\"{player.value*c.prior_value:.2f}\" for c in bot.root_node.children]))\n",
    "    game_board.apply_action(action)\n",
    "    print(game_board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b677b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
