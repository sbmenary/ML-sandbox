{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69d1a5f",
   "metadata": {},
   "source": [
    "#  Play bot (neural MCTS)\n",
    "\n",
    "---\n",
    "\n",
    "Author: S. Menary [sbmenary@gmail.com]\n",
    "\n",
    "Date  : 2023-01-18, last edit 2023-01-19\n",
    "\n",
    "Brief : Play a game of Connect 4 against bot that uses MCTS with a learned policy/value function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54ef68",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "- Import key Python and PyPI packages and print their versions for reproducibility.\n",
    "- Import the required Connect 4 game, bot and utility objects from our framework\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff93cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=====================================##\n",
    "##  All imports should be placed here  ##\n",
    "##=====================================##\n",
    "\n",
    "##  Python core libs\n",
    "import sys\n",
    "\n",
    "##  PyPI libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "##  Local packages\n",
    "from connect4.utils  import DebugLevel\n",
    "from connect4.game   import GameBoard\n",
    "from connect4.MCTS   import PolicyStrategy\n",
    "from connect4.bot    import Bot_NeuralMCTS\n",
    "from connect4.neural import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e7c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Python version is 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "     Numpy version is 1.23.2\n",
      "Tensorflow version is 2.11.0\n"
     ]
    }
   ],
   "source": [
    "##======================================##\n",
    "##  Print versions for reproducibility  ##\n",
    "##======================================##\n",
    "\n",
    "print(f\"    Python version is {sys.version}\")\n",
    "print(f\"     Numpy version is {np.__version__}\")\n",
    "print(f\"Tensorflow version is {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752564a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Play a game vs bot\n",
    "\n",
    "Play a game of connect 4 against our bot!\n",
    "\n",
    "The `GameBoard` object is used as a Connect 4 game environment. It may be updated using a command like `game_board.apply_action(column_index)`. We can create a simple ASCII representation of our game board using `print(game_board)`. See `help(GameBoard)` for more useful manipulation methods.\n",
    "\n",
    "The `Bot_NeuralMCTS` object is used to apply bot actions using neural MCTS. Call `bot.take_move(game_board, duration)` to run MCTS for `duration` seconds and then play a bot move. Turning up the `duration` parameter will improve the bot by allowing it to search for longer.\n",
    "\n",
    "The option `PolicyStrategy.GREEDY_POSTERIOR_POLICY` commands the bot to choose the action with the highest posterior policy estimated using MCTS. To act greedily over the action values instead, use `PolicyStrategy.GREEDY_POSTERIOR_VALUES`. For a stochastic approach, use `PolicyStrategy.SAMPLE_POSTERIOR_POLICY`.\n",
    "\n",
    "You may also skip the MCTS entirely by using `duration=0` and selecting actions using the _prior_ evaluations. In particular:\n",
    "- `PolicyStrategy.GREEDY_PRIOR_POLICY` means we select the action that maximises the policy evaluated using a single neural network pass.\n",
    "- `PolicyStrategy.GREEDY_PRIOR_VALUES` means we select the action that maximises the value evaluated using a single neural network pass on each of the child nodes.\n",
    "- `PolicyStrategy.SAMPLE_PRIOR_POLICY` means we sample from the policy evaluated using a single neural network pass.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f6f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=========================================##\n",
    "##  Load our neural policy/value function  ##\n",
    "##=========================================##\n",
    "\n",
    "##  Configure which model to load\n",
    "model_name = \"../models/test_run/neural_model_v7.h5\"\n",
    "\n",
    "##  Load this model\n",
    "model = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016132ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n"
     ]
    }
   ],
   "source": [
    "##=====================##\n",
    "##  Create a new game  ##\n",
    "##=====================##\n",
    "\n",
    "game_board = GameBoard()\n",
    "bot        = Bot_NeuralMCTS(model, policy_strategy=PolicyStrategy.GREEDY_PRIOR_POLICY)\n",
    "print(game_board)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27992527",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now play moves until the game is complete!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4802ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: X\n"
     ]
    }
   ],
   "source": [
    "##===============##\n",
    "##  Take moves!  ##\n",
    "##===============##\n",
    "\n",
    "##  Select our move\n",
    "column_idx = 1\n",
    "\n",
    "##  Apply our move as Player X\n",
    "game_board.apply_action(column_idx)\n",
    "print(game_board)\n",
    "\n",
    "##  Apply the responding bot move move as Player O\n",
    "if not game_board.get_result() :\n",
    "    bot.take_move(game_board, duration=0, debug_lvl=DebugLevel.LOW)\n",
    "    print(game_board)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150eb04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bot-vs-bot game\n",
    "\n",
    "Let's watch the bot play itself!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8a0f122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.02 0.02 0.02 0.86 0.02 0.02 0.02\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.02 0.02 0.37 0.20 0.33 0.02 0.02\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.02 0.02 0.39 0.44 0.03 0.06 0.04\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.05 0.04 0.25 0.44 0.05 0.14 0.03\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.05 0.32 0.16 0.08 0.15 0.16\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.07 0.04 0.29 0.24 0.22 0.10 0.04\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.10 0.08 0.10 0.21 0.17 0.18 0.16\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.07 0.06 0.33 0.08 0.28 0.12 0.06\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.08 0.20 0.09 0.25 0.17 0.14\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.06 0.08 0.05 0.54 0.16 0.04\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.04 0.06 0.07 0.11 0.49 0.15 0.08\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.04 0.04 0.07 0.04 0.73 0.06 0.03\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.10 0.35 0.10 0.04 0.19 0.13\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 2\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | . | . | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.09 0.11 0.02 0.09 0.38 0.25 0.06\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.15 0.04 0.14 0.13 0.37 0.10\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.10 0.13 0.03 0.10 0.10 0.44 0.09\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.14 0.04 0.17 0.10 0.34 0.14\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.14 0.09 0.04 0.12 0.13 0.42 0.07\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | . | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.07 0.11 0.04 0.27 0.12 0.25 0.13\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 3\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | . | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.22 0.17 0.06 0.06 0.16 0.24 0.10\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | . | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.08 0.13 0.05 0.11 0.15 0.32 0.15\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 5\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.24 0.22 0.13 0.08 0.17 0.00 0.16\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 0\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: NONE\n",
      "Selecting greedy action from prior policy\n",
      "Prior policy is 0.09 0.15 0.10 0.11 0.23 0.00 0.33\n",
      "Action values are:  N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Visit counts are:   N/A     N/A     N/A     N/A     N/A     N/A   \n",
      "Selecting action 6\n",
      "+---+---+---+---+---+---+---+\n",
      "| . | . | . | . | . | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| . | . | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | . |\n",
      "| . | . | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[34mO\u001b[0m | \u001b[34mO\u001b[0m | . |\n",
      "| \u001b[34mO\u001b[0m | . | \u001b[34mO\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m | \u001b[31mX\u001b[0m |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n",
      "+---+---+---+---+---+---+---+\n",
      "Game result is: X\n"
     ]
    }
   ],
   "source": [
    "##========================##\n",
    "##  Play bot-vs-bot game  ##\n",
    "##========================##\n",
    "\n",
    "\n",
    "##  Set up and display game\n",
    "game_board = GameBoard()\n",
    "result     = game_board.get_result()\n",
    "print(game_board)\n",
    "\n",
    "##  Keep playing moves until the game concludes\n",
    "while not result :\n",
    "    bot.take_move(game_board, duration=0, debug_lvl=DebugLevel.LOW)\n",
    "    result = game_board.get_result()\n",
    "    print(game_board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeaa64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
