{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc444d26",
   "metadata": {},
   "source": [
    "#  1.  Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994a4274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ste/miniforge3/envs/tf-sandbox-py3p9/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Required imports\n",
    "###\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7523a7",
   "metadata": {},
   "source": [
    "# 2. Download and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d6728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Download a dataset\n",
    "###\n",
    "\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                               with_info=True,\n",
    "                               as_supervised=True)\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e468a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'examples' is type <class 'dict'>\n",
      "\n",
      "Found key 'train' of type <class 'str'>\n",
      "Found item '<PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>' of type <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "Item has length 51785\n",
      "\n",
      "Found key 'validation' of type <class 'str'>\n",
      "Found item '<PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>' of type <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "Item has length 1193\n",
      "\n",
      "Found key 'test' of type <class 'str'>\n",
      "Found item '<PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>' of type <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "Item has length 1803\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Explore the 'examples' object\n",
    "###\n",
    "\n",
    "print(f\"'examples' is type {type(examples)}\")\n",
    "for key, item in examples.items() :\n",
    "    print()\n",
    "    print(f\"Found key '{key}' of type {type(key)}\")\n",
    "    print(f\"Found item '{item}' of type {type(item)}\")\n",
    "    print(f\"Item has length {len(item)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b264945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create first_example by calling train_examples.take(n=1). Creates a new dataset with slice of n datapoints.\n",
      "> type is <class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>\n",
      "> length is 1\n",
      "\n",
      "This is still a dataset-type object. We still have to iterate to pick out individual datapoints.\n",
      "\n",
      "Each iter item has type <class 'tuple'> and length 2. These are Tensor objects.\n",
      "Calling Tensor.numpy() let's us see their values\n",
      "1. dp[0] = b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'\n",
      "2. dp[1] = b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
      "\n",
      "I guess these are the labelled datapoints, and we will see how the pipeline uses them. They are currently stored as Python bytestring objects. To read then better, we can call the .decode('utf-8') method and get:\n",
      "1. e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "2. and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 12:08:41.656251: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-09 12:08:41.681402: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-12-09 12:08:41.695093: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###  Inspect a datapoint\n",
    "###\n",
    "\n",
    "print(\"Create first_example by calling train_examples.take(n=1). Creates a new dataset with slice of n datapoints.\")\n",
    "first_example = train_examples.take(1)\n",
    "print(f\"> type is {type(first_example)}\")\n",
    "print(f\"> length is {len(first_example)}\\n\")\n",
    "\n",
    "print(\"This is still a dataset-type object. We still have to iterate to pick out individual datapoints.\\n\")\n",
    "\n",
    "for dp in first_example :\n",
    "    print(f\"Each iter item has type {type(dp)} and length {len(dp)}. These are Tensor objects.\")\n",
    "    print(f\"Calling Tensor.numpy() let's us see their values\")\n",
    "    print(f\"1. dp[0] = {dp[0].numpy()}\")\n",
    "    print(f\"2. dp[1] = {dp[1].numpy()}\")\n",
    "    \n",
    "print(\"\\nI guess these are the labelled datapoints, and we will see how the pipeline uses them. They are currently \\\n",
    "stored as Python bytestring objects. To read then better, we can call the .decode('utf-8') method and get:\")\n",
    "\n",
    "for dp in first_example :\n",
    "    print(f\"1. {dp[0].numpy().decode('utf-8')}\")\n",
    "    print(f\"2. {dp[1].numpy().decode('utf-8')}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d53730f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples.batch(3).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f486f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Examples in Portuguese:\n",
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "> Examples in English:\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 12:08:41.718634: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "###\n",
    "\n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    print('> Examples in Portuguese:')\n",
    "    for pt in pt_examples.numpy():\n",
    "        print(pt.decode('utf-8'))\n",
    "    print()\n",
    "\n",
    "    print('> Examples in English:')\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978dae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf8254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c9b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
