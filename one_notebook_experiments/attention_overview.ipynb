{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b5a3be",
   "metadata": {},
   "source": [
    "# Attention: overview and exploration\n",
    "\n",
    "Brief: implement and explore attention layers to gain understanding and intuition\n",
    "\n",
    "Author: Stephen Menary, sbmenary@gmail.com\n",
    "\n",
    "Date: 13/12/2022\n",
    "\n",
    "Last edit: 21/12/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fe5e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intro\n",
    "\n",
    "An attention layer takes $N$ feature vectors as input and creates $N$ feature vectors as output. For each token, we look at the features of a reference sequence. Using a simple linear transformation of the token and reference token features, we calculate a set of _attention weights_ that determine how much information the token should be pulling from the reference. The attention weights are normalised to unity using a `softmax` operation to maintain a consistent scale.\n",
    "\n",
    "Self-attention occurs when the query and reference are the same sequence. Our model effectively allows the tokens to pass information to one another with strengths determined by the attention matrix. This makes the self-attention layer a type of graph network with a continuous adjacency matrix defined by the attention weights. The strength of information diffusion is small when the attention mechanism deems the tokens to be independent. We could view other attention layers as graph networks as well, where the graph contains both the query and reference sequences and the adjacency matrix contains many $0$s so that information may only propagate in certain directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e8223",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attention\n",
    "\n",
    "Consider the transformation $x^{i,l} \\rightarrow x^{i,l+1}$, where $x^{i,l}$ is the feature vector for token $i$ in layer $l$. We \"attend\" to a _set of_ vectors $\\{y^j \\forall j\\}$ (the feature vectors of the reference sequence). We do this by performing a linear transformation $Q^{i}_\\alpha = \\sum_\\beta A_{\\alpha\\beta} x^{i,l}_\\beta$, and a separate linear transformation $K^{j}_\\alpha = \\sum_\\beta B_{\\alpha\\beta} y^{j}_\\beta$ for every $j$. The strength of the connection is calculated as the dot product $s_{i,j} = \\sum_\\alpha Q^{i}_\\alpha K^{j}_\\alpha= \\sum_{\\alpha,\\beta,\\gamma} y^{j}_\\gamma B^T_{\\gamma\\alpha} A_{\\alpha\\beta} x^{i,l}_\\beta$.\n",
    "\n",
    "The attention weights are obtained by enforcing positive definiteness and normalisation to unity using a `Softmax` function, $w_{ij} ~=~ \\mathrm{Softmax}_j\\left(s_{ij}\\right)$ which allows us to interpret $w_{ij}$ as a probability distribution over $j$. At this stage, all we have done is calculate a matrix connecting element $i$ of the query sequence with element $j$ of the reference matrix. This is turned into an actual propagation of node features by (i) transforming the reference sequence into a new sequence of feature vectors called _values_, then (ii) aggregating these value vectors according to the attention weights. \n",
    "\n",
    "Using the same method as for keys, the value vectors are calculatd as $V^{j}_\\alpha = \\sum_\\beta C_{\\alpha\\beta} y^{j}_\\beta$. The updated features are then calculated as $x_\\mu^{i,l+1} = \\sum_{j} w_{ij} V^{j}_\\mu$. Compacting the indices over feature vectors, we may summarise the entire layer as\n",
    "\\begin{equation}\n",
    "    \\mathrm{Attention}\\left(Q,K,V\\right) ~=~ \\mathrm{Softmax}\\left(QK^T\\right)~V\n",
    "\\end{equation}\n",
    "Why do we want to write the attention layer in this way? The answer is that we may construct $Q$, $K$ and $V$ in whatever way we want. Here we used simple `matmul` transformations using the matrices $A$, $B$ and $C$. But we didn't have to! For example, we can use dense layers that are the same but with additive bias terms, or CNN layers, and we may share layers when constructing $Q$, $K$ and $V$.\n",
    "\n",
    "Actually, we have one more detail to include. We have implemented \"dot-product attention\", because the attention weights $w_{ij}$ are calculated as the dot-product of the transformed feature vectors of length $d_k$. Other forms of attention mechanism are possible, e.g. additive attention. Now, if $d_k$ is very large, then the function $w_{ij}\\sim\\exp\\left[s_{ij}\\right]=\\exp\\left[\\sum_\\alpha Q^{i}_\\alpha K^{j}_\\alpha\\right]$. If we model the intial query-key pairs as normally distributed with mean $0$ and variance $1$, then the exponent is a sum of $d_k$ normal distributions, which itself is a normal distribution with variance $d_k$. We now have $w_{ij} \\sim \\exp\\left[\\sim \\mathcal{N}(0;d_k)\\right]$. As $d_k$ gets larger, the variance gets larger, and so we begin to see large values in the exponent. However, the _gradient_ of the `Softmax` function becomes very small for large exponents. This makes learning very slow. To counteract this effect, the AIAYN paper scales the dot-product by $1/\\sqrt{d_k}$ so the scale of arguments is roughly independent of $d_k$:\n",
    "\\begin{equation}\n",
    "    \\mathrm{Attention}\\left(Q,K,V\\right) ~=~ \\mathrm{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)~V\n",
    "\\end{equation}\n",
    "Of course, the $Q$ and $K$ matrices should be initialised such that they have reasonable values themselves.\n",
    "\n",
    "The other configurable length is the size of the output feature vectors, $d_v$. In principle this can be any length, but we often use the same as the input so they may be linearly combined. This means that all feature vectors will be the same length throughout the model, and we can call this $d_\\mathrm{model}$.\n",
    "\n",
    "Let us now try to implement a custom attention layer. Of course, Keras has it's own attention layers, but for education purposes we will implement our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a44742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "Matplotlib version is 3.6.2\n",
      "Numpy version is 1.23.2\n",
      "Tensorflow version is 2.11.0\n"
     ]
    }
   ],
   "source": [
    "#============================================\n",
    "#===  Import packages and print versions  ===\n",
    "#============================================\n",
    "\n",
    "##  Import Python packages\n",
    "import sys\n",
    "\n",
    "##  Import additional packages\n",
    "import matplotlib as mpl\n",
    "import numpy      as np\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "\n",
    "##  Import objects from packages\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Add, Concatenate, Dense, Dropout, Embedding, Input, Layer, LayerNormalization, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "##  Print versions for reproducibility\n",
    "print(f\"Python version is {sys.version}\".replace(\"\\n\", \" | \"))\n",
    "print(f\"Matplotlib version is {mpl.__version__}\")\n",
    "print(f\"Numpy version is {np.__version__}\")\n",
    "print(f\"Tensorflow version is {tf.__version__}\")\n",
    "#print(f\"Tensorflow-datasets version is {tfds.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bdf568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "#===  Create keras layer for dot-product attention  ===\n",
    "#======================================================\n",
    "\n",
    "class LinearDotProductAttention(Layer) :\n",
    "    def __init__(self, dk, dv, same_transform_QK=False, same_transform_KV=False, use_causal_mask=False):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.dk      = dk\n",
    "        self.dv      = dv\n",
    "        self.sqrt_dk = np.sqrt(dk)\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "        \n",
    "        ##  Create Softmax layer, which can handle masking whereas tf.nn.softmax cannot\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "        ##  Create Dense transform of query feature vectors\n",
    "        self.query_transform = Dense(dk)\n",
    "        \n",
    "        ##  Create Dense transform of key feature vectors\n",
    "        if same_transform_QK : self.key_transform = self.query_transform\n",
    "        else                 : self.key_transform = Dense(dk)\n",
    "            \n",
    "        ##  Create Dense transform of value feature vectors\n",
    "        if same_transform_KV : \n",
    "            if dk != dk :\n",
    "                raise ValueError(f\"Can only use same transform for keys and values if dk (={dk}) \\\n",
    "                                   is equal to dv (={dv})\")\n",
    "            self.value_transform = self.key_transform\n",
    "        else : \n",
    "            self.value_transform = Dense(dv)\n",
    "                    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Resolve inputs\n",
    "        if len(inputs) != 2 : raise ValueError(f\"inputs of length {len(inputs)}, expected 2\")\n",
    "        query_features = inputs[0]  # shape [batch_size, query_sequence_length, query_feature_length]\n",
    "        ref_features   = inputs[1]  # shape [batch_size, ref_sequence_length  , ref_feature_length  ]\n",
    "                \n",
    "        ##  Calculate Q, K, V matrices\n",
    "        Q = self.query_transform(query_features, training=training)  # shape [batch_size, query_sequence_length, dk]\n",
    "        K = self.key_transform  (ref_features  , training=training)  # shape [batch_size, ref_sequence_length  , dk]\n",
    "        V = self.value_transform(ref_features  , training=training)  # shape [batch_size, ref_sequence_length  , dv]\n",
    "                \n",
    "        ##  Calculate transpose of K, without modifying the first axis which indexes batch samples\n",
    "        K_T = tf.transpose(K, perm=[0,2,1])  # shape [batch_size, dk, ref_sequence_length]\n",
    "                \n",
    "        ##  Calculate dot-product attention scores\n",
    "        x = tf.matmul(Q, K_T)             # shape [batch_size, query_sequence_length, ref_sequence_length]\n",
    "        x = x / self.sqrt_dk              # shape [batch_size, query_sequence_length, ref_sequence_length]\n",
    "        \n",
    "        ##  Create a causal mask on-the-fly if needed\n",
    "        mask = None\n",
    "        if self.use_causal_mask :\n",
    "            mask_shape = tf.shape(x)\n",
    "            mask = self._create_causal_mask(mask_shape)\n",
    "                \n",
    "        ##  Calculate attention weights\n",
    "        x = self.softmax(x, mask=mask, training=training)   # shape [batch_size, query_sequence_length, ref_sequence_length]\n",
    "                        \n",
    "        ##  Attend to reference sequence and return updated feature vector of length dv\n",
    "        x = tf.matmul(x, V)              # shape [batch_size, query_sequence_length, dv] \n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def _create_causal_mask(self, mask_shape) :\n",
    "        \"\"\"\n",
    "        Method docstring\n",
    "        - using trick for creating causal mask from from keras base_dense_attention class method\n",
    "        - https://github.com/keras-team/keras/blob/e6784e4302c7b8cd116b74a784f4b78d60e83c26/keras/layers/attention/base_dense_attention.py\n",
    "        Mask is 1 for elements that we want to include, 0 for elements we want to exclude\n",
    "        Axis -1 is the \"reference sequence\" index\n",
    "        Axis -2 is the \"query sequence\" index\n",
    "        We want mask to be 1 only when Axis -1 <= Axis -2, the same as Axis -2 >= Axis -1\n",
    "        With indices [row, col], we have 1 when row >= col, giving a lower triangular matrix\n",
    "        \"\"\"\n",
    "        ones_like_x = tf.ones(shape=mask_shape, dtype=tf.int32)\n",
    "        query_index = tf.cumsum(ones_like_x, axis=-2)\n",
    "        ref_index   = tf.cumsum(ones_like_x, axis=-1)\n",
    "        return tf.greater_equal(query_index, ref_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ab9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================\n",
    "#===  Create method for building simple model that performs one attention operation  ===\n",
    "#=======================================================================================\n",
    "\n",
    "def build_attention_model(query_length, ref_length=None, dk=10, dv=None, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\"\n",
    "    ##  If ref_length or dv not provided then set equal to query_length\n",
    "    if not ref_length : ref_length = query_length\n",
    "    if not dv         : dv         = query_length\n",
    "        \n",
    "    ##  Create model inputs\n",
    "    query_sequence = Input((None, query_length))\n",
    "    ref_sequence   = Input((None, ref_length  ))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = LinearDotProductAttention(dk, dv)([query_sequence, ref_sequence])\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model([query_sequence, ref_sequence], x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6692db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"attention_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 512)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 512)]  0           []                               \n",
      "                                                                                                  \n",
      " linear_dot_product_attention (  (None, None, 16)    24624       ['input_1[0][0]',                \n",
      " LinearDotProductAttention)                                       'input_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,624\n",
      "Trainable params: 24,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_in    = 512\n",
    "d_model = 128\n",
    "dk      = d_model / 8\n",
    "dv      = d_model / 8\n",
    "\n",
    "model = build_attention_model(d_in, d_in, dk=dk, dv=dv, name=\"attention_model\")\n",
    "\n",
    "model.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dde0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "Multi-head attention essentially means that we run several attention operations in parallel. Each head calculates a simple representation for every token, and combines them into a pair-wise attention weight. This weight only knows the context of those two tokens and nothing else. It then uses the attention weights to average the values assigned to each token. This averaging step limits how many different tokens may be attended to, and the simnple representation limits how much information may be contained.\n",
    "\n",
    "By performing multiple attention steps in parallel, each token may attend to many different locations, using many different representations. Furthermore, by combining the results using a dense layer (rather than simple averaging), we may attend to multi-token combinations.\n",
    "\n",
    "In summary: multi-head attention allows us to attend to many different locations using different feature transformations, allowing for a more expressive propagation of information.\n",
    "\n",
    "The recipe is:\n",
    "- perform $h$ attention steps in parallel\n",
    "- concatenate the results into a $h \\cdot d_v$-dimensional feature vector\n",
    "- use a Dense layer to project down to a $d_v$-dimensional feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67641b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================\n",
    "#===  Create keras layer for multi-head attention  ===\n",
    "#=====================================================\n",
    "\n",
    "class MultiHeadAttention(Layer) :\n",
    "    \"\"\"\n",
    "    Class docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, d_out, dk_per_head, dv_per_head, use_causal_mask=True, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.num_heads       = num_heads\n",
    "        self.d_out           = d_out\n",
    "        self.dk_per_head     = dk_per_head   \n",
    "        self.dv_per_head     = dv_per_head\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "        \n",
    "        ##  Create heads\n",
    "        self.heads = [LinearDotProductAttention(self.dk_per_head, self.dv_per_head, use_causal_mask=use_causal_mask) \n",
    "                      for hi in range(num_heads)]\n",
    "                    \n",
    "        ##  Create other keras layers\n",
    "        self.concat  = Concatenate()\n",
    "        self.linear  = Dense(d_out)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"    \n",
    "        ##  Calculate attention heads\n",
    "        #        shape [batch_size, query_sequence_length, dv] for each list element\n",
    "        x = [head(inputs, training=training) for head in self.heads]\n",
    "        \n",
    "        ##  Concatenate heads and project onto single output\n",
    "        x = self.concat (x)\n",
    "        x = self.dropout(x, training=training)  \n",
    "        x = self.linear (x, training=training)     \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf653edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================\n",
    "#===  Create method for building simple model that performs one multi-head attention operation  ===\n",
    "#==================================================================================================\n",
    "\n",
    "def build_multi_head_attention_model(num_heads, d_out, query_length, ref_length=None, dk_per_head=10, \n",
    "                                     dv_per_head=None, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\"\n",
    "    ##  If ref_length or dv not provided then set equal to query_length\n",
    "    if not ref_length  : ref_length  = query_length\n",
    "    if not dv_per_head : dv_per_head = int(query_length/num_heads)\n",
    "        \n",
    "    ##  Create model inputs\n",
    "    query_sequence = Input((None, query_length))\n",
    "    ref_sequence   = Input((None, ref_length  ))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = MultiHeadAttention(num_heads, d_out, dk_per_head, dv_per_head)([query_sequence, ref_sequence])\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model([query_sequence, ref_sequence], x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420606d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"multi_head_attention_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, None, 128)   66048       ['input_3[0][0]',                \n",
      " dAttention)                                                      'input_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_in        = 128\n",
    "d_model     = 128\n",
    "num_heads   = 8\n",
    "dk_per_head = int(d_model / num_heads)\n",
    "dv_per_head = int(d_model / num_heads)\n",
    "\n",
    "model = build_multi_head_attention_model(num_heads, d_model, d_in, dk_per_head=dk_per_head, \n",
    "                                         dv_per_head=dv_per_head, name=\"multi_head_attention_model\")\n",
    "\n",
    "model.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9286f4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, None, 128)   66048       ['input_5[0][0]',                \n",
      " eadAttention)                                                    'input_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,048\n",
      "Trainable params: 66,048\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention as KMHA\n",
    "\n",
    "def build_keras_multi_head_attention_model(num_heads, d_model, dk_per_head, dv_per_head, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\"\n",
    "        \n",
    "    ##  Create model inputs\n",
    "    query_sequence = Input((None, d_model))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = KMHA(num_heads, key_dim=dk_per_head, value_dim=dv_per_head)(query_sequence, query_sequence)\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model(query_sequence, x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    \n",
    "test_model = build_keras_multi_head_attention_model(num_heads, d_model, dk_per_head, dv_per_head)\n",
    "\n",
    "test_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa160b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##   Encoder blocks\n",
    "\n",
    "We will build a sequence encoder by stacking blocks build on the attention mechanism. Each block will perform **self-attention**, meaning that we use the same sequence for both the query and reference. This means that a sequence will attend to itself to create an abstract representation, just like how we stack layers in a CNN or MLP. \n",
    "\n",
    "To construct each block, the first ingredient we will add is a **skip connection**. This means that we use addition or concatenation to combine the output of the attention layer with its own input to create a joint representation. In our case we will use addition to retain the same dimensionality. We could include a variable scalar to control how much information we want to include from the processed vs unprocessed representations, although the original AIAYN does not. Perhaps this is because this scale will be learned naturally as the scale of $V$. Skip connections can be _short_ skipping only one or two layers, or _long_ skipping many layers and maybe connecting directly to the final processing layers. We will use short skipp connections that bypass only the multi-head attention layer. The advantages of skip connections are:\n",
    "- we may re-use low-level features in our output / later layers\n",
    "- if e.g. positional information is lost in the attention layer, it may still propagate further down the network\n",
    "- allow us to learn deeper networks (unclear about explanation of why)\n",
    "\n",
    "We will use **layer normalisation** to keep our activations on a consistent scale. This is like batch normalisation, but it applies to individual datapoints rather than batches. For this reason, it only really works if the dimension $d_v$ is large enough, so the activations form a distribution with some notable shape. Note that vectors $[-1, 9]$, $[400, 500]$, $[-0.1, 0.1]$ etc will all be represented as $[-1, 1]$ after layer normalisation.\n",
    "\n",
    "Instead of feeding the resulting feature vectors directly into a new layer, we will introduce two dense layers to process the vector. This will turn the information into something abstract that will work together with the next to propagate information efficiently. The first layer has dimension $d_{ff}$ and a `relu` activation, and the second layer has dimension $d_v$ and a linear activation. Once again we will combine the output with a skip connection so the encoder block may be considered to _modify_ and not _replace_ the input.\n",
    "\n",
    "Let us combine all of these encoder block elements into a single layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e44d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================\n",
    "#===  Create keras layer for encoder block  ===\n",
    "#==============================================\n",
    "\n",
    "class EncoderBlock(Layer) :\n",
    "    \"\"\"\n",
    "    Class docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dk_per_head, dv_per_head, dff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.d_model     = d_model\n",
    "        self.num_heads   = num_heads\n",
    "        self.dk_per_head = dk_per_head\n",
    "        self.dv_per_head = dv_per_head\n",
    "        self.dff         = dff\n",
    "        \n",
    "        ##  Create attention layer\n",
    "        self.mha = MultiHeadAttention(num_heads, d_model, dk_per_head, dv_per_head)\n",
    "                    \n",
    "        ##  Create other keras layers\n",
    "        self.add1    = Add()\n",
    "        self.norm1   = LayerNormalization()\n",
    "        self.add2    = Add()\n",
    "        self.norm2   = LayerNormalization()\n",
    "        self.dense1  = Dense(dff, activation=\"relu\")\n",
    "        self.dense2  = Dense(d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def call(self, query_features, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Calculate multi-head attention\n",
    "        x_skip = query_features\n",
    "        x = self.mha([query_features, query_features], training=training)   # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Combine attention output with skip-connection\n",
    "        x = self.add1([x, x_skip], training=training)    # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.norm1(x, training=training)             # shape [batch_size, query_sequence_length, d_model] \n",
    "        x_skip = x\n",
    "                \n",
    "        ##  Feed-forward processing of linearly-combined feature vectors from each head\n",
    "        x = self.dense1 (x, training=training)            # shape [batch_size, query_sequence_length, dff] \n",
    "        x = self.dense2 (x, training=training)            # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.dropout(x, training=training)            # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Skip-connect and return\n",
    "        x = self.add2([x, x_skip], training=training)    # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.norm2(x, training=training)             # shape [batch_size, query_sequence_length, d_model]     \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ebbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================\n",
    "#===  Create method for building simple model that performs one multi-head attention operation  ===\n",
    "#==================================================================================================\n",
    "\n",
    "def build_encoder_block_model(d_model, num_heads, dk_per_head, dv_per_head, dff, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\" \n",
    "    ##  Create model inputs\n",
    "    query_features = Input((None, d_model))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = EncoderBlock(d_model, num_heads, dk_per_head, dv_per_head, dff)(query_features)\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model(query_features, x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2124d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None, 128)]       0         \n",
      "                                                                 \n",
      " encoder_block (EncoderBlock  (None, None, 128)        198272    \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,272\n",
      "Trainable params: 198,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_model     = 128\n",
    "num_heads   = 8\n",
    "dk_per_head = int(d_model / num_heads)\n",
    "dv_per_head = int(d_model / num_heads)\n",
    "dff         = 512\n",
    "\n",
    "model = build_encoder_block_model(d_model, num_heads, dk_per_head=dk_per_head, dv_per_head=dv_per_head, \n",
    "                                  dff=dff, name=\"encoder\")\n",
    "\n",
    "model.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc27ead8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Input embedding\n",
    "\n",
    "Our total encoder model is made of:\n",
    "- An embedding layer to create an efficient representation of the inputs. This uses a keras Embedding layer which possibly requires that the inputs are integer type. The output is of size $d_\\mathrm{model}$.\n",
    "- A positional encoding layer which adds information about sequence position. We add this to the embedding to create tokens that represent the bare token and its position\n",
    "- Several encoder blocks which perform attention and feed-forward processing, with output size $d_model$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689217f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer) :\n",
    "    \n",
    "    def __init__(self, d_in, d_model) :\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.d_in      = d_in\n",
    "        self.d_model   = d_model\n",
    "        \n",
    "        ##  Store Tensor object with pre-computed positional encodings\n",
    "        self.encoded_positions = self.create_encoded_positions_tensor(d_in, d_model)\n",
    "\n",
    "    def call(self, x) :\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Return slice of stored encoded_positions Tensor with correct shape\n",
    "        length = tf.shape(x)[1]\n",
    "        return self.encoded_positions[tf.newaxis, :length, :]\n",
    "    \n",
    "    def create_encoded_positions_tensor(self, d_in, d_model) :\n",
    "        \"\"\"\n",
    "        Method docstring\n",
    "        \"\"\"\n",
    "        ##  Create numpy array with positions\n",
    "        positions = np.arange(d_in)   # shape (d_in)\n",
    "        \n",
    "        ##  Combine with indices to create 2D array of angles\n",
    "        half_indices = np.arange(d_model/2)   # shape (d_model/2)\n",
    "        angles = (10000**(-half_indices))     # shape (d_model/2)\n",
    "        angles = np.outer(positions, angles)  # shape (d_in, d_model/2)\n",
    "\n",
    "        ##  Interleave sing and cos of angles into single 2D array of positional encodings\n",
    "        pos_encoding = np.concatenate([np.sin(angles), np.cos(angles)], axis=-1)   # shape (d_in, d_model)\n",
    "\n",
    "        ##  Return Tensor of positional encodings\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37afc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "#===  Create keras layer for encoder  ===\n",
    "#========================================\n",
    "\n",
    "class Encoder(Layer) :\n",
    "    \"\"\"\n",
    "    Class docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.num_blocks  = num_blocks\n",
    "        self.d_model     = d_model\n",
    "        self.num_heads   = num_heads\n",
    "        self.dk_per_head = dk_per_head\n",
    "        self.dv_per_head = dv_per_head\n",
    "        self.dff         = dff\n",
    "        self.emb_scalar  = tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "                    \n",
    "        ##  Create embedding layers\n",
    "        self.add               = Add()\n",
    "        self.token_embedding   = Embedding(d_in, d_model, mask_zero=True)\n",
    "        self.position_encoding = PositionalEncoding(d_in, d_model)\n",
    "        \n",
    "        ##  Create encoder block layers\n",
    "        self.encoder_blocks = [EncoderBlock(d_model, num_heads, dk_per_head, dv_per_head, dff) \n",
    "                               for ei in range(num_blocks)]\n",
    "        \n",
    "    def call(self, query_features, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Calculate embeddings\n",
    "        token_embedding   = self.token_embedding(query_features, training=training) # shape [batch_size, query_sequence_length, d_model]\n",
    "        token_embedding  *= self.emb_scalar                                         # shape [batch_size, query_sequence_length, d_model]\n",
    "        position_encoding = self.position_encoding(query_features)                  # shape [batch_size, query_sequence_length, d_model]\n",
    "        x = self.add([token_embedding, position_encoding], training=training)       # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Pass through encoder blocks\n",
    "        for encoder_block in self.encoder_blocks :\n",
    "            x = encoder_block(x, training=training)\n",
    "            \n",
    "        ##  Return encoded sequence\n",
    "        return x\n",
    "    \n",
    "    '''def compute_mask(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Method docstring\n",
    "        \"\"\"\n",
    "        return self.token_embedding.compute_mask(*args, **kwargs)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae86e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#===  Create method for building simple model that performs the encoding  ===\n",
    "#============================================================================\n",
    "\n",
    "def build_encoder_model(num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\" \n",
    "    ##  Create model inputs\n",
    "    query_features = Input((None,))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = Encoder(num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff)(query_features)\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model(query_features, x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33647a18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " encoder (Encoder)           (None, None, 128)         3632768   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,632,768\n",
      "Trainable params: 3,632,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_in        = 7765   # portuguese vocab size\n",
    "d_model     = 128\n",
    "num_blocks  = 4\n",
    "num_heads   = 8\n",
    "dk_per_head = d_model  # int(d_model / num_heads)\n",
    "dv_per_head = d_model  # int(d_model / num_heads)\n",
    "dff         = 512\n",
    "\n",
    "model = build_encoder_model(num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff, name=\"encoder\")\n",
    "\n",
    "model.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f6de9",
   "metadata": {},
   "source": [
    "## Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851ded42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================\n",
    "#===  Create keras layer for encoder block  ===\n",
    "#==============================================\n",
    "\n",
    "class DecoderBlock(Layer) :\n",
    "    \"\"\"\n",
    "    Class docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dk_per_head, dv_per_head, dff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.d_model     = d_model\n",
    "        self.num_heads   = num_heads\n",
    "        self.dk_per_head = dk_per_head\n",
    "        self.dv_per_head = dv_per_head\n",
    "        self.dff         = dff\n",
    "        \n",
    "        ##  Create attention layers\n",
    "        self.masked_attention = MultiHeadAttention(num_heads, d_model, dk_per_head, dv_per_head, use_causal_mask=True )\n",
    "        self.cross_attention  = MultiHeadAttention(num_heads, d_model, dk_per_head, dv_per_head, use_causal_mask=False)\n",
    "                    \n",
    "        ##  Create other keras layers\n",
    "        self.add1    = Add()\n",
    "        self.norm1   = LayerNormalization()\n",
    "        self.add2    = Add()\n",
    "        self.norm2   = LayerNormalization()\n",
    "        self.add3    = Add()\n",
    "        self.norm3   = LayerNormalization()\n",
    "        self.dense1  = Dense(dff, activation=\"relu\")\n",
    "        self.dense2  = Dense(d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Resolve inputs\n",
    "        if len(inputs) != 2 : raise ValueError(f\"inputs of length {len(inputs)}, expected 2\")\n",
    "        decoder_input  = inputs[0]\n",
    "        encoder_output = inputs[1]\n",
    "        \n",
    "        ##  Calculate masked self-attention\n",
    "        x_skip = decoder_input\n",
    "        x = self.masked_attention([decoder_input, decoder_input], training=training)   # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Combine attention output with skip-connection\n",
    "        x = self.add1([x, x_skip], training=training)          # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.norm1(x, training=training)                   # shape [batch_size, query_sequence_length, d_model] \n",
    "        x_skip = x\n",
    "        \n",
    "        ##  Calculate cross-attention\n",
    "        x = self.cross_attention([x, encoder_output], training=training)   # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Combine attention output with skip-connection\n",
    "        x = self.add2([x, x_skip], training=training)          # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.norm2(x, training=training)                   # shape [batch_size, query_sequence_length, d_model] \n",
    "        x_skip = x\n",
    "        \n",
    "        ##  Feed-forward processing of linearly-combined feature vectors from each head\n",
    "        x = self.dense1 (x, training=training)            # shape [batch_size, query_sequence_length, dff] \n",
    "        x = self.dense2 (x, training=training)            # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.dropout(x, training=training)            # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Skip-connect and return\n",
    "        x = self.add3([x, x_skip], training=training)          # shape [batch_size, query_sequence_length, d_model] \n",
    "        x = self.norm3(x, training=training)                   # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9586ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================\n",
    "#===  Create method for building simple model that performs one multi-head attention operation  ===\n",
    "#==================================================================================================\n",
    "\n",
    "def build_decoder_block_model(d_model, num_heads, dk_per_head, dv_per_head, dff, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\" \n",
    "    ##  Create model inputs\n",
    "    query_features = Input((None, d_model))\n",
    "    ref_features   = Input((None, d_model))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = DecoderBlock(d_model, num_heads, dk_per_head, dv_per_head, dff)([query_features, ref_features])\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model([query_features, ref_features], x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4471b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_block_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder_block (DecoderBlock)   (None, None, 128)    264576      ['input_8[0][0]',                \n",
      "                                                                  'input_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 264,576\n",
      "Trainable params: 264,576\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_model     = 128\n",
    "num_heads   = 8\n",
    "dk_per_head = int(d_model / num_heads)\n",
    "dv_per_head = int(d_model / num_heads)\n",
    "dff         = 512\n",
    "\n",
    "model = build_decoder_block_model(d_model, num_heads, dk_per_head=dk_per_head, dv_per_head=dv_per_head, \n",
    "                                  dff=dff, name=\"decoder_block_model\")\n",
    "\n",
    "model.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bd71b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec9b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "#===  Create keras layer for decoder  ===\n",
    "#========================================\n",
    "\n",
    "class Decoder(Layer) :\n",
    "    \"\"\"\n",
    "    Class docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.num_blocks  = num_blocks\n",
    "        self.d_in        = d_in\n",
    "        self.d_model     = d_model\n",
    "        self.num_heads   = num_heads\n",
    "        self.dk_per_head = dk_per_head\n",
    "        self.dv_per_head = dv_per_head\n",
    "        self.dff         = dff\n",
    "        self.emb_scalar  = tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "                    \n",
    "        ##  Create embedding layers\n",
    "        self.add               = Add()\n",
    "        self.token_embedding   = Embedding(d_in, d_model, mask_zero=True)\n",
    "        self.position_encoding = PositionalEncoding(d_in, d_model)\n",
    "        \n",
    "        ##  Create decoder block layers\n",
    "        self.decoder_blocks = [DecoderBlock(d_model, num_heads, dk_per_head, dv_per_head, dff) \n",
    "                               for ei in range(num_blocks)]\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Resolve inputs\n",
    "        if len(inputs) != 2 : raise ValueError(f\"inputs of length {len(inputs)}, expected 2\")\n",
    "        decoder_input  = inputs[0]\n",
    "        encoder_output = inputs[1]\n",
    "        \n",
    "        ##  Calculate embeddings\n",
    "        token_embedding   = self.token_embedding(decoder_input, training=training) # shape [batch_size, query_sequence_length, d_model]\n",
    "        token_embedding  *= self.emb_scalar                                        # shape [batch_size, query_sequence_length, d_model]\n",
    "        position_encoding = self.position_encoding(decoder_input)                  # shape [batch_size, query_sequence_length, d_model]\n",
    "        x = self.add([token_embedding, position_encoding], training=training)      # shape [batch_size, query_sequence_length, d_model] \n",
    "                \n",
    "        ##  Pass through decoder blocks\n",
    "        for decoder_block in self.decoder_blocks :\n",
    "            x = decoder_block([x, encoder_output], training=training)\n",
    "            \n",
    "        ##  Return token probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9a1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#===  Create method for building simple model that performs the decoding  ===\n",
    "#============================================================================\n",
    "\n",
    "def build_decoder_model(num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\" \n",
    "    ##  Create model inputs\n",
    "    query_features = Input((None,))\n",
    "    ref_features   = Input((None, d_model))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = Decoder(num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff)([query_features, ref_features])\n",
    "    \n",
    "    ##  Create model, but don't compile as we don't yet have a well-defined loss function\n",
    "    model = Model([query_features, ref_features], x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fb3fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder (Decoder)              (None, None, 128)    5647104     ['input_10[0][0]',               \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,647,104\n",
      "Trainable params: 5,647,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_in        = 7010\n",
    "d_model     = 128\n",
    "num_blocks  = 4\n",
    "num_heads   = 8\n",
    "dk_per_head = d_model  # int(d_model / num_heads)\n",
    "dv_per_head = d_model  # int(d_model / num_heads)\n",
    "dff         = 512\n",
    "\n",
    "model = build_decoder_model(num_blocks, d_in, d_model, num_heads, dk_per_head, dv_per_head, dff, name=\"decoder\")\n",
    "\n",
    "model.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f92a3",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa343e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "#===  Create keras layer for decoder  ===\n",
    "#========================================\n",
    "\n",
    "class Transformer(Layer) :\n",
    "    \"\"\"\n",
    "    Class docstring\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks, d_in, d_out, d_model, num_heads, dk_per_head, dv_per_head, dff):\n",
    "        \"\"\"\n",
    "        Init docstring\n",
    "        \"\"\"\n",
    "        ##  Initialise base class\n",
    "        super().__init__()\n",
    "        \n",
    "        ##  Store dimension information\n",
    "        self.num_blocks  = num_blocks\n",
    "        self.d_in        = d_in\n",
    "        self.d_out       = d_out\n",
    "        self.d_model     = d_model\n",
    "        self.num_heads   = num_heads\n",
    "        self.dk_per_head = dk_per_head\n",
    "        self.dv_per_head = dv_per_head\n",
    "        self.dff         = dff\n",
    "        self.emb_scalar  = tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "                    \n",
    "        ##  Create encoder and decoder layers\n",
    "        self.encoder = Encoder(num_blocks, d_in , d_model, num_heads, dk_per_head, dv_per_head, dff)\n",
    "        self.decoder = Decoder(num_blocks, d_out, d_model, num_heads, dk_per_head, dv_per_head, dff)\n",
    "                \n",
    "        ##  Create layers to convert decoder output to token probabilities\n",
    "        self.linear  = Dense(d_out)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Call docstring\n",
    "        \"\"\"\n",
    "        ##  Resolve inputs\n",
    "        if len(inputs) != 2 : raise ValueError(f\"inputs of length {len(inputs)}, expected 2\")\n",
    "        encoder_input = inputs[0]\n",
    "        decoder_input = inputs[1]\n",
    "        \n",
    "        ##  Calculate encoding\n",
    "        x = self.encoder(encoder_input, training=training)\n",
    "                \n",
    "        ##  Calculate decoding\n",
    "        x = self.decoder([decoder_input, x], training=training)\n",
    "                        \n",
    "        ##  Turn decoder outputs into token probabilities\n",
    "        x = self.linear (x, training=training)\n",
    "        x = self.softmax(x, training=training)\n",
    "            \n",
    "        ##  Return token probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e5f5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#===  Create method for building simple model that performs the decoding  ===\n",
    "#============================================================================\n",
    "\n",
    "def build_transformer_model(num_blocks, d_in, d_out, d_model, num_heads, dk_per_head, dv_per_head, dff, name=None) :\n",
    "    \"\"\"\n",
    "    Method docstring\n",
    "    \"\"\" \n",
    "    ##  Create model inputs\n",
    "    original_message   = Input((None,))\n",
    "    translated_message = Input((None,))\n",
    "    \n",
    "    ##  Perform attention step\n",
    "    x = Transformer(num_blocks, d_in, d_out, d_model, num_heads, dk_per_head, dv_per_head, dff)([original_message, translated_message])\n",
    "    \n",
    "    ##  Create model\n",
    "    model = Model([original_message, translated_message], x, name=name)\n",
    "    \n",
    "    ##  Return\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49093f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_1 (Transformer)    (None, None, 7010)   10184162    ['input_14[0][0]',               \n",
      "                                                                  'input_15[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,184,162\n",
      "Trainable params: 10,184,162\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#==============================================================\n",
    "#===  Build and print model to check for structural errors  ===\n",
    "#==============================================================\n",
    "\n",
    "d_in        = 7765    # portuguese vocab size\n",
    "d_out       = 7010    # english vocab size\n",
    "d_model     = 128\n",
    "num_blocks  = 4\n",
    "num_heads   = 8\n",
    "dk_per_head = d_model  # int(d_model / num_heads)\n",
    "dv_per_head = d_model  # int(d_model / num_heads)\n",
    "dff         = 512\n",
    "\n",
    "transformer = build_transformer_model(num_blocks, d_in, d_out, d_model, num_heads, dk_per_head, dv_per_head, \n",
    "                                      dff, name=\"transformer\")\n",
    "\n",
    "transformer.summary(expand_nested=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "159ea3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 343 5351 5122 7580 4693 5521  965 2847 1894  267  421 2502  405 5599\n",
      "    62 4444 4641 3127 1506 6238 2282 7389  680]], shape=(1, 23), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2349 6745 2682 4439 6642 3803 1927 6798  301 5927 2179 1697 2458 5773\n",
      "  2286 4358]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[6745 2682 4439 6642 3803 1927 6798  301 5927 2179 1697 2458 5773 2286\n",
      "  4358 1331]], shape=(1, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_pt = np.random.randint(d_in , size=(1,23))\n",
    "seq_en   = np.random.randint(d_out, size=(1,17))\n",
    "input_en = seq_en[:,:-1]\n",
    "label_en = seq_en[:,1: ]\n",
    "\n",
    "input_pt = tf.cast(input_pt, dtype=tf.int32)\n",
    "input_en = tf.cast(input_en, dtype=tf.int32)\n",
    "label_en = tf.cast(label_en, dtype=tf.int32)\n",
    "\n",
    "print(input_pt)\n",
    "print(input_en)\n",
    "print(label_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a960bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = transformer([input_pt, input_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c971c502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 16, 7010])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901118c1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c977f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef masked_loss(label, pred):\\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\\n    loss = loss_object(label, pred)\\n\\n    mask = tf.where(label==0, 0, 1)\\n    mask = tf.cast(mask, dtype=loss.dtype)\\n    loss *= mask\\n\\n    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\\n    return loss\\n\\ndef masked_accuracy(label, pred):\\n    pred  = tf.argmax(pred)\\n    label = tf.cast (label, pred.dtype)\\n    mask  = tf.where(label==0, 0, 1)\\n    match = tf.where(label==pred, 1, 0)\\n    match = mask * match\\n    match = tf.cast(match, dtype=tf.float32)\\n    mask  = tf.cast(mask , dtype=tf.float32)\\n    return tf.reduce_sum(match) / tf.reduce_sum(mask)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def masked_loss(label, pred):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.where(label==0, 0, 1)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred  = tf.argmax(pred)\n",
    "    label = tf.cast (label, pred.dtype)\n",
    "    mask  = tf.where(label==0, 0, 1)\n",
    "    match = tf.where(label==pred, 1, 0)\n",
    "    match = mask * match\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask  = tf.cast(mask , dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd200945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    mask = label != 0\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b3f4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "transformer.compile(\n",
    "    loss      = masked_loss,\n",
    "    optimizer = optimizer,\n",
    "    metrics   = [masked_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2671804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = tf.data.Dataset.load(\"tokenised_train_batches\")\n",
    "val_batches   = tf.data.Dataset.load(\"tokenised_val_batches\"  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00e60af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "810/810 [==============================] - 1086s 1s/step - loss: 2.1438 - masked_accuracy: 0.5788 - val_loss: 2.5176 - val_masked_accuracy: 0.5490\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 1057s 1s/step - loss: 2.0441 - masked_accuracy: 0.5928 - val_loss: 2.4931 - val_masked_accuracy: 0.5551\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 1054s 1s/step - loss: 1.9627 - masked_accuracy: 0.6042 - val_loss: 2.4798 - val_masked_accuracy: 0.5599\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 1054s 1s/step - loss: 1.8848 - masked_accuracy: 0.6164 - val_loss: 2.5116 - val_masked_accuracy: 0.5563\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 1052s 1s/step - loss: 1.8164 - masked_accuracy: 0.6264 - val_loss: 2.4743 - val_masked_accuracy: 0.5625\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 1056s 1s/step - loss: 1.7488 - masked_accuracy: 0.6369 - val_loss: 2.4936 - val_masked_accuracy: 0.5648\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 1054s 1s/step - loss: 1.6934 - masked_accuracy: 0.6452 - val_loss: 2.4889 - val_masked_accuracy: 0.5626\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 1053s 1s/step - loss: 1.6386 - masked_accuracy: 0.6540 - val_loss: 2.5050 - val_masked_accuracy: 0.5653\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 1053s 1s/step - loss: 1.5929 - masked_accuracy: 0.6610 - val_loss: 2.5221 - val_masked_accuracy: 0.5615\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 1054s 1s/step - loss: 1.5437 - masked_accuracy: 0.6683 - val_loss: 2.5568 - val_masked_accuracy: 0.5639\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 1052s 1s/step - loss: 1.5020 - masked_accuracy: 0.6757 - val_loss: 2.5677 - val_masked_accuracy: 0.5606\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 1052s 1s/step - loss: 1.4671 - masked_accuracy: 0.6809 - val_loss: 2.6034 - val_masked_accuracy: 0.5572\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 1052s 1s/step - loss: 1.4266 - masked_accuracy: 0.6880 - val_loss: 2.6269 - val_masked_accuracy: 0.5563\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 1052s 1s/step - loss: 1.3901 - masked_accuracy: 0.6940 - val_loss: 2.6527 - val_masked_accuracy: 0.5544\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 1051s 1s/step - loss: 1.3613 - masked_accuracy: 0.6986 - val_loss: 2.6967 - val_masked_accuracy: 0.5534\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 1051s 1s/step - loss: 1.3213 - masked_accuracy: 0.7057 - val_loss: 2.7721 - val_masked_accuracy: 0.5479\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 1052s 1s/step - loss: 1.2952 - masked_accuracy: 0.7101 - val_loss: 2.7797 - val_masked_accuracy: 0.5508\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 1053s 1s/step - loss: 1.2658 - masked_accuracy: 0.7151 - val_loss: 2.7643 - val_masked_accuracy: 0.5548\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 1054s 1s/step - loss: 1.2391 - masked_accuracy: 0.7197 - val_loss: 2.8109 - val_masked_accuracy: 0.5536\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 1054s 1s/step - loss: 1.2163 - masked_accuracy: 0.7233 - val_loss: 2.7781 - val_masked_accuracy: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x286f0c7f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformer.fit(\n",
    "    train_batches, \n",
    "    epochs=20, \n",
    "    validation_data=val_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "749051bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.save('saved_model/transformer.h5')\n",
    "# model.save_weights('saved_model/transformer_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0964638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_sentence(sentence, transformer, tokenizers, max_tokens=30) :\n",
    "    with tf.device(\"CPU\") :\n",
    "        tokenised_sentence = tokenizers.pt.tokenize([sentence]).to_tensor()\n",
    "        \n",
    "        start_end   = tokenizers.en.tokenize([''])[0]\n",
    "        begin_token = start_end[0 ]\n",
    "        end_token   = start_end[-1]\n",
    "        \n",
    "        tokenised_translated_sentence = tf.cast([[begin_token]], dtype=tf.int64)\n",
    "                \n",
    "        best_token, num_tokens = begin_token, 1\n",
    "        while best_token != end_token and num_tokens < max_tokens :\n",
    "            token_probs = transformer([tokenised_sentence, tokenised_translated_sentence])\n",
    "            best_token  = tf.argmax(token_probs[0,-1,:], axis=-1)\n",
    "            tokenised_translated_sentence = tf.concat([tokenised_translated_sentence, \n",
    "                                                       tf.reshape(best_token, (1,1))],\n",
    "                                                      axis=-1)\n",
    "            num_tokens += 1\n",
    "            \n",
    "        translated_sentence = tokenizers.en.detokenize(tokenised_translated_sentence)[0]\n",
    "        return translated_sentence.numpy().decode('utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "794c086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_model_fname = 'ted_hrlr_translate_pt_en_converter'\n",
    "tokenizers = tf.saved_model.load(tokenizer_model_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eddf3411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese sentence  : No h ma nesta sala, h apenas trs bananas e uma toranja\n",
      "Ground truth         : There is no apple in this room, there are only three bananas and a grapefruit\n",
      "Predicted translation: there ' s no apple in this room , there ' s only three braft and a srarararararararara\n"
     ]
    }
   ],
   "source": [
    "en_sentence = \"There is no apple in this room, there are only three bananas and a grapefruit\"\n",
    "pt_sentence = \"No h ma nesta sala, h apenas trs bananas e uma toranja\"\n",
    "\n",
    "pred_sentence = translate_sentence(pt_sentence, transformer, tokenizers)\n",
    "\n",
    "print(f\"Portuguese sentence  : {pt_sentence}\")\n",
    "print(f\"Ground truth         : {en_sentence}\")\n",
    "print(f\"Predicted translation: {pred_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
